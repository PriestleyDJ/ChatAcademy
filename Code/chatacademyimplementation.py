# -*- coding: utf-8 -*-
"""ChatAcademyImplementation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lUWHGEOSojUxIwz2cJX6rSXs1599_5tl

# LLama Index Demo - By Seth Steele
---

This is a simple demo of RAG on LLama-2

## 1. Change to GPU runtime
Click on "Runtime" -> "Change runtime type" and make sure "T4 GPU" is selected (the only GPU available on the free plan).

## 2. Installation and Setup

The following snippet of code will:
1. Install the transformers and accelerate libraries that we will use to access and run the Llama model.
2. Initiate a login to your HuggingFace account.
3. Install the necessary packages and our LLama-2 LLM.
4. Imports the packages we need
5. Connects to the google drive to access data if needed (Comment this out if not on colab)

This second step is nessecary because, whilst Llama is an open-source model, access to it is still restricted to those who have been given access by Meta. Instructions for getting access to Llama + granting that access to your HuggingFace account can be found here: https://ai.meta.com/llama/get-started/
"""

import os

hf_token = sys.argv[1]

from peft import LoraConfig
from datasets import load_dataset
from trl import SFTTrainer
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, pipeline, logging
from llama_index.core import VectorStoreIndex, PromptTemplate, Settings, Document, Response
from llama_index.llms.huggingface import HuggingFaceLLM
from llama_index.core.evaluation import FaithfulnessEvaluator, RelevancyEvaluator, CorrectnessEvaluator, DatasetGenerator, BatchEvalRunner
from bs4 import BeautifulSoup
import torch
import sys

"""**Note** - you may have to restart the runtime
by clicking "Runtime" -> "Restart runtime" after loading in the accelerator library for the subsequent code to run.

# 3. Setup The LLM

These are the settings that change the LLM in use to the 7 billion parameter model of Llama-2 and get it ready for fine tuning.
"""

compute_dtype = getattr(torch, "float16")

baseModel = "meta-llama/Llama-2-70b-chat-hf"
"""
quant_config = BitsAndBytesConfig(
    load_in_4bit=True,
    load_in_8bit=False,
    bnb_4bit_compute_dtype=compute_dtype,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
)

llm = AutoModelForCausalLM.from_pretrained(
    baseModel,
    quantization_config=quant_config,
    device_map={"": 0}
)

llm.config.use_cache = False
llm.config.pretraining_tp = 1

datasetName = "cais/mmlu"
dataset = load_dataset(datasetName , split="test", name = "all")

new_model = "llama-2-7b-chat-academy-test"

tokenizer = AutoTokenizer.from_pretrained(baseModel, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

peft_params = LoraConfig(
    lora_alpha=16,
    lora_dropout=0.1,
    r=64,
    bias="none",
    task_type="QUESTION_ANS",
)

training_params = TrainingArguments(
    output_dir="./results",
    num_train_epochs=1,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=1,
    optim="paged_adamw_32bit",
    save_steps=25,
    logging_steps=25,
    learning_rate=2e-4,
    weight_decay=0.001,
    fp16=False,
    bf16=False,
    max_grad_norm=0.3,
    max_steps=-1,
    warmup_ratio=0.03,
    group_by_length=True,
    lr_scheduler_type="constant",
    report_to="tensorboard"
)

trainer = SFTTrainer(
    model=llm,
    train_dataset=dataset,
    peft_config=peft_params,
    dataset_text_field="question",
    max_seq_length=None,
    tokenizer=tokenizer,
    args=training_params,
    packing=False,
)

trainer.model.save_pretrained(new_model)
trainer.tokenizer.save_pretrained(new_model)

trainedLLM = HuggingFaceLLM(
    model_name= new_model,
    tokenizer_name= new_model,
    query_wrapper_prompt=PromptTemplate("<s> [INST] {query_str} [/INST] "),
    context_window=3900,
    model_kwargs={"token": hf_token, "quantization_config": quant_config},
    tokenizer_kwargs={"token": hf_token},
    device_map="auto",
)
"""

llm = HuggingFaceLLM(model_id=baseModel)

"""#  4. Load the data and build an index

The following code defines some custom readers creates an index over the xml files.

This part defines all these methods.
"""

def roleLogic(isStudent, isStaff):
  if(isStudent == "true"):
    if(isStaff == "false"):
      return "student"
    else:
      return "student and academic"
  else:
    if(isStaff == "true"):
      return "academic"
    else:
      return "neither student nor academic"

def abstractReader(root):
  abstract = ""
  try:
      abstract = root.find("api:field", {"name": "abstract", "type":"text", "display-name":"Abstract"}).text
  except AttributeError:
     abstract = "N/A"
  return abstract

def authorFromPubReader(root):
  authorsData = root.find("api:field", {"name": "authors", "type":"person-list", "display-name":"Authors"})
  authorData = authorsData.find_all("api:person")
  authorList = []
  for author in authorData:
    initials = author.find("api:initials").text
    try:
      first_name = author.find("api:first-names").text
    except AttributeError:
      first_name = initials
    last_name = author.find("api:last-name").text
    authorDict = {"initials": initials, "first name": first_name, "last name": last_name}
    authorList.append(authorDict)
  return authorList

def pubDateReader(root):
  data = root.find("api:field", {"name": "publication-date", "type":"person-list", "display-name":"Authors"})
  day = None
  month = None
  year = None
  try:
    year = data.find("api:year").text
  except AttributeError:
    year = "N/A"
  try:
    month = data.find("api:month").text
  except AttributeError:
    month = "N/A"
  try:
    day = data.find("api:day").text
  except AttributeError:
    day = "N/A"

  return day, month, year

def keywordsReader(root):
  try:
     keywords = root.find("api:field", {"name": "keywords", "type":"keyword-list", "display-name":"Keywords"}).find_all("api:keyword").text
  except:
     keywords = "N/A"
  return keywords

def publicationReader(pubFiles):
  publicationDocs = []
  for path in pubFiles:
    with open(path, 'r') as f:
      data = f.read()
    root = BeautifulSoup(data, "xml")
    abstract = abstractReader(root)
    authorList = authorFromPubReader(root)
    title = root.find("api:field", {"name": "title", "type":"text", "display-name":"Title"}).text
    keywords = keywordsReader(root)
    date = pubDateReader(root)
    doc = Document(
      text = abstract,
      metadata={
            "authors": authorList,
            "title": title,
            "keywords": keywords,
            "day published": date[0],
            "month published": date[1],
            "year published": date[2]
            }
    )
    publicationDocs.append(doc)
  return publicationDocs

def authorReader(autFiles):
  authorDocs = []
  for path in autFiles:
    with open(path, 'r') as f:
      data = f.read()
    root = BeautifulSoup(data, "xml")
    title = root.find("api:title").text
    initials = root.find("api:initials").text
    first_name = root.find("api:first-name").text
    last_name = root.find("api:last-name").text
    role = roleLogic(root.find("api:is-student").text, root.find("api:is-academic").text)
    current_member = root.find("api:is-current-staff").text
    email = root.find("api:email-address").text
    department = root.find("api:primary-group-descriptor").text
    arrival_date = root.find("api:arrive-date").text
    doc = Document(
      text = title + " " + first_name + " " + last_name,
      metadata={
          "title": title,
          "initials": initials,
          "first name": first_name,
          "last_name": last_name,
          "role": role,
          "current member": current_member,
          "email": email,
          "department": department,
          "arrival_date": arrival_date,
          }
    )
    authorDocs.append(doc)
  return authorDocs

"""And this section actually builds the index."""

def absoluteFilePaths(directory):
    files = []
    for dirpath,_,filenames in os.walk(directory):
        for f in filenames:
            files.append(os.path.abspath(os.path.join(dirpath, f)))
    return files

publicationFiles = absoluteFilePaths("/users/aca19sjs/chatAcademy/data/publications")
authorFiles = absoluteFilePaths("/users/aca19sjs/chatAcademy/data/staff")


docs = publicationReader(publicationFiles) + authorReader(authorFiles)

dataset_generator = DatasetGenerator.from_documents(docs, llm=llm)

qas = dataset_generator.generate_dataset_from_nodes(num=3)

Settings.chunk_size = 512
Settings.chunk_overlap = 50
Settings.llm = llm
Settings.embed_model = "local:BAAI/bge-small-en-v1.5"

index = VectorStoreIndex.from_documents(docs)

"""# 5. Use the model to respond to a query
In this section we can write out our query and then get the model to respond.

The following line is simply to set our query, change this to whatever you would like to ask the model.
"""

prompt ="Tell me about a paper titled Connectionist simulation of attitude learning: Asymmetries in the acquisition of positive and negative evaluations by JR Eiser?"

query_engine = index.as_query_engine()
response = query_engine.query(prompt)
print(response)

#chat_engine = index.as_chat_engine()
#response = chat_engine.chat(prompt)
#print(response)

"""# 6. Import Some Dataset Questions
This section of the implementation takes some predetermined questions abou the system and judges how relevant the response is.
"""

faithfulness_llm = FaithfulnessEvaluator(llm=llm)
relevancy_llm = RelevancyEvaluator(llm=llm)
correctness_llm = CorrectnessEvaluator(llm=llm)

runner = BatchEvalRunner(
    {"faithfulness": faithfulness_llm, "relevancy": relevancy_llm, "correctness": correctness_llm},
    workers=8,
)

eval_results = runner.aevaluate_queries(
    index.as_query_engine(llm=llm), queries=qas.questions
)

def get_eval_results(key, eval_results):
    results = eval_results[key]
    correct = 0
    for result in results:
        if result.passing:
            correct += 1
    score = correct / len(results)
    return f"{key} Score: {score} \n"

save_path = '/users/aca19sjs/chatAcademy/Datasets/'

fileName = os.path.join(save_path, "results.txt")  
file1 = open(fileName, "w")
toFile = input(str(get_eval_results("correctness", eval_results)+ get_eval_results("faithfulness", eval_results)+ get_eval_results("relevancy", eval_results)))
file1.write(toFile)
file1.close()