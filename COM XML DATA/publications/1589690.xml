<?xml version="1.0" encoding="utf-8"?><api:response xmlns:api="http://www.symplectic.co.uk/publications/api"><api:version uri="https://mypublications.shef.ac.uk/" elements-version="6.17.0.4095" schema-version="6.13" product-name="myPublications"/><api:request href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/publications/1589690"/><api:result><api:object category="publication" id="1589690" last-affected-when="2024-02-14T10:37:38.367+00:00" last-modified-when="2024-02-14T10:37:38.367+00:00" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/publications/1589690" created-when="2024-01-31T10:37:37.13+00:00" type-id="4" type-display-name="Conference proceedings paper" type="conference"><api:privacy-level>Public</api:privacy-level><api:privacy-level-locked>false</api:privacy-level-locked><api:ever-approved>true</api:ever-approved><api:reporting-date-1>2021-01-01</api:reporting-date-1><api:allow-type-switching>true</api:allow-type-switching><api:records><api:record format="native" id="4531632" source-id="7" source-name="scopus" source-display-name="Scopus" id-at-source="2-s2.0-85124331548" last-modified-when="2024-02-14T10:37:38.37+00:00"><api:citation-count>1</api:citation-count><api:native><api:field name="abstract" type="text" display-name="Abstract"><api:text>High sample complexity remains a barrier to the application of reinforcement learning (RL), particularly in multi-agent systems. A large body of work has demonstrated that exploration mechanisms based on the principle of optimism under uncertainty can significantly improve the sample efficiency of RL in single agent tasks. This work seeks to understand the role of optimistic exploration in non-cooperative multi-agent settings. We will show that, in zero-sum games, optimistic exploration can cause the learner to waste time sampling parts of the state space that are irrelevant to strategic play, as they can only be reached through cooperation between both players. To address this issue, we introduce a formal notion of strategically efficient exploration in Markov games, and use this to develop two strategically efficient learning algorithms for finite Markov games. We demonstrate that these methods can be significantly more sample efficient than their optimistic counterparts.</api:text></api:field><api:field name="authors" type="person-list" display-name="Authors"><api:people><api:person><api:links><api:link type="elements/user" id="37828" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/users/37828"/></api:links><api:last-name>Loftin</api:last-name><api:initials>R</api:initials><api:first-names>R</api:first-names><api:separate-first-names><api:first-name>R</api:first-name></api:separate-first-names><api:addresses><api:address iso-country-code="GB"><api:line type="organisation">Microsoft Research Cambridge</api:line><api:line type="city">Cambridge</api:line><api:line type="country">United Kingdom</api:line></api:address></api:addresses><api:identifiers><api:identifier scheme="scopus-author-id">58514845500</api:identifier></api:identifiers></api:person><api:person><api:last-name>Saha</api:last-name><api:initials>A</api:initials><api:first-names>A</api:first-names><api:separate-first-names><api:first-name>A</api:first-name></api:separate-first-names><api:addresses><api:address iso-country-code="US"><api:line type="organisation">Microsoft Research</api:line><api:line type="city">Redmond</api:line><api:line type="country">United States</api:line></api:address></api:addresses><api:identifiers><api:identifier scheme="scopus-author-id">56555874900</api:identifier></api:identifiers></api:person><api:person><api:last-name>Devlin</api:last-name><api:initials>S</api:initials><api:first-names>S</api:first-names><api:separate-first-names><api:first-name>S</api:first-name></api:separate-first-names><api:addresses><api:address iso-country-code="GB"><api:line type="organisation">Microsoft Research Cambridge</api:line><api:line type="city">Cambridge</api:line><api:line type="country">United Kingdom</api:line></api:address></api:addresses><api:identifiers><api:identifier scheme="scopus-author-id">37107376400</api:identifier></api:identifiers></api:person><api:person><api:last-name>Hofmann</api:last-name><api:initials>K</api:initials><api:first-names>K</api:first-names><api:separate-first-names><api:first-name>K</api:first-name></api:separate-first-names><api:addresses><api:address iso-country-code="GB"><api:line type="organisation">Microsoft Research Cambridge</api:line><api:line type="city">Cambridge</api:line><api:line type="country">United Kingdom</api:line></api:address></api:addresses><api:identifiers><api:identifier scheme="scopus-author-id">23011906300</api:identifier></api:identifiers></api:person></api:people></api:field><api:field name="journal" type="text" display-name="Title of published proceedings"><api:text>37th Conference on Uncertainty in Artificial Intelligence, UAI 2021</api:text></api:field><api:field name="pagination" type="pagination" display-name="Pagination"><api:pagination><api:begin-page>1587</api:begin-page><api:end-page>1596</api:end-page></api:pagination></api:field><api:field name="publication-date" type="date" display-name="Publication date"><api:date><api:day>1</api:day><api:month>1</api:month><api:year>2021</api:year></api:date></api:field><api:field name="publication-status" type="text" display-name="Status"><api:text>Published</api:text></api:field><api:field name="title" type="text" display-name="Title"><api:text>Strategically Efficient Exploration in Competitive Multi-agent Reinforcement Learning</api:text></api:field></api:native></api:record></api:records><api:fields/><api:relationships href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/publications/1589690/relationships"/></api:object></api:result></api:response>