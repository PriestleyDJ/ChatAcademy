<?xml version="1.0" encoding="utf-8"?><api:response xmlns:api="http://www.symplectic.co.uk/publications/api"><api:version uri="https://mypublications.shef.ac.uk/" elements-version="6.17.0.4095" schema-version="6.13" product-name="myPublications"/><api:request href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/publications/1562827"/><api:result><api:object category="publication" id="1562827" last-affected-when="2024-03-31T17:01:53.323+01:00" last-modified-when="2024-03-31T17:01:53.323+01:00" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/publications/1562827" created-when="2023-10-11T15:03:52.753+01:00" type-id="4" type-display-name="Conference proceedings paper" type="conference"><api:privacy-level>Public</api:privacy-level><api:privacy-level-locked>false</api:privacy-level-locked><api:ever-approved>true</api:ever-approved><api:reporting-date-1>2024-01-19</api:reporting-date-1><api:allow-type-switching>true</api:allow-type-switching><api:records><api:record format="native" id="4364067" source-id="28" source-name="eprints" source-display-name="White Rose Research Online" id-at-source="204202" last-modified-when="2024-02-09T16:05:53.917+00:00"><api:native><api:field name="abstract" type="text" display-name="Abstract"><api:text>Future augmented reality devices have the capacity to enhance
human perception and provide assistive functions in complex communication scenarios. Active speaker detection (ASD) systems
that are robust to egocentric data are critical to this. Egocentric
ASD is challenging due to overlapping speech, single-channel
recording, and dynamic scenes. A novel module that uses a
data-efficient image transformer (DeiT) to extract features encapsulating the acoustic properties of each scene, and a positional
conditioning mechanism is proposed. The module is evaluated in
conjunction with TalkNet, an existing ASD architecture, on two
audiovisual datasets: Ego4D (egocentric) and AVA-ActiveSpeaker
(exocentric), achieving 29% and 0.38% relative improvement in
mean Average Precision (mAP), respectively, while retaining a
parameter efficient build. A qualitative analysis is also presented,
implicitly demonstrating that contextual information is leveraged</api:text></api:field><api:field name="acceptance-date" type="date" display-name="Date of acceptance"><api:date><api:day>22</api:day><api:month>9</api:month><api:year>2023</api:year></api:date></api:field><api:field name="authors" type="person-list" display-name="Authors"><api:people><api:person><api:links><api:link type="elements/user" id="32949" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/users/32949"/></api:links><api:last-name>Clarke</api:last-name><api:initials>J</api:initials><api:first-names>J</api:first-names><api:separate-first-names><api:first-name>J</api:first-name></api:separate-first-names></api:person><api:person><api:links><api:link type="elements/user" id="28" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/users/28"/></api:links><api:last-name>Gotoh</api:last-name><api:initials>Y</api:initials><api:first-names>Y</api:first-names><api:separate-first-names><api:first-name>Y</api:first-name></api:separate-first-names></api:person><api:person><api:links><api:link type="elements/user" id="30214" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/users/30214"/></api:links><api:last-name>Goetze</api:last-name><api:initials>S</api:initials><api:first-names>S</api:first-names><api:separate-first-names><api:first-name>S</api:first-name></api:separate-first-names></api:person></api:people></api:field><api:field name="c-data-accessibility" type="text" display-name="Data Accessibility"><api:text>DataN</api:text></api:field><api:field name="c-data-availability-statement" type="boolean" display-name="Data Availability Statement"><api:boolean>false</api:boolean></api:field><api:field name="c-goldoa" type="boolean" display-name="Gold Open Access?"><api:boolean>false</api:boolean></api:field><api:field name="c-licence-statement" type="boolean" display-name="Licence statement?"><api:boolean>false</api:boolean></api:field><api:field name="c-ref-no-issn" type="boolean" display-name="REF No ISSN"><api:boolean>false</api:boolean></api:field><api:field name="c-rights-retention-opt-out" type="boolean" display-name="Rights retention opt out"><api:boolean>false</api:boolean></api:field><api:field name="doi" type="text" display-name="DOI"><api:text>10.1109/ASRU57964.2023.10389764</api:text><api:links><api:link type="doi" href="http://doi.org/10.1109/ASRU57964.2023.10389764"/><api:link type="altmetric" href="http://www.altmetric.com/details.php?doi=10.1109/ASRU57964.2023.10389764"/></api:links></api:field><api:field name="finish-date" type="date" display-name="Conference finish date"><api:date><api:day>16</api:day><api:month>12</api:month><api:year>2023</api:year></api:date></api:field><api:field name="isbn-13" type="text" display-name="ISBN-13"><api:text>979-8-3503-0690-3</api:text></api:field><api:field name="journal" type="text" display-name="Title of published proceedings"><api:text>2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</api:text></api:field><api:field name="keywords" type="keyword-list" display-name="Keywords"><api:keywords><api:keyword>Active speaker detection</api:keyword><api:keyword>context modelling</api:keyword><api:keyword>data-efficient image transformers</api:keyword></api:keywords></api:field><api:field name="location" type="text" display-name="Conference place"><api:text>Taipei, Taiwan</api:text></api:field><api:field name="name-of-conference" type="text" display-name="Name of conference"><api:text>IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU 2023),</api:text></api:field><api:field name="notes" type="text" display-name="Other information"><api:text>Â© 2023 The Authors. Except as otherwise noted, this author-accepted version of a proceedings paper is published in 2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU) is made available via the University of Sheffield Research Publications and Copyright Policy under the terms of the Creative Commons Attribution 4.0 International License (CC-BY 4.0), which permits unrestricted use, distribution and reproduction in any medium, provided the original work is properly cited. To view a copy of this licence, visit  http://creativecommons.org/licenses/by/4.0/</api:text></api:field><api:field name="online-publication-date" type="date" display-name="Online publication date"><api:date><api:day>19</api:day><api:month>1</api:month><api:year>2024</api:year></api:date></api:field><api:field name="publication-date" type="date" display-name="Publication date"><api:date><api:day>19</api:day><api:month>1</api:month><api:year>2024</api:year></api:date></api:field><api:field name="publication-status" type="text" display-name="Status"><api:text>Published</api:text></api:field><api:field name="public-url" type="text" display-name="Public URL"><api:text>https://eprints.whiterose.ac.uk/id/eprint/204202</api:text></api:field><api:field name="publisher" type="text" display-name="Publisher"><api:text>Institute of Electrical and Electronics Engineers (IEEE)</api:text></api:field><api:field name="record-created-at-source-date" type="date" display-name="Record created at source"><api:date><api:day>12</api:day><api:month>10</api:month><api:year>2023</api:year></api:date></api:field><api:field name="record-made-public-at-source-date" type="date" display-name="Record made publicly available"><api:date><api:day>12</api:day><api:month>10</api:month><api:year>2023</api:year></api:date></api:field><api:field name="repository-status" type="text" display-name="Availability"><api:text>Public</api:text></api:field><api:field name="start-date" type="date" display-name="Conference start date"><api:date><api:day>16</api:day><api:month>12</api:month><api:year>2023</api:year></api:date></api:field><api:field name="title" type="text" display-name="Title"><api:text>Improving audiovisual active speaker detection in egocentric recordings with the data-efficient image transformer</api:text></api:field><api:files><api:file proprietary-id="https://eprints.whiterose.ac.uk/id/file/6704710"><api:filename>ASRU_AV-ASD_DeiT_cr.pdf</api:filename><api:file-url>https://eprints.whiterose.ac.uk/204202/1/ASRU_AV-ASD_DeiT_cr.pdf</api:file-url><api:extension>pdf</api:extension><api:file-size>3658273</api:file-size><api:mime-type>application/pdf</api:mime-type><api:checksum algorithm="md5">5BB3E5858037BAF89C536E5C722ED2BA</api:checksum><api:embargo-release-date>2024-02-06</api:embargo-release-date><api:file-version>Author accepted manuscript</api:file-version></api:file></api:files></api:native></api:record><api:record format="native" id="4531579" source-id="13" source-name="crossref" source-display-name="Crossref" id-at-source="10.1109/asru57964.2023.10389764" last-modified-when="2024-02-14T10:13:48.203+00:00"><api:native><api:field name="authors" type="person-list" display-name="Authors"><api:people><api:person><api:links><api:link type="elements/user" id="32949" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/users/32949"/></api:links><api:last-name>Clarke</api:last-name><api:initials>J</api:initials><api:first-names>Jason</api:first-names><api:separate-first-names><api:first-name>Jason</api:first-name></api:separate-first-names></api:person><api:person><api:links><api:link type="elements/user" id="28" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/users/28"/></api:links><api:last-name>Gotoh</api:last-name><api:initials>Y</api:initials><api:first-names>Yoshihiko</api:first-names><api:separate-first-names><api:first-name>Yoshihiko</api:first-name></api:separate-first-names></api:person><api:person><api:links><api:link type="elements/user" id="30214" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/users/30214"/></api:links><api:last-name>Goetze</api:last-name><api:initials>S</api:initials><api:first-names>Stefan</api:first-names><api:separate-first-names><api:first-name>Stefan</api:first-name></api:separate-first-names></api:person></api:people></api:field><api:field name="doi" type="text" display-name="DOI"><api:text>10.1109/asru57964.2023.10389764</api:text><api:links><api:link type="doi" href="http://doi.org/10.1109/asru57964.2023.10389764"/><api:link type="altmetric" href="http://www.altmetric.com/details.php?doi=10.1109/asru57964.2023.10389764"/></api:links></api:field><api:field name="finish-date" type="date" display-name="Conference finish date"><api:date><api:day>20</api:day><api:month>12</api:month><api:year>2023</api:year></api:date></api:field><api:field name="journal" type="text" display-name="Title of published proceedings"><api:text>2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</api:text></api:field><api:field name="name-of-conference" type="text" display-name="Name of conference"><api:text>2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</api:text></api:field><api:field name="publication-date" type="date" display-name="Publication date"><api:date><api:day>16</api:day><api:month>12</api:month><api:year>2023</api:year></api:date></api:field><api:field name="publication-status" type="text" display-name="Status"><api:text>Published</api:text></api:field><api:field name="publisher" type="text" display-name="Publisher"><api:text>IEEE</api:text></api:field><api:field name="publisher-url" type="text" display-name="Link 2"><api:text>http://dx.doi.org/10.1109/asru57964.2023.10389764</api:text></api:field><api:field name="record-created-at-source-date" type="date" display-name="Record created at source"><api:date><api:day>23</api:day><api:month>1</api:month><api:year>2024</api:year></api:date></api:field><api:field name="start-date" type="date" display-name="Conference start date"><api:date><api:day>16</api:day><api:month>12</api:month><api:year>2023</api:year></api:date></api:field><api:field name="title" type="text" display-name="Title"><api:text>Improving Audiovisual Active Speaker Detection in Egocentric Recordings with the Data-Efficient Image Transformer</api:text></api:field></api:native></api:record><api:record format="native" id="4544178" source-id="7" source-name="scopus" source-display-name="Scopus" id-at-source="2-s2.0-85184657983" last-modified-when="2024-03-07T05:42:04.827+00:00"><api:citation-count>0</api:citation-count><api:native><api:field name="abstract" type="text" display-name="Abstract"><api:text>Future augmented reality devices have the capacity to enhance human perception and provide assistive functions in complex communication scenarios. Active speaker detection (ASD) systems that are robust to egocentric data are critical to this. Egocentric ASD is challenging due to overlapping speech, single-channel recording, and dynamic scenes. A novel module that uses a data-efficient image transformer (DeiT) to extract features encapsulating the acoustic properties of each scene, and a positional conditioning mechanism is proposed. The module is evaluated in conjunction with TalkNet, an existing ASD architecture, on two audiovisual datasets: Ego4D (egocentric) and AVA-ActiveSpeaker (exocentric), achieving 29% and 0.38% relative improvement in mean Average Precision (mAP), respectively, while retaining a parameter efficient build. A qualitative analysis is also presented, implicitly demonstrating that contextual information is leveraged.</api:text></api:field><api:field name="authors" type="person-list" display-name="Authors"><api:people><api:person><api:links><api:link type="elements/user" id="32949" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/users/32949"/></api:links><api:last-name>Clarke</api:last-name><api:initials>J</api:initials><api:first-names>J</api:first-names><api:separate-first-names><api:first-name>J</api:first-name></api:separate-first-names><api:addresses><api:address iso-country-code="GB"><api:line type="organisation">The University of Sheffield</api:line><api:line type="city">Sheffield</api:line><api:line type="country">United Kingdom</api:line></api:address></api:addresses><api:identifiers><api:identifier scheme="scopus-author-id">58876430400</api:identifier></api:identifiers></api:person><api:person><api:links><api:link type="elements/user" id="28" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/users/28"/></api:links><api:last-name>Gotoh</api:last-name><api:initials>Y</api:initials><api:first-names>Y</api:first-names><api:separate-first-names><api:first-name>Y</api:first-name></api:separate-first-names><api:addresses><api:address iso-country-code="GB"><api:line type="organisation">The University of Sheffield</api:line><api:line type="city">Sheffield</api:line><api:line type="country">United Kingdom</api:line></api:address></api:addresses><api:identifiers><api:identifier scheme="scopus-author-id">35592218100</api:identifier></api:identifiers></api:person><api:person><api:links><api:link type="elements/user" id="30214" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/users/30214"/></api:links><api:last-name>Goetze</api:last-name><api:initials>S</api:initials><api:first-names>S</api:first-names><api:separate-first-names><api:first-name>S</api:first-name></api:separate-first-names><api:addresses><api:address iso-country-code="GB"><api:line type="organisation">The University of Sheffield</api:line><api:line type="city">Sheffield</api:line><api:line type="country">United Kingdom</api:line></api:address></api:addresses><api:identifiers><api:identifier scheme="scopus-author-id">21833521500</api:identifier></api:identifiers></api:person></api:people></api:field><api:field name="doi" type="text" display-name="DOI"><api:text>10.1109/ASRU57964.2023.10389764</api:text><api:links><api:link type="doi" href="http://doi.org/10.1109/ASRU57964.2023.10389764"/><api:link type="altmetric" href="http://www.altmetric.com/details.php?doi=10.1109/ASRU57964.2023.10389764"/></api:links></api:field><api:field name="isbn-13" type="text" display-name="ISBN-13"><api:text>9798350306897</api:text></api:field><api:field name="journal" type="text" display-name="Title of published proceedings"><api:text>2023 IEEE Automatic Speech Recognition and Understanding Workshop, ASRU 2023</api:text></api:field><api:field name="publication-date" type="date" display-name="Publication date"><api:date><api:day>1</api:day><api:month>1</api:month><api:year>2023</api:year></api:date></api:field><api:field name="publication-status" type="text" display-name="Status"><api:text>Published</api:text></api:field><api:field name="title" type="text" display-name="Title"><api:text>Improving Audiovisual Active Speaker Detection in Egocentric Recordings with the Data-Efficient Image Transformer</api:text></api:field></api:native></api:record><api:record format="native" id="4531572" source-id="10" source-name="dimensions" source-display-name="Dimensions" id-at-source="pub.1168098037" last-modified-when="2024-03-13T10:13:50.533+00:00"><api:citation-count>0</api:citation-count><api:native><api:field name="authors" type="person-list" display-name="Authors"><api:people><api:person><api:links><api:link type="elements/user" id="32949" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/users/32949"/></api:links><api:last-name>Clarke</api:last-name><api:initials>J</api:initials><api:first-names>Jason</api:first-names><api:separate-first-names><api:first-name>Jason</api:first-name></api:separate-first-names><api:addresses><api:address iso-country-code="GB"><api:line type="organisation">University of Sheffield</api:line><api:line type="city">Sheffield</api:line><api:line type="country">United Kingdom</api:line></api:address></api:addresses><api:author-types><api:author-type>corresponding</api:author-type></api:author-types></api:person><api:person><api:links><api:link type="elements/user" id="28" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/users/28"/></api:links><api:last-name>Gotoh</api:last-name><api:initials>Y</api:initials><api:first-names>Yoshihiko</api:first-names><api:separate-first-names><api:first-name>Yoshihiko</api:first-name></api:separate-first-names><api:addresses><api:address iso-country-code="GB"><api:line type="organisation">University of Sheffield</api:line><api:line type="city">Sheffield</api:line><api:line type="country">United Kingdom</api:line></api:address></api:addresses></api:person><api:person><api:links><api:link type="elements/user" id="30214" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/users/30214"/></api:links><api:last-name>Goetze</api:last-name><api:initials>S</api:initials><api:first-names>Stefan</api:first-names><api:separate-first-names><api:first-name>Stefan</api:first-name></api:separate-first-names><api:addresses><api:address iso-country-code="GB"><api:line type="organisation">University of Sheffield</api:line><api:line type="city">Sheffield</api:line><api:line type="country">United Kingdom</api:line></api:address></api:addresses><api:identifiers><api:identifier scheme="dimensions-researcher-id">ur.011701236142.18</api:identifier></api:identifiers></api:person></api:people></api:field><api:field name="doi" type="text" display-name="DOI"><api:text>10.1109/asru57964.2023.10389764</api:text><api:links><api:link type="doi" href="http://doi.org/10.1109/asru57964.2023.10389764"/><api:link type="altmetric" href="http://www.altmetric.com/details.php?doi=10.1109/asru57964.2023.10389764"/></api:links></api:field><api:field name="keywords" type="keyword-list" display-name="Keywords"><api:keywords><api:keyword scheme="for-2020">46 Information and Computing Sciences</api:keyword><api:keyword scheme="for-2020">40 Engineering</api:keyword><api:keyword scheme="for-2020">4603 Computer Vision and Multimedia Computation</api:keyword></api:keywords></api:field><api:field name="name-of-conference" type="text" display-name="Name of conference"><api:text>2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</api:text></api:field><api:field name="open-access-status" type="text" display-name="Open access status"><api:text>Closed Access</api:text></api:field><api:field name="pagination" type="pagination" display-name="Pagination"><api:pagination><api:begin-page>1</api:begin-page><api:end-page>8</api:end-page></api:pagination></api:field><api:field name="publication-date" type="date" display-name="Publication date"><api:date><api:day>20</api:day><api:month>12</api:month><api:year>2023</api:year></api:date></api:field><api:field name="publisher" type="text" display-name="Publisher"><api:text>Institute of Electrical and Electronics Engineers (IEEE)</api:text></api:field><api:field name="record-created-at-source-date" type="date" display-name="Record created at source"><api:date><api:day>21</api:day><api:month>1</api:month><api:year>2024</api:year></api:date></api:field><api:field name="title" type="text" display-name="Title"><api:text>Improving Audiovisual Active Speaker Detection in Egocentric Recordings with the Data-Efficient Image Transformer</api:text></api:field><api:field name="volume" type="text" display-name="Volume"><api:text>00</api:text></api:field></api:native></api:record><api:record format="native" id="4363942" source-id="1" source-name="manual" source-display-name="Manual" id-at-source="93BDB646-DB5B-4D3E-B455-EE06EC283462" last-modified-when="2023-10-11T15:03:52.757+01:00"><api:verification-status>unverified</api:verification-status><api:native><api:field name="acceptance-date" type="date" display-name="Date of acceptance"><api:date><api:day>22</api:day><api:month>9</api:month><api:year>2023</api:year></api:date></api:field><api:field name="authors" type="person-list" display-name="Authors"><api:people><api:person><api:links><api:link type="elements/user" id="32949" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/users/32949"/></api:links><api:last-name>Clarke</api:last-name><api:initials>J</api:initials><api:first-names>Jason</api:first-names><api:separate-first-names><api:first-name>Jason</api:first-name></api:separate-first-names></api:person><api:person><api:links><api:link type="elements/user" id="28" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/users/28"/></api:links><api:last-name>Gotoh</api:last-name><api:initials>Y</api:initials><api:first-names>Yoshihiko</api:first-names><api:separate-first-names><api:first-name>Yoshihiko</api:first-name></api:separate-first-names><api:author-types><api:author-type>last</api:author-type></api:author-types></api:person><api:person><api:links><api:link type="elements/user" id="30214" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/users/30214"/></api:links><api:last-name>Goetze</api:last-name><api:initials>S</api:initials><api:first-names>Stefan</api:first-names><api:separate-first-names><api:first-name>Stefan</api:first-name></api:separate-first-names><api:author-types><api:author-type>last</api:author-type></api:author-types></api:person></api:people></api:field><api:field name="c-data-availability-statement" type="boolean" display-name="Data Availability Statement"><api:boolean>false</api:boolean></api:field><api:field name="c-licence-statement" type="boolean" display-name="Licence statement?"><api:boolean>false</api:boolean></api:field><api:field name="c-ref-no-issn" type="boolean" display-name="REF No ISSN"><api:boolean>false</api:boolean></api:field><api:field name="c-ref-pre-2014" type="boolean" display-name="REF pre-2014"><api:boolean>false</api:boolean></api:field><api:field name="c-rights-retention-opt-out" type="boolean" display-name="Rights retention opt out"><api:boolean>false</api:boolean></api:field><api:field name="journal" type="text" display-name="Title of published proceedings"><api:text>Proc. IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU 2023)</api:text></api:field><api:field name="name-of-conference" type="text" display-name="Name of conference"><api:text>IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU 2023),</api:text></api:field><api:field name="publication-status" type="text" display-name="Status"><api:text>Accepted</api:text></api:field><api:field name="record-created-at-source-date" type="date" display-name="Record created at source"><api:date><api:day>11</api:day><api:month>10</api:month><api:year>2023</api:year></api:date></api:field><api:field name="title" type="text" display-name="Title"><api:text>IMPROVING AUDIOVISUAL ACTIVE SPEAKER DETECTION IN EGOCENTRIC RECORDINGS WITH THE DATA-EFFICIENT IMAGE TRANSFORMER</api:text></api:field></api:native></api:record></api:records><api:fields/><api:all-labels type="keyword-list"><api:keywords><api:keyword origin="record-data" source="eprints">Active speaker detection</api:keyword><api:keyword origin="record-data" source="eprints">context modelling</api:keyword><api:keyword origin="record-data" source="eprints">data-efficient image transformers</api:keyword><api:keyword scheme="for-2020" origin="record-data" source="dimensions">46 Information and Computing Sciences</api:keyword><api:keyword scheme="for-2020" origin="record-data" source="dimensions">40 Engineering</api:keyword><api:keyword scheme="for-2020" origin="record-data" source="dimensions">4603 Computer Vision and Multimedia Computation</api:keyword></api:keywords></api:all-labels><api:relationships href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/publications/1562827/relationships"/></api:object></api:result></api:response>