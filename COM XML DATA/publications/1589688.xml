<?xml version="1.0" encoding="utf-8"?><api:response xmlns:api="http://www.symplectic.co.uk/publications/api"><api:version uri="https://mypublications.shef.ac.uk/" elements-version="6.17.0.4095" schema-version="6.13" product-name="myPublications"/><api:request href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/publications/1589688"/><api:result><api:object category="publication" id="1589688" last-affected-when="2024-02-16T14:28:12.417+00:00" last-modified-when="2024-02-16T14:28:12.417+00:00" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/publications/1589688" created-when="2024-01-31T10:37:36.49+00:00" type-id="4" type-display-name="Conference proceedings paper" type="conference"><api:privacy-level>Public</api:privacy-level><api:privacy-level-locked>false</api:privacy-level-locked><api:ever-approved>true</api:ever-approved><api:reporting-date-1>2021-07-30</api:reporting-date-1><api:allow-type-switching>true</api:allow-type-switching><api:records><api:record format="native" id="4540544" source-id="28" source-name="eprints" source-display-name="White Rose Research Online" id-at-source="209128" last-modified-when="2024-02-16T14:28:12.42+00:00"><api:native><api:field name="abstract" type="text" display-name="Abstract"><api:text>High sample complexity remains a barrier to the application of reinforcement learning (RL), particularly in multi-agent systems. A large body of work has demonstrated that exploration mechanisms based on the principle of optimism under uncertainty can significantly improve the sample efficiency of RL in single agent tasks. This work seeks to understand the role of optimistic exploration in non-cooperative multi-agent settings. We will show that, in zero-sum games, optimistic exploration can cause the learner to waste time sampling parts of the state space that are irrelevant to strategic play, as they can only be reached through cooperation between both players. To address this issue, we introduce a formal notion of strategically efficient exploration in Markov games, and use this to develop two strategically efficient learning algorithms for finite Markov games. We demonstrate that these methods can be significantly more sample efficient than their optimistic counterparts.</api:text></api:field><api:field name="authors" type="person-list" display-name="Authors"><api:people><api:person><api:links><api:link type="elements/user" id="37828" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/users/37828"/></api:links><api:last-name>Loftin</api:last-name><api:initials>R</api:initials><api:first-names>R</api:first-names><api:separate-first-names><api:first-name>R</api:first-name></api:separate-first-names></api:person><api:person><api:last-name>Saha</api:last-name><api:initials>A</api:initials><api:first-names>A</api:first-names><api:separate-first-names><api:first-name>A</api:first-name></api:separate-first-names></api:person><api:person><api:last-name>Devlin</api:last-name><api:initials>S</api:initials><api:first-names>S</api:first-names><api:separate-first-names><api:first-name>S</api:first-name></api:separate-first-names></api:person><api:person><api:last-name>Hofmann</api:last-name><api:initials>K</api:initials><api:first-names>K</api:first-names><api:separate-first-names><api:first-name>K</api:first-name></api:separate-first-names></api:person></api:people></api:field><api:field name="c-data-accessibility" type="text" display-name="Data Accessibility"><api:text>Data2</api:text></api:field><api:field name="c-data-availability-statement" type="boolean" display-name="Data Availability Statement"><api:boolean>false</api:boolean></api:field><api:field name="c-goldoa" type="boolean" display-name="Gold Open Access?"><api:boolean>false</api:boolean></api:field><api:field name="c-licence-statement" type="boolean" display-name="Licence statement?"><api:boolean>false</api:boolean></api:field><api:field name="c-ref-no-issn" type="boolean" display-name="REF No ISSN"><api:boolean>false</api:boolean></api:field><api:field name="c-rights-retention-opt-out" type="boolean" display-name="Rights retention opt out"><api:boolean>false</api:boolean></api:field><api:field name="eissn" type="text" display-name="eISSN"><api:text>2640-3498</api:text></api:field><api:field name="finish-date" type="date" display-name="Conference finish date"><api:date><api:day>27</api:day><api:month>7</api:month><api:year>2021</api:year></api:date></api:field><api:field name="journal" type="text" display-name="Title of published proceedings"><api:text>Proceedings of the Thirty-Seventh Conference on Uncertainty in Artificial Intelligence (UAI 2021)</api:text></api:field><api:field name="location" type="text" display-name="Conference place"><api:text>Online</api:text></api:field><api:field name="name-of-conference" type="text" display-name="Name of conference"><api:text>37th Conference on Uncertainty in Artificial Intelligence</api:text></api:field><api:field name="notes" type="text" display-name="Other information"><api:text>Â© 2021 The Authors.</api:text></api:field><api:field name="oa-location-file-version" type="text" display-name="OA location file version"><api:text>Published version</api:text></api:field><api:field name="oa-location-url" type="text" display-name="OA location URL"><api:text>https://proceedings.mlr.press/v161/loftin21a/loftin21a.pdf</api:text></api:field><api:field name="pagination" type="pagination" display-name="Pagination"><api:pagination><api:begin-page>1587</api:begin-page><api:end-page>1596</api:end-page></api:pagination></api:field><api:field name="publication-date" type="date" display-name="Publication date"><api:date><api:day>30</api:day><api:month>7</api:month><api:year>2021</api:year></api:date></api:field><api:field name="publication-status" type="text" display-name="Status"><api:text>Published</api:text></api:field><api:field name="public-url" type="text" display-name="Public URL"><api:text>https://eprints.whiterose.ac.uk/id/eprint/209128</api:text></api:field><api:field name="publisher" type="text" display-name="Publisher"><api:text>ML Research Press</api:text></api:field><api:field name="publisher-url" type="text" display-name="Link 2"><api:text>https://proceedings.mlr.press/v161/loftin21a.html</api:text></api:field><api:field name="record-created-at-source-date" type="date" display-name="Record created at source"><api:date><api:day>13</api:day><api:month>2</api:month><api:year>2024</api:year></api:date></api:field><api:field name="record-made-public-at-source-date" type="date" display-name="Record made publicly available"><api:date><api:day>16</api:day><api:month>2</api:month><api:year>2024</api:year></api:date></api:field><api:field name="repository-status" type="text" display-name="Availability"><api:text>Public</api:text></api:field><api:field name="start-date" type="date" display-name="Conference start date"><api:date><api:day>27</api:day><api:month>7</api:month><api:year>2021</api:year></api:date></api:field><api:field name="title" type="text" display-name="Title"><api:text>Strategically efficient exploration in competitive multi-agent reinforcement learning</api:text></api:field><api:field name="volume" type="text" display-name="Volume"><api:text>161</api:text></api:field></api:native></api:record><api:record format="native" id="4531630" source-id="7" source-name="scopus" source-display-name="Scopus" id-at-source="2-s2.0-85163372668" last-modified-when="2024-02-14T10:37:37.46+00:00"><api:citation-count>1</api:citation-count><api:native><api:field name="abstract" type="text" display-name="Abstract"><api:text>High sample complexity remains a barrier to the application of reinforcement learning (RL), particularly in multi-agent systems. A large body of work has demonstrated that exploration mechanisms based on the principle of optimism under uncertainty can significantly improve the sample efficiency of RL in single agent tasks. This work seeks to understand the role of optimistic exploration in non-cooperative multi-agent settings. We will show that, in zero-sum games, optimistic exploration can cause the learner to waste time sampling parts of the state space that are irrelevant to strategic play, as they can only be reached through cooperation between both players. To address this issue, we introduce a formal notion of strategically efficient exploration in Markov games, and use this to develop two strategically efficient learning algorithms for finite Markov games. We demonstrate that these methods can be significantly more sample efficient than their optimistic counterparts.</api:text></api:field><api:field name="authors" type="person-list" display-name="Authors"><api:people><api:person><api:links><api:link type="elements/user" id="37828" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/users/37828"/></api:links><api:last-name>Loftin</api:last-name><api:initials>R</api:initials><api:first-names>R</api:first-names><api:separate-first-names><api:first-name>R</api:first-name></api:separate-first-names><api:addresses><api:address iso-country-code="GB"><api:line type="organisation">Microsoft Research Cambridge</api:line><api:line type="city">Cambridge</api:line><api:line type="country">United Kingdom</api:line></api:address></api:addresses><api:identifiers><api:identifier scheme="scopus-author-id">58514845500</api:identifier></api:identifiers></api:person><api:person><api:last-name>Saha</api:last-name><api:initials>A</api:initials><api:first-names>A</api:first-names><api:separate-first-names><api:first-name>A</api:first-name></api:separate-first-names><api:addresses><api:address iso-country-code="US"><api:line type="organisation">Microsoft Research</api:line><api:line type="city">Redmond</api:line><api:line type="country">United States</api:line></api:address></api:addresses><api:identifiers><api:identifier scheme="scopus-author-id">56555874900</api:identifier></api:identifiers></api:person><api:person><api:last-name>Devlin</api:last-name><api:initials>S</api:initials><api:first-names>S</api:first-names><api:separate-first-names><api:first-name>S</api:first-name></api:separate-first-names><api:addresses><api:address iso-country-code="GB"><api:line type="organisation">Microsoft Research Cambridge</api:line><api:line type="city">Cambridge</api:line><api:line type="country">United Kingdom</api:line></api:address></api:addresses><api:identifiers><api:identifier scheme="scopus-author-id">37107376400</api:identifier></api:identifiers></api:person><api:person><api:last-name>Hofmann</api:last-name><api:initials>K</api:initials><api:first-names>K</api:first-names><api:separate-first-names><api:first-name>K</api:first-name></api:separate-first-names><api:addresses><api:address iso-country-code="GB"><api:line type="organisation">Microsoft Research Cambridge</api:line><api:line type="city">Cambridge</api:line><api:line type="country">United Kingdom</api:line></api:address></api:addresses><api:identifiers><api:identifier scheme="scopus-author-id">23011906300</api:identifier></api:identifiers></api:person></api:people></api:field><api:field name="eissn" type="text" display-name="eISSN"><api:text>2640-3498</api:text></api:field><api:field name="journal" type="text" display-name="Title of published proceedings"><api:text>Proceedings of Machine Learning Research</api:text></api:field><api:field name="pagination" type="pagination" display-name="Pagination"><api:pagination><api:begin-page>1587</api:begin-page><api:end-page>1596</api:end-page></api:pagination></api:field><api:field name="publication-date" type="date" display-name="Publication date"><api:date><api:day>1</api:day><api:month>1</api:month><api:year>2021</api:year></api:date></api:field><api:field name="publication-status" type="text" display-name="Status"><api:text>Published</api:text></api:field><api:field name="title" type="text" display-name="Title"><api:text>Strategically Efficient Exploration in Competitive Multi-agent Reinforcement Learning</api:text></api:field><api:field name="volume" type="text" display-name="Volume"><api:text>161</api:text></api:field></api:native></api:record></api:records><api:fields/><api:relationships href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/publications/1589688/relationships"/><api:last-flagged-as-grant-not-listed>2024-02-13T09:39:36.663+00:00</api:last-flagged-as-grant-not-listed></api:object></api:result></api:response>