<?xml version="1.0" encoding="utf-8"?><api:response xmlns:api="http://www.symplectic.co.uk/publications/api"><api:version uri="https://mypublications.shef.ac.uk/" elements-version="6.17.0.4095" schema-version="6.13" product-name="myPublications"/><api:request href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/publications/1524987"/><api:result><api:object category="publication" id="1524987" last-affected-when="2024-04-25T07:28:10.127+01:00" last-modified-when="2024-04-25T07:28:10.127+01:00" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/publications/1524987" created-when="2023-02-02T10:31:15.37+00:00" type-id="5" type-display-name="Journal article" type="journal-article"><api:privacy-level>Public</api:privacy-level><api:privacy-level-locked>false</api:privacy-level-locked><api:ever-approved>true</api:ever-approved><api:reporting-date-1>2023-02-02</api:reporting-date-1><api:allow-type-switching>true</api:allow-type-switching><api:records><api:record format="native" id="4260523" source-id="28" source-name="eprints" source-display-name="White Rose Research Online" id-at-source="195981" last-modified-when="2024-02-02T02:05:58.263+00:00"><api:native><api:field name="abstract" type="text" display-name="Abstract"><api:text>Dense captioning generates more detailed spoken
descriptions for complex visual scenes. Despite several promising
leads, existing methods still have two broad limitations: 1)
The vast majority of prior arts only consider visual contextual
clues during captioning but ignore potentially important textual
context; 2) current imbalanced learning mechanisms limit the
diversity of vocabulary learned from the dictionary, thus giving
rise to low language-learning efficiency. To alleviate these gaps, in
this paper, we propose an end-to-end enhanced dense captioning
architecture, namely Enhanced Transformer Dense Captioner
(ETDC), which obtains textual context from surrounding regions
and dynamically diversifies the vocabulary bank during captioning. Concretely, we first propose the Textual Context Module
(TCM), which is integrated into each self-attention layer of the
Transformer decoder, to capture the surrounding textual context.
Moreover, we take full advantage of the class information of
object context and propose a Dynamic Vocabulary Frequency
Histogram (DVFH) re-sampling strategy during training to
balance words with different frequencies. The proposed method
is tested on the standard dense captioning datasets and surpasses
the state-of-the-art methods in terms of mean Average Precision
(mAP).</api:text></api:field><api:field name="acceptance-date" type="date" display-name="Date of acceptance"><api:date><api:day>15</api:day><api:month>1</api:month><api:year>2023</api:year></api:date></api:field><api:field name="authors" type="person-list" display-name="Authors"><api:people><api:person><api:last-name>Shao</api:last-name><api:initials>Z</api:initials><api:first-names>Z</api:first-names><api:separate-first-names><api:first-name>Z</api:first-name></api:separate-first-names></api:person><api:person><api:links><api:link type="elements/user" id="36768" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/users/36768"/></api:links><api:last-name>Han</api:last-name><api:initials>J</api:initials><api:first-names>J</api:first-names><api:separate-first-names><api:first-name>J</api:first-name></api:separate-first-names></api:person><api:person><api:last-name>Debattista</api:last-name><api:initials>K</api:initials><api:first-names>K</api:first-names><api:separate-first-names><api:first-name>K</api:first-name></api:separate-first-names></api:person><api:person><api:last-name>Pang</api:last-name><api:initials>Y</api:initials><api:first-names>Y</api:first-names><api:separate-first-names><api:first-name>Y</api:first-name></api:separate-first-names></api:person></api:people></api:field><api:field name="doi" type="text" display-name="DOI"><api:text>10.1109/tmm.2023.3241517</api:text><api:links><api:link type="doi" href="http://doi.org/10.1109/tmm.2023.3241517"/><api:link type="altmetric" href="http://www.altmetric.com/details.php?doi=10.1109/tmm.2023.3241517"/></api:links></api:field><api:field name="eissn" type="text" display-name="eISSN"><api:text>1941-0077</api:text><api:links><api:link type="elements/journal" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/journals/974865"/></api:links></api:field><api:field name="issn" type="text" display-name="ISSN"><api:text>1520-9210</api:text><api:links><api:link type="elements/journal" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/journals/974865"/></api:links></api:field><api:field name="journal" type="text" display-name="Journal"><api:text>IEEE Transactions on Multimedia</api:text></api:field><api:field name="keywords" type="keyword-list" display-name="Keywords"><api:keywords><api:keyword>Dense Captioning</api:keyword><api:keyword>Enhanced Transformer Dense Captioner</api:keyword><api:keyword>Textual Context Module</api:keyword><api:keyword>Dynamic Vocabulary Frequency Histogram</api:keyword></api:keywords></api:field><api:field name="notes" type="text" display-name="Other information"><api:text>Â© 2023 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other users, including reprinting/ republishing this material for advertising or promotional purposes, creating new collective works for resale or redistribution to servers or lists, or reuse of any copyrighted components of this work in other works. Reproduced in accordance with the publisher's self-archiving policy.</api:text></api:field><api:field name="online-publication-date" type="date" display-name="Online publication date"><api:date><api:day>2</api:day><api:month>2</api:month><api:year>2023</api:year></api:date></api:field><api:field name="publication-status" type="text" display-name="Status"><api:text>Published online</api:text></api:field><api:field name="public-url" type="text" display-name="Public URL"><api:text>https://eprints.whiterose.ac.uk/id/eprint/195981</api:text></api:field><api:field name="publisher" type="text" display-name="Publisher"><api:text>Institute of Electrical and Electronics Engineers</api:text></api:field><api:field name="record-created-at-source-date" type="date" display-name="Record created at source"><api:date><api:day>2</api:day><api:month>2</api:month><api:year>2023</api:year></api:date></api:field><api:field name="record-made-public-at-source-date" type="date" display-name="Record made publicly available"><api:date><api:day>2</api:day><api:month>2</api:month><api:year>2023</api:year></api:date></api:field><api:field name="repository-status" type="text" display-name="Availability"><api:text>Public</api:text></api:field><api:field name="title" type="text" display-name="Title"><api:text>Textual context-aware dense captioning with diverse words</api:text></api:field><api:files><api:file proprietary-id="https://eprints.whiterose.ac.uk/id/file/6345783"><api:filename>TMM-JHan.pdf</api:filename><api:file-url>https://eprints.whiterose.ac.uk/195981/1/TMM-JHan.pdf</api:file-url><api:extension>pdf</api:extension><api:file-size>1084281</api:file-size><api:mime-type>application/pdf</api:mime-type><api:checksum algorithm="md5">57A45DC5D0E58768C161DEC72A3EBC38</api:checksum><api:embargo-release-date>2024-02-02</api:embargo-release-date><api:file-version>Author accepted manuscript</api:file-version></api:file></api:files></api:native></api:record><api:record format="native" id="4260522" source-id="1" source-name="manual" source-display-name="Manual" id-at-source="C82D36AE-9999-49A6-8172-68D0BD80CD08" last-modified-when="2023-02-23T16:09:17.363+00:00" is-locked="true"><api:verification-status>verified</api:verification-status><api:verification-comment>CD 23/02/2023</api:verification-comment><api:native><api:field name="abstract" type="text" display-name="Abstract"><api:text>Dense captioning generates more detailed spoken descriptions for complex visual scenes. Despite several promising leads, existing methods still have two broad limitations: 1) The vast majority of prior arts only consider visual contextual clues during captioning but ignore potentially important textual context; 2) current imbalanced learning mechanisms limit the diversity of vocabulary learned from the dictionary, thus giving rise to low language-learning efficiency. To alleviate these gaps, in this paper, we propose an end-to-end enhanced dense captioning architecture, namely Enhanced Transformer Dense Captioner (ETDC), which obtains textual context from surrounding regions and dynamically diversifies the vocabulary bank during captioning. Concretely, we first propose the Textual Context Module (TCM), which is integrated into each self-attention layer of the Transformer decoder, to capture the surrounding textual context. Moreover, we take full advantage of the class information of object context and propose a Dynamic Vocabulary Frequency Histogram (DVFH) re-sampling strategy during training to balance words with different frequencies. The proposed method is tested on the standard dense captioning datasets and surpasses the state-of-the-art methods in terms of mean Average Precision (mAP).</api:text></api:field><api:field name="acceptance-date" type="date" display-name="Date of acceptance"><api:date><api:day>15</api:day><api:month>1</api:month><api:year>2023</api:year></api:date></api:field><api:field name="authors" type="person-list" display-name="Authors"><api:people><api:person><api:last-name>Shao</api:last-name><api:initials>Z</api:initials><api:first-names>Zhuang</api:first-names><api:separate-first-names><api:first-name>Zhuang</api:first-name></api:separate-first-names><api:author-types><api:author-type>first</api:author-type></api:author-types></api:person><api:person><api:links><api:link type="elements/user" id="36768" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/users/36768"/></api:links><api:last-name>Han</api:last-name><api:initials>J</api:initials><api:first-names>Jungong</api:first-names><api:separate-first-names><api:first-name>Jungong</api:first-name></api:separate-first-names><api:author-types><api:author-type>corresponding</api:author-type></api:author-types><api:roles><api:role type="contributor">Supervision</api:role></api:roles></api:person><api:person><api:last-name>Debattista</api:last-name><api:initials>K</api:initials><api:first-names>Kurt</api:first-names><api:separate-first-names><api:first-name>Kurt</api:first-name></api:separate-first-names></api:person><api:person><api:last-name>Pang</api:last-name><api:initials>Y</api:initials><api:first-names>Yanwei</api:first-names><api:separate-first-names><api:first-name>Yanwei</api:first-name></api:separate-first-names></api:person></api:people></api:field><api:field name="c-data-accessibility" type="text" display-name="Data Accessibility"><api:text>DataN</api:text></api:field><api:field name="c-data-availability-statement" type="boolean" display-name="Data Availability Statement"><api:boolean>false</api:boolean></api:field><api:field name="c-goldoa" type="boolean" display-name="Gold Open Access?"><api:boolean>false</api:boolean></api:field><api:field name="c-licence-statement" type="boolean" display-name="Licence statement?"><api:boolean>false</api:boolean></api:field><api:field name="c-ref-no-issn" type="boolean" display-name="REF No ISSN"><api:boolean>false</api:boolean></api:field><api:field name="c-ref-pre-2014" type="boolean" display-name="REF pre-2014"><api:boolean>false</api:boolean></api:field><api:field name="doi" type="text" display-name="DOI"><api:text>10.1109/tmm.2023.3241517</api:text><api:links><api:link type="doi" href="http://doi.org/10.1109/tmm.2023.3241517"/><api:link type="altmetric" href="http://www.altmetric.com/details.php?doi=10.1109/tmm.2023.3241517"/></api:links></api:field><api:field name="eissn" type="text" display-name="eISSN"><api:text>1941-0077</api:text><api:links><api:link type="elements/journal" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/journals/974865"/></api:links></api:field><api:field name="issn" type="text" display-name="ISSN"><api:text>1520-9210</api:text><api:links><api:link type="elements/journal" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/journals/974865"/></api:links></api:field><api:field name="journal" type="text" display-name="Journal"><api:text>IEEE Transactions on Multimedia</api:text></api:field><api:field name="online-publication-date" type="date" display-name="Online publication date"><api:date><api:day>2</api:day><api:month>2</api:month><api:year>2023</api:year></api:date></api:field><api:field name="publication-date" type="date" display-name="Publication date"><api:date><api:day>2</api:day><api:month>2</api:month><api:year>2023</api:year></api:date></api:field><api:field name="publication-status" type="text" display-name="Status"><api:text>Published online</api:text></api:field><api:field name="publisher" type="text" display-name="Publisher"><api:text>Institute of Electrical and Electronics Engineers</api:text></api:field><api:field name="record-created-at-source-date" type="date" display-name="Record created at source"><api:date><api:day>2</api:day><api:month>2</api:month><api:year>2023</api:year></api:date></api:field><api:field name="title" type="text" display-name="Title"><api:text>Textual context-aware dense captioning with diverse words</api:text></api:field><api:field name="types" type="list" display-name="Sub types"><api:items><api:item>Original research article</api:item></api:items></api:field></api:native></api:record><api:record format="native" id="4265550" source-id="13" source-name="crossref" source-display-name="Crossref" id-at-source="10.1109/tmm.2023.3241517" last-modified-when="2023-12-21T06:34:33.84+00:00"><api:native><api:field name="authors" type="person-list" display-name="Authors"><api:people><api:person><api:last-name>Shao</api:last-name><api:initials>Z</api:initials><api:first-names>Zhuang</api:first-names><api:separate-first-names><api:first-name>Zhuang</api:first-name></api:separate-first-names><api:identifiers><api:identifier scheme="orcid">0000-0001-7824-0985</api:identifier></api:identifiers></api:person><api:person><api:links><api:link type="elements/user" id="36768" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/users/36768"/></api:links><api:last-name>Han</api:last-name><api:initials>J</api:initials><api:first-names>Jungong</api:first-names><api:separate-first-names><api:first-name>Jungong</api:first-name></api:separate-first-names><api:identifiers><api:identifier scheme="orcid">0000-0003-4361-956X</api:identifier></api:identifiers></api:person><api:person><api:last-name>Debattista</api:last-name><api:initials>K</api:initials><api:first-names>Kurt</api:first-names><api:separate-first-names><api:first-name>Kurt</api:first-name></api:separate-first-names><api:identifiers><api:identifier scheme="orcid">0000-0003-2982-5199</api:identifier></api:identifiers></api:person><api:person><api:last-name>Pang</api:last-name><api:initials>Y</api:initials><api:first-names>Yanwei</api:first-names><api:separate-first-names><api:first-name>Yanwei</api:first-name></api:separate-first-names><api:identifiers><api:identifier scheme="orcid">0000-0001-6670-3727</api:identifier></api:identifiers></api:person></api:people></api:field><api:field name="doi" type="text" display-name="DOI"><api:text>10.1109/tmm.2023.3241517</api:text><api:links><api:link type="doi" href="http://doi.org/10.1109/tmm.2023.3241517"/><api:link type="altmetric" href="http://www.altmetric.com/details.php?doi=10.1109/tmm.2023.3241517"/></api:links></api:field><api:field name="eissn" type="text" display-name="eISSN"><api:text>1941-0077</api:text><api:links><api:link type="elements/journal" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/journals/974865"/></api:links></api:field><api:field name="issn" type="text" display-name="ISSN"><api:text>1520-9210</api:text><api:links><api:link type="elements/journal" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/journals/974865"/></api:links></api:field><api:field name="journal" type="text" display-name="Journal"><api:text>IEEE Transactions on Multimedia</api:text></api:field><api:field name="pagination" type="pagination" display-name="Pagination"><api:pagination><api:begin-page>8753</api:begin-page><api:end-page>8766</api:end-page></api:pagination></api:field><api:field name="publication-date" type="date" display-name="Publication date"><api:date><api:year>2023</api:year></api:date></api:field><api:field name="publication-status" type="text" display-name="Status"><api:text>Published</api:text></api:field><api:field name="publisher" type="text" display-name="Publisher"><api:text>Institute of Electrical and Electronics Engineers (IEEE)</api:text></api:field><api:field name="publisher-url" type="text" display-name="Link 2"><api:text>http://dx.doi.org/10.1109/tmm.2023.3241517</api:text></api:field><api:field name="record-created-at-source-date" type="date" display-name="Record created at source"><api:date><api:day>20</api:day><api:month>12</api:month><api:year>2023</api:year></api:date></api:field><api:field name="title" type="text" display-name="Title"><api:text>Textual Context-Aware Dense Captioning With Diverse Words</api:text></api:field><api:field name="volume" type="text" display-name="Volume"><api:text>25</api:text></api:field></api:native></api:record><api:record format="native" id="4271413" source-id="7" source-name="scopus" source-display-name="Scopus" id-at-source="2-s2.0-85148441586" last-modified-when="2024-04-25T07:28:10.13+01:00"><api:citation-count>23</api:citation-count><api:native><api:field name="abstract" type="text" display-name="Abstract"><api:text>Dense captioning generates more detailed spoken descriptions for complex visual scenes. Despite several promising leads, existing methods still have two broad limitations: 1) The vast majority of prior arts only consider visual contextual clues during captioning but ignore potentially important textual context; 2) current imbalanced learning mechanisms limit the diversity of vocabulary learned from the dictionary, thus giving rise to low language-learning efficiency. To alleviate these gaps, in this paper, we propose an end-to-end enhanced dense captioning architecture, namely Enhanced Transformer Dense Captioner (ETDC), which obtains textual context from surrounding regions and dynamically diversifies the vocabulary bank during captioning. Concretely, we first propose the Textual Context Module (TCM), which is integrated into each self-attention layer of the Transformer decoder, to capture the surrounding textual context. Moreover, we take full advantage of the class information of object context and propose a Dynamic Vocabulary Frequency Histogram (DVFH) re-sampling strategy during training to balance words with different frequencies. The proposed method is tested on the standard dense captioning datasets and surpasses the state-of-the-art methods in terms of mean Average Precision (mAP).</api:text></api:field><api:field name="authors" type="person-list" display-name="Authors"><api:people><api:person><api:last-name>Shao</api:last-name><api:initials>Z</api:initials><api:first-names>Z</api:first-names><api:separate-first-names><api:first-name>Z</api:first-name></api:separate-first-names><api:addresses><api:address iso-country-code="GB"><api:line type="organisation">Warwick Manufacturing Group</api:line><api:line type="city">Coventry</api:line><api:line type="country">United Kingdom</api:line></api:address></api:addresses><api:identifiers><api:identifier scheme="scopus-author-id">57225041270</api:identifier><api:identifier scheme="orcid">0000-0001-7824-0985</api:identifier></api:identifiers></api:person><api:person><api:links><api:link type="elements/user" id="36768" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/users/36768"/></api:links><api:last-name>Han</api:last-name><api:initials>J</api:initials><api:first-names>J</api:first-names><api:separate-first-names><api:first-name>J</api:first-name></api:separate-first-names><api:addresses><api:address iso-country-code="GB"><api:line type="organisation">The University of Sheffield</api:line><api:line type="city">Sheffield</api:line><api:line type="country">United Kingdom</api:line></api:address></api:addresses><api:identifiers><api:identifier scheme="scopus-author-id">14522692900</api:identifier><api:identifier scheme="orcid">0000-0003-4361-956X</api:identifier></api:identifiers></api:person><api:person><api:last-name>Debattista</api:last-name><api:initials>K</api:initials><api:first-names>K</api:first-names><api:separate-first-names><api:first-name>K</api:first-name></api:separate-first-names><api:addresses><api:address iso-country-code="GB"><api:line type="organisation">Warwick Manufacturing Group</api:line><api:line type="city">Coventry</api:line><api:line type="country">United Kingdom</api:line></api:address></api:addresses><api:identifiers><api:identifier scheme="scopus-author-id">10240295400</api:identifier><api:identifier scheme="orcid">0000-0003-2982-5199</api:identifier></api:identifiers></api:person><api:person><api:last-name>Pang</api:last-name><api:initials>Y</api:initials><api:first-names>Y</api:first-names><api:separate-first-names><api:first-name>Y</api:first-name></api:separate-first-names><api:addresses><api:address iso-country-code="CN"><api:line type="organisation">Tianjin University</api:line><api:line type="city">Tianjin</api:line><api:line type="country">China</api:line></api:address><api:address iso-country-code="CN"><api:line type="organisation">Shanghai Artificial Intelligence Laboratory</api:line><api:line type="city">Shanghai</api:line><api:line type="country">China</api:line></api:address></api:addresses><api:identifiers><api:identifier scheme="scopus-author-id">55660281200</api:identifier><api:identifier scheme="orcid">0000-0001-6670-3727</api:identifier></api:identifiers></api:person></api:people></api:field><api:field name="doi" type="text" display-name="DOI"><api:text>10.1109/TMM.2023.3241517</api:text><api:links><api:link type="doi" href="http://doi.org/10.1109/TMM.2023.3241517"/><api:link type="altmetric" href="http://www.altmetric.com/details.php?doi=10.1109/TMM.2023.3241517"/></api:links></api:field><api:field name="eissn" type="text" display-name="eISSN"><api:text>1941-0077</api:text><api:links><api:link type="elements/journal" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/journals/974865"/></api:links></api:field><api:field name="issn" type="text" display-name="ISSN"><api:text>1520-9210</api:text><api:links><api:link type="elements/journal" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/journals/974865"/></api:links></api:field><api:field name="journal" type="text" display-name="Journal"><api:text>IEEE Transactions on Multimedia</api:text></api:field><api:field name="pagination" type="pagination" display-name="Pagination"><api:pagination><api:begin-page>8753</api:begin-page><api:end-page>8766</api:end-page></api:pagination></api:field><api:field name="publication-date" type="date" display-name="Publication date"><api:date><api:day>1</api:day><api:month>1</api:month><api:year>2023</api:year></api:date></api:field><api:field name="publication-status" type="text" display-name="Status"><api:text>Published</api:text></api:field><api:field name="title" type="text" display-name="Title"><api:text>Textual Context-Aware Dense Captioning With Diverse Words</api:text></api:field><api:field name="types" type="list" display-name="Sub types"><api:items><api:item>Journal Article</api:item></api:items></api:field><api:field name="volume" type="text" display-name="Volume"><api:text>25</api:text></api:field></api:native></api:record><api:record format="native" id="4265519" source-id="10" source-name="dimensions" source-display-name="Dimensions" id-at-source="pub.1155075450" last-modified-when="2024-04-25T06:21:46.173+01:00"><api:citation-count>24</api:citation-count><api:native><api:field name="authors" type="person-list" display-name="Authors"><api:people><api:person><api:last-name>Shao</api:last-name><api:initials>Z</api:initials><api:first-names>Zhuang</api:first-names><api:separate-first-names><api:first-name>Zhuang</api:first-name></api:separate-first-names><api:addresses><api:address iso-country-code="GB"><api:line type="organisation">University of Warwick</api:line><api:line type="city">Coventry</api:line><api:line type="country">United Kingdom</api:line></api:address></api:addresses><api:identifiers><api:identifier scheme="dimensions-researcher-id">ur.010730225220.98</api:identifier></api:identifiers></api:person><api:person><api:links><api:link type="elements/user" id="36768" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/users/36768"/></api:links><api:last-name>Han</api:last-name><api:initials>J</api:initials><api:first-names>Jungong</api:first-names><api:separate-first-names><api:first-name>Jungong</api:first-name></api:separate-first-names><api:addresses><api:address iso-country-code="GB"><api:line type="organisation">University of Sheffield</api:line><api:line type="city">Sheffield</api:line><api:line type="country">United Kingdom</api:line></api:address></api:addresses><api:identifiers><api:identifier scheme="dimensions-researcher-id">ur.013141527713.39</api:identifier></api:identifiers><api:author-types><api:author-type>corresponding</api:author-type></api:author-types></api:person><api:person><api:last-name>Debattista</api:last-name><api:initials>K</api:initials><api:first-names>Kurt</api:first-names><api:separate-first-names><api:first-name>Kurt</api:first-name></api:separate-first-names><api:addresses><api:address iso-country-code="GB"><api:line type="organisation">University of Warwick</api:line><api:line type="city">Coventry</api:line><api:line type="country">United Kingdom</api:line></api:address></api:addresses><api:identifiers><api:identifier scheme="dimensions-researcher-id">ur.01306060014.51</api:identifier></api:identifiers></api:person><api:person><api:last-name>Pang</api:last-name><api:initials>Y</api:initials><api:first-names>Yanwei</api:first-names><api:separate-first-names><api:first-name>Yanwei</api:first-name></api:separate-first-names><api:addresses><api:address iso-country-code="CN"><api:line type="organisation">Tianjin University</api:line><api:line type="city">Tianjin</api:line><api:line type="country">China</api:line></api:address><api:address iso-country-code="CN"><api:line type="organisation">Shanghai Artificial Intelligence Laboratory</api:line><api:line type="city">Shanghai</api:line><api:line type="country">China</api:line></api:address></api:addresses><api:identifiers><api:identifier scheme="dimensions-researcher-id">ur.010231654061.18</api:identifier></api:identifiers></api:person></api:people></api:field><api:field name="doi" type="text" display-name="DOI"><api:text>10.1109/tmm.2023.3241517</api:text><api:links><api:link type="doi" href="http://doi.org/10.1109/tmm.2023.3241517"/><api:link type="altmetric" href="http://www.altmetric.com/details.php?doi=10.1109/tmm.2023.3241517"/></api:links></api:field><api:field name="issn" type="text" display-name="ISSN"><api:text>1520-9210</api:text><api:links><api:link type="elements/journal" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/journals/974865"/></api:links></api:field><api:field name="journal" type="text" display-name="Journal"><api:text>IEEE Transactions on Multimedia</api:text></api:field><api:field name="keywords" type="keyword-list" display-name="Keywords"><api:keywords><api:keyword scheme="for-2020">46 Information and Computing Sciences</api:keyword><api:keyword scheme="for-2020">4603 Computer Vision and Multimedia Computation</api:keyword><api:keyword scheme="rcdc">Basic Behavioral and Social Science</api:keyword><api:keyword scheme="rcdc">Behavioral and Social Science</api:keyword></api:keywords></api:field><api:field name="online-publication-date" type="date" display-name="Online publication date"><api:date><api:day>2</api:day><api:month>2</api:month><api:year>2023</api:year></api:date></api:field><api:field name="open-access-status" type="text" display-name="Open access status"><api:text>Closed Access</api:text></api:field><api:field name="pagination" type="pagination" display-name="Pagination"><api:pagination><api:begin-page>8753</api:begin-page><api:end-page>8766</api:end-page></api:pagination></api:field><api:field name="publisher" type="text" display-name="Publisher"><api:text>Institute of Electrical and Electronics Engineers (IEEE)</api:text></api:field><api:field name="record-created-at-source-date" type="date" display-name="Record created at source"><api:date><api:day>3</api:day><api:month>2</api:month><api:year>2023</api:year></api:date></api:field><api:field name="title" type="text" display-name="Title"><api:text>Textual Context-Aware Dense Captioning With Diverse Words</api:text></api:field><api:field name="volume" type="text" display-name="Volume"><api:text>25</api:text></api:field></api:native></api:record><api:record format="native" id="4516906" source-id="11" source-name="wos-lite" source-display-name="Web of Science (Lite)" id-at-source="WOS:001125902000015" last-modified-when="2024-04-24T21:14:58.74+01:00"><api:citation-count>8</api:citation-count><api:native><api:field name="authors" type="person-list" display-name="Authors"><api:people><api:person><api:last-name>Shao</api:last-name><api:initials>Z</api:initials><api:first-names>Zhuang</api:first-names><api:separate-first-names><api:first-name>Zhuang</api:first-name></api:separate-first-names><api:identifiers><api:identifier scheme="researcherid">GYU-2414-2022</api:identifier></api:identifiers></api:person><api:person><api:links><api:link type="elements/user" id="36768" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/users/36768"/></api:links><api:last-name>Han</api:last-name><api:initials>J</api:initials><api:first-names>Jungong</api:first-names><api:separate-first-names><api:first-name>Jungong</api:first-name></api:separate-first-names></api:person><api:person><api:last-name>Debattista</api:last-name><api:initials>K</api:initials><api:first-names>Kurt</api:first-names><api:separate-first-names><api:first-name>Kurt</api:first-name></api:separate-first-names></api:person><api:person><api:last-name>Pang</api:last-name><api:initials>Y</api:initials><api:first-names>Yanwei</api:first-names><api:separate-first-names><api:first-name>Yanwei</api:first-name></api:separate-first-names></api:person></api:people></api:field><api:field name="author-url" type="text" display-name="Link 1"><api:text>https://www.webofscience.com/api/gateway?GWVersion=2&amp;SrcApp=sheffield_elements_live&amp;SrcAuth=WosAPI&amp;KeyUT=WOS:001125902000015&amp;DestLinkType=FullRecord&amp;DestApp=WOS_CPL</api:text></api:field><api:field name="doi" type="text" display-name="DOI"><api:text>10.1109/TMM.2023.3241517</api:text><api:links><api:link type="doi" href="http://doi.org/10.1109/TMM.2023.3241517"/><api:link type="altmetric" href="http://www.altmetric.com/details.php?doi=10.1109/TMM.2023.3241517"/></api:links></api:field><api:field name="eissn" type="text" display-name="eISSN"><api:text>1941-0077</api:text><api:links><api:link type="elements/journal" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/journals/974865"/></api:links></api:field><api:field name="external-identifiers" type="identifier-list" display-name="External identifiers"><api:identifiers><api:identifier scheme="isidoc">CN3Z4</api:identifier></api:identifiers></api:field><api:field name="issn" type="text" display-name="ISSN"><api:text>1520-9210</api:text><api:links><api:link type="elements/journal" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/journals/974865"/></api:links></api:field><api:field name="journal" type="text" display-name="Journal"><api:text>IEEE TRANSACTIONS ON MULTIMEDIA</api:text></api:field><api:field name="keywords" type="keyword-list" display-name="Keywords"><api:keywords><api:keyword>Dense Captioning</api:keyword><api:keyword>Enhanced Transformer Dense Captioner</api:keyword><api:keyword>Textual Context Module</api:keyword><api:keyword>Dynamic Vocabulary Frequency Histogram</api:keyword></api:keywords></api:field><api:field name="pagination" type="pagination" display-name="Pagination"><api:pagination><api:begin-page>8753</api:begin-page><api:end-page>8766</api:end-page></api:pagination></api:field><api:field name="publication-date" type="date" display-name="Publication date"><api:date><api:year>2023</api:year></api:date></api:field><api:field name="publication-status" type="text" display-name="Status"><api:text>Published</api:text></api:field><api:field name="title" type="text" display-name="Title"><api:text>Textual Context-Aware Dense Captioning With Diverse Words</api:text></api:field><api:field name="types" type="list" display-name="Sub types"><api:items><api:item>Article</api:item></api:items></api:field><api:field name="volume" type="text" display-name="Volume"><api:text>25</api:text></api:field></api:native></api:record></api:records><api:fields/><api:all-labels type="keyword-list"><api:keywords><api:keyword origin="record-data" source="eprints">Dense Captioning</api:keyword><api:keyword origin="record-data" source="eprints">Enhanced Transformer Dense Captioner</api:keyword><api:keyword origin="record-data" source="eprints">Textual Context Module</api:keyword><api:keyword origin="record-data" source="eprints">Dynamic Vocabulary Frequency Histogram</api:keyword><api:keyword scheme="for-2020" origin="record-data" source="dimensions">46 Information and Computing Sciences</api:keyword><api:keyword scheme="for-2020" origin="record-data" source="dimensions">4603 Computer Vision and Multimedia Computation</api:keyword><api:keyword scheme="rcdc" origin="record-data" source="dimensions">Basic Behavioral and Social Science</api:keyword><api:keyword scheme="rcdc" origin="record-data" source="dimensions">Behavioral and Social Science</api:keyword><api:keyword origin="record-data" source="wos-lite">Dense Captioning</api:keyword><api:keyword origin="record-data" source="wos-lite">Enhanced Transformer Dense Captioner</api:keyword><api:keyword origin="record-data" source="wos-lite">Textual Context Module</api:keyword><api:keyword origin="record-data" source="wos-lite">Dynamic Vocabulary Frequency Histogram</api:keyword><api:keyword scheme="for" origin="issn-inferred">08 Information and Computing Sciences</api:keyword><api:keyword scheme="for" origin="issn-inferred">09 Engineering</api:keyword><api:keyword scheme="science-metrix" origin="issn-inferred">Artificial Intelligence &amp; Image Processing</api:keyword><api:keyword scheme="for-2020" origin="issn-inferred">40 Engineering</api:keyword><api:keyword scheme="for-2020" origin="issn-inferred">46 Information and computing sciences</api:keyword></api:keywords></api:all-labels><api:journal issn="1520-9210" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/journals/974865" title="IEEE Transactions on Multimedia"><api:records><api:record source-name="summary"><api:title>IEEE Transactions on Multimedia</api:title></api:record></api:records></api:journal><api:relationships href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/publications/1524987/relationships"/><api:flagged-as-not-externally-funded>true</api:flagged-as-not-externally-funded></api:object></api:result></api:response>