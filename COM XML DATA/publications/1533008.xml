<?xml version="1.0" encoding="utf-8"?><api:response xmlns:api="http://www.symplectic.co.uk/publications/api"><api:version uri="https://mypublications.shef.ac.uk/" elements-version="6.17.0.4095" schema-version="6.13" product-name="myPublications"/><api:request href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/publications/1533008"/><api:result><api:object category="publication" id="1533008" last-affected-when="2024-04-15T00:08:25.403+01:00" last-modified-when="2024-04-15T00:08:25.403+01:00" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/publications/1533008" created-when="2023-03-24T09:51:35.893+00:00" type-id="5" type-display-name="Journal article" type="journal-article"><api:privacy-level>Public</api:privacy-level><api:privacy-level-locked>false</api:privacy-level-locked><api:ever-approved>true</api:ever-approved><api:reporting-date-1>2023-03-27</api:reporting-date-1><api:allow-type-switching>true</api:allow-type-switching><api:records><api:record format="native" id="4281494" source-id="28" source-name="eprints" source-display-name="White Rose Research Online" id-at-source="197754" last-modified-when="2023-04-24T13:44:31.3+01:00"><api:native><api:field name="abstract" type="text" display-name="Abstract"><api:text>Temporal consistency and content preservation are the prominent challenges in artistic video style transfer. To address these challenges, we present a technique that utilizes depth data and we demonstrate this on real-world videos from the web, as well as on a standard video dataset of three-dimensional computer-generated content. Our algorithm employs an image-transformation network combined with a depth encoder network for stylizing video sequences. For improved global structure preservation and temporal stability, the depth encoder network encodes ground-truth depth information which is fused into the stylization network. To further enforce temporal coherence, we employ ConvLSTM layers in the encoder, and a loss function based on calculated depth information for the output frames is also used. We show that our approach is capable of producing stylized videos with improved temporal consistency compared to state-of-the-art methods whilst also successfully transferring the artistic style of a target painting.</api:text></api:field><api:field name="acceptance-date" type="date" display-name="Date of acceptance"><api:date><api:day>24</api:day><api:month>3</api:month><api:year>2023</api:year></api:date></api:field><api:field name="authors" type="person-list" display-name="Authors"><api:people><api:person><api:links><api:link type="elements/user" id="31376" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/users/31376"/></api:links><api:last-name>Ioannou</api:last-name><api:initials>E</api:initials><api:first-names>E</api:first-names><api:separate-first-names><api:first-name>E</api:first-name></api:separate-first-names></api:person><api:person><api:links><api:link type="elements/user" id="1553" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/users/1553"/></api:links><api:last-name>Maddock</api:last-name><api:initials>S</api:initials><api:first-names>S</api:first-names><api:separate-first-names><api:first-name>S</api:first-name></api:separate-first-names></api:person></api:people></api:field><api:field name="c-data-accessibility" type="text" display-name="Data Accessibility"><api:text>Data2</api:text></api:field><api:field name="c-data-availability-statement" type="boolean" display-name="Data Availability Statement"><api:boolean>true</api:boolean></api:field><api:field name="c-goldoa" type="boolean" display-name="Gold Open Access?"><api:boolean>true</api:boolean></api:field><api:field name="c-licence-statement" type="boolean" display-name="Licence statement?"><api:boolean>false</api:boolean></api:field><api:field name="c-ref-no-issn" type="boolean" display-name="REF No ISSN"><api:boolean>false</api:boolean></api:field><api:field name="c-rights-retention-opt-out" type="boolean" display-name="Rights retention opt out"><api:boolean>false</api:boolean></api:field><api:field name="doi" type="text" display-name="DOI"><api:text>10.3390/computers12040069</api:text><api:links><api:link type="doi" href="http://doi.org/10.3390/computers12040069"/><api:link type="altmetric" href="http://www.altmetric.com/details.php?doi=10.3390/computers12040069"/></api:links></api:field><api:field name="eissn" type="text" display-name="eISSN"><api:text>2073-431X</api:text><api:links><api:link type="elements/journal" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/journals/961674"/></api:links></api:field><api:field name="issue" type="text" display-name="Issue"><api:text>4</api:text></api:field><api:field name="journal" type="text" display-name="Journal"><api:text>Computers</api:text></api:field><api:field name="keywords" type="keyword-list" display-name="Keywords"><api:keywords><api:keyword>neural style transfer</api:keyword><api:keyword>deep learning</api:keyword><api:keyword>depth estimation</api:keyword><api:keyword>temporal consistency</api:keyword></api:keywords></api:field><api:field name="notes" type="text" display-name="Other information"><api:text>Â© 2023 by the authors. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (https://creativecommons.org/licenses/by/4.0/).</api:text></api:field><api:field name="number" type="text" display-name="Article number"><api:text>69</api:text></api:field><api:field name="online-publication-date" type="date" display-name="Online publication date"><api:date><api:day>27</api:day><api:month>3</api:month><api:year>2023</api:year></api:date></api:field><api:field name="publication-date" type="date" display-name="Publication date"><api:date><api:month>4</api:month><api:year>2023</api:year></api:date></api:field><api:field name="publication-status" type="text" display-name="Status"><api:text>Published</api:text></api:field><api:field name="public-url" type="text" display-name="Public URL"><api:text>https://eprints.whiterose.ac.uk/id/eprint/197754</api:text></api:field><api:field name="publisher" type="text" display-name="Publisher"><api:text>MDPI</api:text></api:field><api:field name="publisher-licence" type="text" display-name="Publisher licence"><api:text>CC BY</api:text></api:field><api:field name="record-created-at-source-date" type="date" display-name="Record created at source"><api:date><api:day>27</api:day><api:month>3</api:month><api:year>2023</api:year></api:date></api:field><api:field name="record-made-public-at-source-date" type="date" display-name="Record made publicly available"><api:date><api:day>28</api:day><api:month>3</api:month><api:year>2023</api:year></api:date></api:field><api:field name="repository-status" type="text" display-name="Availability"><api:text>Public</api:text></api:field><api:field name="title" type="text" display-name="Title"><api:text>Depth-aware neural style transfer for videos</api:text></api:field><api:field name="volume" type="text" display-name="Volume"><api:text>12</api:text></api:field><api:files><api:file proprietary-id="https://eprints.whiterose.ac.uk/id/file/6422833"><api:filename>computers-12-00069.pdf</api:filename><api:file-url>https://eprints.whiterose.ac.uk/197754/1/computers-12-00069.pdf</api:file-url><api:extension>pdf</api:extension><api:file-size>17394332</api:file-size><api:mime-type>application/pdf</api:mime-type><api:checksum algorithm="md5">E4F5A8E0BD91D20B2E8D7E99E8ECA176</api:checksum><api:file-version>Published version</api:file-version></api:file></api:files></api:native></api:record><api:record format="native" id="4281148" source-id="1" source-name="manual" source-display-name="Manual" id-at-source="E051FCA7-7D83-4186-9CB1-EE232B97A32F" last-modified-when="2023-03-28T11:03:43.66+01:00" is-locked="true"><api:verification-status>verified</api:verification-status><api:verification-comment>28/03/2023 TW</api:verification-comment><api:native><api:field name="abstract" type="text" display-name="Abstract"><api:text>Temporal consistency and content preservation are the prominent challenges in artistic video style transfer. To address these challenges, we present a technique that utilizes depth data and we demonstrate this on real-world videos from the web, as well as on a standard video dataset of three-dimensional computer-generated content. Our algorithm employs an image-transformation network combined with a depth encoder network for stylizing video sequences. For improved global structure preservation and temporal stability, the depth encoder network encodes ground-truth depth information which is fused into the stylization network. To further enforce temporal coherence, we employ ConvLSTM layers in the encoder, and a loss function based on calculated depth information for the output frames is also used. We show that our approach is capable of producing stylized videos with improved temporal consistency compared to state-of-the-art methods whilst also successfully transferring the artistic style of a target painting.</api:text></api:field><api:field name="acceptance-date" type="date" display-name="Date of acceptance"><api:date><api:day>24</api:day><api:month>3</api:month><api:year>2023</api:year></api:date></api:field><api:field name="authors" type="person-list" display-name="Authors"><api:people><api:person><api:links><api:link type="elements/user" id="31376" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/users/31376"/></api:links><api:last-name>Ioannou</api:last-name><api:initials>E</api:initials><api:first-names>Eleftherios</api:first-names><api:separate-first-names><api:first-name>Eleftherios</api:first-name></api:separate-first-names><api:author-types><api:author-type>first</api:author-type></api:author-types></api:person><api:person><api:links><api:link type="elements/user" id="1553" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/users/1553"/></api:links><api:last-name>Maddock</api:last-name><api:initials>S</api:initials><api:first-names>Stephen</api:first-names><api:separate-first-names><api:first-name>Stephen</api:first-name></api:separate-first-names></api:person></api:people></api:field><api:field name="c-apc-source" type="text" display-name="APC funding source"><api:text>Not applicable</api:text></api:field><api:field name="c-data-accessibility" type="text" display-name="Data Accessibility"><api:text>Data2</api:text></api:field><api:field name="c-data-availability-statement" type="boolean" display-name="Data Availability Statement"><api:boolean>true</api:boolean></api:field><api:field name="c-goldoa" type="boolean" display-name="Gold Open Access?"><api:boolean>true</api:boolean></api:field><api:field name="c-licence-statement" type="boolean" display-name="Licence statement?"><api:boolean>false</api:boolean></api:field><api:field name="c-ref-no-issn" type="boolean" display-name="REF No ISSN"><api:boolean>false</api:boolean></api:field><api:field name="c-ref-pre-2014" type="boolean" display-name="REF pre-2014"><api:boolean>false</api:boolean></api:field><api:field name="doi" type="text" display-name="DOI"><api:text>10.3390/computers12040069</api:text><api:links><api:link type="doi" href="http://doi.org/10.3390/computers12040069"/><api:link type="altmetric" href="http://www.altmetric.com/details.php?doi=10.3390/computers12040069"/></api:links></api:field><api:field name="issn" type="text" display-name="ISSN"><api:text>2073-431X</api:text><api:links><api:link type="elements/journal" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/journals/961674"/></api:links></api:field><api:field name="issue" type="text" display-name="Issue"><api:text>4</api:text></api:field><api:field name="journal" type="text" display-name="Journal"><api:text>Computers</api:text></api:field><api:field name="number" type="text" display-name="Article number"><api:text>69</api:text></api:field><api:field name="online-publication-date" type="date" display-name="Online publication date"><api:date><api:day>27</api:day><api:month>3</api:month><api:year>2023</api:year></api:date></api:field><api:field name="open-access-status" type="text" display-name="Open access status"><api:text>Gold OA</api:text></api:field><api:field name="publication-date" type="date" display-name="Publication date"><api:date><api:month>4</api:month><api:year>2023</api:year></api:date></api:field><api:field name="publication-status" type="text" display-name="Status"><api:text>Published</api:text></api:field><api:field name="publisher-licence" type="text" display-name="Publisher licence"><api:text>CC BY</api:text></api:field><api:field name="record-created-at-source-date" type="date" display-name="Record created at source"><api:date><api:day>24</api:day><api:month>3</api:month><api:year>2023</api:year></api:date></api:field><api:field name="title" type="text" display-name="Title"><api:text>Depth-aware neural style transfer for videos</api:text></api:field><api:field name="types" type="list" display-name="Sub types"><api:items><api:item>Original research article</api:item></api:items></api:field><api:field name="volume" type="text" display-name="Volume"><api:text>12</api:text></api:field></api:native></api:record><api:record format="native" id="4281610" source-id="13" source-name="crossref" source-display-name="Crossref" id-at-source="10.3390/computers12040069" last-modified-when="2023-04-12T19:49:37.423+01:00"><api:native><api:field name="abstract" type="text" display-name="Abstract"><api:text>&lt;jats:p&gt;Temporal consistency and content preservation are the prominent challenges in artistic video style transfer. To address these challenges, we present a technique that utilizes depth data and we demonstrate this on real-world videos from the web, as well as on a standard video dataset of three-dimensional computer-generated content. Our algorithm employs an image-transformation network combined with a depth encoder network for stylizing video sequences. For improved global structure preservation and temporal stability, the depth encoder network encodes ground-truth depth information which is fused into the stylization network. To further enforce temporal coherence, we employ ConvLSTM layers in the encoder, and a loss function based on calculated depth information for the output frames is also used. We show that our approach is capable of producing stylized videos with improved temporal consistency compared to state-of-the-art methods whilst also successfully transferring the artistic style of a target painting.&lt;/jats:p&gt;</api:text></api:field><api:field name="authors" type="person-list" display-name="Authors"><api:people><api:person><api:links><api:link type="elements/user" id="31376" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/users/31376"/></api:links><api:last-name>Ioannou</api:last-name><api:initials>E</api:initials><api:first-names>Eleftherios</api:first-names><api:separate-first-names><api:first-name>Eleftherios</api:first-name></api:separate-first-names><api:identifiers><api:identifier scheme="orcid">0000-0003-3892-2492</api:identifier></api:identifiers></api:person><api:person><api:links><api:link type="elements/user" id="1553" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/users/1553"/></api:links><api:last-name>Maddock</api:last-name><api:initials>S</api:initials><api:first-names>Steve</api:first-names><api:separate-first-names><api:first-name>Steve</api:first-name></api:separate-first-names><api:identifiers><api:identifier scheme="orcid">0000-0003-3179-0263</api:identifier></api:identifiers></api:person></api:people></api:field><api:field name="doi" type="text" display-name="DOI"><api:text>10.3390/computers12040069</api:text><api:links><api:link type="doi" href="http://doi.org/10.3390/computers12040069"/><api:link type="altmetric" href="http://www.altmetric.com/details.php?doi=10.3390/computers12040069"/></api:links></api:field><api:field name="eissn" type="text" display-name="eISSN"><api:text>2073-431X</api:text><api:links><api:link type="elements/journal" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/journals/961674"/></api:links></api:field><api:field name="issue" type="text" display-name="Issue"><api:text>4</api:text></api:field><api:field name="journal" type="text" display-name="Journal"><api:text>Computers</api:text></api:field><api:field name="language" type="text" display-name="Language"><api:text>en</api:text></api:field><api:field name="online-publication-date" type="date" display-name="Online publication date"><api:date><api:day>27</api:day><api:month>3</api:month><api:year>2023</api:year></api:date></api:field><api:field name="pagination" type="pagination" display-name="Pagination"><api:pagination><api:begin-page>69</api:begin-page><api:end-page>69</api:end-page></api:pagination></api:field><api:field name="publication-status" type="text" display-name="Status"><api:text>Published online</api:text></api:field><api:field name="publisher" type="text" display-name="Publisher"><api:text>MDPI AG</api:text></api:field><api:field name="publisher-url" type="text" display-name="Link 2"><api:text>http://dx.doi.org/10.3390/computers12040069</api:text></api:field><api:field name="record-created-at-source-date" type="date" display-name="Record created at source"><api:date><api:day>29</api:day><api:month>3</api:month><api:year>2023</api:year></api:date></api:field><api:field name="title" type="text" display-name="Title"><api:text>Depth-Aware Neural Style Transfer for Videos</api:text></api:field><api:field name="volume" type="text" display-name="Volume"><api:text>12</api:text></api:field></api:native></api:record><api:record format="native" id="4296498" source-id="7" source-name="scopus" source-display-name="Scopus" id-at-source="2-s2.0-85153735864" last-modified-when="2023-05-18T01:09:27.787+01:00"><api:citation-count>0</api:citation-count><api:native><api:field name="abstract" type="text" display-name="Abstract"><api:text>Temporal consistency and content preservation are the prominent challenges in artistic video style transfer. To address these challenges, we present a technique that utilizes depth data and we demonstrate this on real-world videos from the web, as well as on a standard video dataset of three-dimensional computer-generated content. Our algorithm employs an image-transformation network combined with a depth encoder network for stylizing video sequences. For improved global structure preservation and temporal stability, the depth encoder network encodes ground-truth depth information which is fused into the stylization network. To further enforce temporal coherence, we employ ConvLSTM layers in the encoder, and a loss function based on calculated depth information for the output frames is also used. We show that our approach is capable of producing stylized videos with improved temporal consistency compared to state-of-the-art methods whilst also successfully transferring the artistic style of a target painting.</api:text></api:field><api:field name="authors" type="person-list" display-name="Authors"><api:people><api:person><api:links><api:link type="elements/user" id="31376" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/users/31376"/></api:links><api:last-name>Ioannou</api:last-name><api:initials>E</api:initials><api:first-names>E</api:first-names><api:separate-first-names><api:first-name>E</api:first-name></api:separate-first-names><api:addresses><api:address iso-country-code="GB"><api:line type="organisation">The University of Sheffield</api:line><api:line type="city">Sheffield</api:line><api:line type="country">United Kingdom</api:line></api:address></api:addresses><api:identifiers><api:identifier scheme="scopus-author-id">57223634877</api:identifier><api:identifier scheme="orcid">0000-0003-3892-2492</api:identifier></api:identifiers></api:person><api:person><api:links><api:link type="elements/user" id="1553" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/users/1553"/></api:links><api:last-name>Maddock</api:last-name><api:initials>S</api:initials><api:first-names>S</api:first-names><api:separate-first-names><api:first-name>S</api:first-name></api:separate-first-names><api:addresses><api:address iso-country-code="GB"><api:line type="organisation">The University of Sheffield</api:line><api:line type="city">Sheffield</api:line><api:line type="country">United Kingdom</api:line></api:address></api:addresses><api:identifiers><api:identifier scheme="scopus-author-id">14024586600</api:identifier><api:identifier scheme="orcid">0000-0003-3179-0263</api:identifier></api:identifiers></api:person></api:people></api:field><api:field name="doi" type="text" display-name="DOI"><api:text>10.3390/computers12040069</api:text><api:links><api:link type="doi" href="http://doi.org/10.3390/computers12040069"/><api:link type="altmetric" href="http://www.altmetric.com/details.php?doi=10.3390/computers12040069"/></api:links></api:field><api:field name="eissn" type="text" display-name="eISSN"><api:text>2073-431X</api:text><api:links><api:link type="elements/journal" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/journals/961674"/></api:links></api:field><api:field name="issue" type="text" display-name="Issue"><api:text>4</api:text></api:field><api:field name="journal" type="text" display-name="Journal"><api:text>Computers</api:text></api:field><api:field name="publication-date" type="date" display-name="Publication date"><api:date><api:day>1</api:day><api:month>4</api:month><api:year>2023</api:year></api:date></api:field><api:field name="publication-status" type="text" display-name="Status"><api:text>Published</api:text></api:field><api:field name="title" type="text" display-name="Title"><api:text>Depth-Aware Neural Style Transfer for Videos</api:text></api:field><api:field name="types" type="list" display-name="Sub types"><api:items><api:item>Journal Article</api:item></api:items></api:field><api:field name="volume" type="text" display-name="Volume"><api:text>12</api:text></api:field></api:native></api:record><api:record format="native" id="4284056" source-id="10" source-name="dimensions" source-display-name="Dimensions" id-at-source="pub.1156553232" last-modified-when="2024-04-15T00:08:25.407+01:00"><api:citation-count>1</api:citation-count><api:native><api:field name="authors" type="person-list" display-name="Authors"><api:people><api:person><api:links><api:link type="elements/user" id="31376" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/users/31376"/></api:links><api:last-name>Ioannou</api:last-name><api:initials>E</api:initials><api:first-names>Eleftherios</api:first-names><api:separate-first-names><api:first-name>Eleftherios</api:first-name></api:separate-first-names><api:addresses><api:address iso-country-code="GB"><api:line type="organisation">University of Sheffield</api:line><api:line type="city">Sheffield</api:line><api:line type="country">United Kingdom</api:line></api:address></api:addresses><api:identifiers><api:identifier scheme="dimensions-researcher-id">ur.07702620437.28</api:identifier></api:identifiers><api:author-types><api:author-type>corresponding</api:author-type></api:author-types></api:person><api:person><api:links><api:link type="elements/user" id="1553" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/users/1553"/></api:links><api:last-name>Maddock</api:last-name><api:initials>S</api:initials><api:first-names>Steve</api:first-names><api:separate-first-names><api:first-name>Steve</api:first-name></api:separate-first-names><api:addresses><api:address iso-country-code="GB"><api:line type="organisation">University of Sheffield</api:line><api:line type="city">Sheffield</api:line><api:line type="country">United Kingdom</api:line></api:address></api:addresses><api:identifiers><api:identifier scheme="dimensions-researcher-id">ur.014705674245.55</api:identifier></api:identifiers><api:author-types><api:author-type>corresponding</api:author-type></api:author-types></api:person></api:people></api:field><api:field name="doi" type="text" display-name="DOI"><api:text>10.3390/computers12040069</api:text><api:links><api:link type="doi" href="http://doi.org/10.3390/computers12040069"/><api:link type="altmetric" href="http://www.altmetric.com/details.php?doi=10.3390/computers12040069"/></api:links></api:field><api:field name="issn" type="text" display-name="ISSN"><api:text>2073-431X</api:text><api:links><api:link type="elements/journal" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/journals/961674"/></api:links></api:field><api:field name="issue" type="text" display-name="Issue"><api:text>4</api:text></api:field><api:field name="journal" type="text" display-name="Journal"><api:text>Computers</api:text></api:field><api:field name="keywords" type="keyword-list" display-name="Keywords"><api:keywords><api:keyword scheme="for-2020">46 Information and Computing Sciences</api:keyword><api:keyword scheme="for-2020">4607 Graphics, Augmented Reality and Games</api:keyword><api:keyword scheme="for-2020">4603 Computer Vision and Multimedia Computation</api:keyword><api:keyword scheme="rcdc">Bioengineering</api:keyword></api:keywords></api:field><api:field name="online-publication-date" type="date" display-name="Online publication date"><api:date><api:day>27</api:day><api:month>3</api:month><api:year>2023</api:year></api:date></api:field><api:field name="open-access-status" type="text" display-name="Open access status"><api:text>Gold OA</api:text></api:field><api:field name="pagination" type="pagination" display-name="Pagination"><api:pagination><api:begin-page>69</api:begin-page></api:pagination></api:field><api:field name="publisher" type="text" display-name="Publisher"><api:text>MDPI</api:text></api:field><api:field name="record-created-at-source-date" type="date" display-name="Record created at source"><api:date><api:day>28</api:day><api:month>3</api:month><api:year>2023</api:year></api:date></api:field><api:field name="title" type="text" display-name="Title"><api:text>Depth-Aware Neural Style Transfer for Videos</api:text></api:field><api:field name="volume" type="text" display-name="Volume"><api:text>12</api:text></api:field></api:native></api:record><api:record format="native" id="4302106" source-id="11" source-name="wos-lite" source-display-name="Web of Science (Lite)" id-at-source="WOS:000977610700001" last-modified-when="2024-03-13T20:33:54.43+00:00"><api:citation-count>0</api:citation-count><api:native><api:field name="authors" type="person-list" display-name="Authors"><api:people><api:person><api:links><api:link type="elements/user" id="31376" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/users/31376"/></api:links><api:last-name>Ioannou</api:last-name><api:initials>E</api:initials><api:first-names>Eleftherios</api:first-names><api:separate-first-names><api:first-name>Eleftherios</api:first-name></api:separate-first-names></api:person><api:person><api:links><api:link type="elements/user" id="1553" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/users/1553"/></api:links><api:last-name>Maddock</api:last-name><api:initials>S</api:initials><api:first-names>Steve</api:first-names><api:separate-first-names><api:first-name>Steve</api:first-name></api:separate-first-names></api:person></api:people></api:field><api:field name="author-url" type="text" display-name="Link 1"><api:text>https://www.webofscience.com/api/gateway?GWVersion=2&amp;SrcApp=sheffield_elements_live&amp;SrcAuth=WosAPI&amp;KeyUT=WOS:000977610700001&amp;DestLinkType=FullRecord&amp;DestApp=WOS_CPL</api:text></api:field><api:field name="doi" type="text" display-name="DOI"><api:text>10.3390/computers12040069</api:text><api:links><api:link type="doi" href="http://doi.org/10.3390/computers12040069"/><api:link type="altmetric" href="http://www.altmetric.com/details.php?doi=10.3390/computers12040069"/></api:links></api:field><api:field name="external-identifiers" type="identifier-list" display-name="External identifiers"><api:identifiers><api:identifier scheme="isidoc">E7WV0</api:identifier></api:identifiers></api:field><api:field name="issn" type="text" display-name="ISSN"><api:text>2073-431X</api:text><api:links><api:link type="elements/journal" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/journals/961674"/></api:links></api:field><api:field name="issue" type="text" display-name="Issue"><api:text>4</api:text></api:field><api:field name="journal" type="text" display-name="Journal"><api:text>COMPUTERS</api:text></api:field><api:field name="keywords" type="keyword-list" display-name="Keywords"><api:keywords><api:keyword>neural style transfer</api:keyword><api:keyword>deep learning</api:keyword><api:keyword>depth estimation</api:keyword><api:keyword>temporal consistency</api:keyword></api:keywords></api:field><api:field name="number" type="text" display-name="Article number"><api:text>ARTN 69</api:text></api:field><api:field name="publication-date" type="date" display-name="Publication date"><api:date><api:year>2023</api:year></api:date></api:field><api:field name="publication-status" type="text" display-name="Status"><api:text>Published</api:text></api:field><api:field name="title" type="text" display-name="Title"><api:text>Depth-Aware Neural Style Transfer for Videos</api:text></api:field><api:field name="types" type="list" display-name="Sub types"><api:items><api:item>Article</api:item></api:items></api:field><api:field name="volume" type="text" display-name="Volume"><api:text>12</api:text></api:field></api:native></api:record></api:records><api:fields/><api:all-labels type="keyword-list"><api:keywords><api:keyword origin="record-data" source="eprints">neural style transfer</api:keyword><api:keyword origin="record-data" source="eprints">deep learning</api:keyword><api:keyword origin="record-data" source="eprints">depth estimation</api:keyword><api:keyword origin="record-data" source="eprints">temporal consistency</api:keyword><api:keyword scheme="for-2020" origin="record-data" source="dimensions">46 Information and Computing Sciences</api:keyword><api:keyword scheme="for-2020" origin="record-data" source="dimensions">4607 Graphics, Augmented Reality and Games</api:keyword><api:keyword scheme="for-2020" origin="record-data" source="dimensions">4603 Computer Vision and Multimedia Computation</api:keyword><api:keyword scheme="rcdc" origin="record-data" source="dimensions">Bioengineering</api:keyword><api:keyword origin="record-data" source="wos-lite">neural style transfer</api:keyword><api:keyword origin="record-data" source="wos-lite">deep learning</api:keyword><api:keyword origin="record-data" source="wos-lite">depth estimation</api:keyword><api:keyword origin="record-data" source="wos-lite">temporal consistency</api:keyword><api:keyword scheme="for-2020" origin="issn-inferred">40 Engineering</api:keyword><api:keyword scheme="for-2020" origin="issn-inferred">46 Information and computing sciences</api:keyword></api:keywords></api:all-labels><api:journal issn="2073-431X" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/journals/961674" title="Computers"><api:records><api:record source-name="summary"><api:title>Computers</api:title></api:record></api:records></api:journal><api:relationships href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/publications/1533008/relationships"/></api:object></api:result></api:response>