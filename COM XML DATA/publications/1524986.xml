<?xml version="1.0" encoding="utf-8"?><api:response xmlns:api="http://www.symplectic.co.uk/publications/api"><api:version uri="https://mypublications.shef.ac.uk/" elements-version="6.17.0.4095" schema-version="6.13" product-name="myPublications"/><api:request href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/publications/1524986"/><api:result><api:object category="publication" id="1524986" last-affected-when="2024-04-25T12:41:22.193+01:00" last-modified-when="2024-04-25T12:41:22.193+01:00" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/publications/1524986" created-when="2023-02-02T10:23:32.333+00:00" type-id="5" type-display-name="Journal article" type="journal-article"><api:privacy-level>Public</api:privacy-level><api:privacy-level-locked>false</api:privacy-level-locked><api:ever-approved>true</api:ever-approved><api:reporting-date-1>2023-01-06</api:reporting-date-1><api:allow-type-switching>true</api:allow-type-switching><api:records><api:record format="native" id="4260521" source-id="28" source-name="eprints" source-display-name="White Rose Research Online" id-at-source="195979" last-modified-when="2024-01-17T13:08:21.9+00:00"><api:native><api:field name="abstract" type="text" display-name="Abstract"><api:text>Semantic segmentation models gain robustness against adverse illumination conditions by taking advantage of complementary information from visible and thermal infrared (RGB-T) images. Despite its importance, most existing RGB-T semantic segmentation models directly adopt primitive fusion strategies, such as elementwise summation, to integrate multimodal features. Such strategies, unfortunately, overlook the modality discrepancies caused by inconsistent unimodal features obtained by two independent feature extractors, thus hindering the exploitation of cross-modal complementary information within the multimodal data. For that, we propose a novel network for RGB-T semantic segmentation, i.e. MDRNet + , which is an improved version of our previous work ABMDRNet. The core of MDRNet + is a brand new idea, termed the strategy of bridging-then-fusing, which mitigates modality discrepancies before cross-modal feature fusion. Concretely, an improved Modality Discrepancy Reduction (MDR + ) subnetwork is designed, which first extracts unimodal features and reduces their modality discrepancies. Afterward, discriminative multimodal features for RGB-T semantic segmentation are adaptively selected and integrated via several channel-weighted fusion (CWF) modules. Furthermore, a multiscale spatial context (MSC) module and a multiscale channel context (MCC) module are presented to effectively capture the contextual information. Finally, we elaborately assemble a challenging RGB-T semantic segmentation dataset, i.e., RTSS, for urban scene understanding to mitigate the lack of well-annotated training data. Comprehensive experiments demonstrate that our proposed model surpasses other state-of-the-art models on the MFNet, PST900, and RTSS datasets remarkably.</api:text></api:field><api:field name="acceptance-date" type="date" display-name="Date of acceptance"><api:date><api:day>27</api:day><api:month>12</api:month><api:year>2022</api:year></api:date></api:field><api:field name="authors" type="person-list" display-name="Authors"><api:people><api:person><api:links><api:link type="elements/user" id="36768" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/users/36768"/></api:links><api:last-name>Han</api:last-name><api:initials>J</api:initials><api:first-names>J</api:first-names><api:separate-first-names><api:first-name>J</api:first-name></api:separate-first-names></api:person><api:person><api:last-name>Shenlu</api:last-name><api:initials>Z</api:initials><api:first-names>Z</api:first-names><api:separate-first-names><api:first-name>Z</api:first-name></api:separate-first-names></api:person><api:person><api:last-name>Liu</api:last-name><api:initials>Y</api:initials><api:first-names>Y</api:first-names><api:separate-first-names><api:first-name>Y</api:first-name></api:separate-first-names></api:person><api:person><api:last-name>Jiao</api:last-name><api:initials>Q</api:initials><api:first-names>Q</api:first-names><api:separate-first-names><api:first-name>Q</api:first-name></api:separate-first-names></api:person><api:person><api:last-name>Zhang</api:last-name><api:initials>Q</api:initials><api:first-names>Q</api:first-names><api:separate-first-names><api:first-name>Q</api:first-name></api:separate-first-names></api:person></api:people></api:field><api:field name="doi" type="text" display-name="DOI"><api:text>10.1109/TNNLS.2022.3233089</api:text><api:links><api:link type="doi" href="http://doi.org/10.1109/TNNLS.2022.3233089"/><api:link type="altmetric" href="http://www.altmetric.com/details.php?doi=10.1109/TNNLS.2022.3233089"/></api:links></api:field><api:field name="eissn" type="text" display-name="eISSN"><api:text>2162-2388</api:text><api:links><api:link type="elements/journal" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/journals/988802"/></api:links></api:field><api:field name="issn" type="text" display-name="ISSN"><api:text>2162-237X</api:text><api:links><api:link type="elements/journal" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/journals/988802"/></api:links></api:field><api:field name="journal" type="text" display-name="Journal"><api:text>IEEE Transactions on Neural Networks and Learning Systems</api:text></api:field><api:field name="keywords" type="keyword-list" display-name="Keywords"><api:keywords><api:keyword>Bridging-then-fusing</api:keyword><api:keyword>contextual information</api:keyword><api:keyword>dataset</api:keyword><api:keyword>modality discrepancy reduction</api:keyword><api:keyword>RGB-T semantic segmentation</api:keyword></api:keywords></api:field><api:field name="notes" type="text" display-name="Other information"><api:text>Â© 2023 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other users, including reprinting/ republishing this material for advertising or promotional purposes, creating new collective works for resale or redistribution to servers or lists, or reuse of any copyrighted components of this work in other works. Reproduced in accordance with the publisher's self-archiving policy.</api:text></api:field><api:field name="online-publication-date" type="date" display-name="Online publication date"><api:date><api:day>6</api:day><api:month>1</api:month><api:year>2023</api:year></api:date></api:field><api:field name="publication-status" type="text" display-name="Status"><api:text>Published online</api:text></api:field><api:field name="public-url" type="text" display-name="Public URL"><api:text>https://eprints.whiterose.ac.uk/id/eprint/195979</api:text></api:field><api:field name="publisher" type="text" display-name="Publisher"><api:text>Institute of Electrical and Electronics Engineers</api:text></api:field><api:field name="record-created-at-source-date" type="date" display-name="Record created at source"><api:date><api:day>2</api:day><api:month>2</api:month><api:year>2023</api:year></api:date></api:field><api:field name="record-made-public-at-source-date" type="date" display-name="Record made publicly available"><api:date><api:day>2</api:day><api:month>2</api:month><api:year>2023</api:year></api:date></api:field><api:field name="repository-status" type="text" display-name="Availability"><api:text>Public</api:text></api:field><api:field name="title" type="text" display-name="Title"><api:text>Mitigating modality discrepancies for RGB-T semantic segmentation</api:text></api:field><api:files><api:file proprietary-id="https://eprints.whiterose.ac.uk/id/file/6347458"><api:filename>TNNLS-2022-P-22975.pdf</api:filename><api:file-url>https://eprints.whiterose.ac.uk/195979/13/TNNLS-2022-P-22975.pdf</api:file-url><api:extension>pdf</api:extension><api:file-size>5178692</api:file-size><api:mime-type>application/pdf</api:mime-type><api:checksum algorithm="md5">A3A62BD881D61517845BFECEA381271B</api:checksum><api:embargo-release-date>2024-01-06</api:embargo-release-date><api:file-version>Author accepted manuscript</api:file-version></api:file></api:files></api:native></api:record><api:record format="native" id="4260520" source-id="1" source-name="manual" source-display-name="Manual" id-at-source="43747B67-CA81-42A5-B31A-84E3E6A6838D" last-modified-when="2023-02-02T12:16:50.413+00:00" is-locked="true"><api:verification-status>verified</api:verification-status><api:verification-comment>CD 02/02/2023</api:verification-comment><api:native><api:field name="abstract" type="text" display-name="Abstract"><api:text>Semantic segmentation models gain robustness against adverse illumination conditions by taking advantage of complementary information from visible and thermal infrared (RGB-T) images. Despite its importance, most existing RGB-T semantic segmentation models directly adopt primitive fusion strategies, such as elementwise summation, to integrate multimodal features. Such strategies, unfortunately, overlook the modality discrepancies caused by inconsistent unimodal features obtained by two independent feature extractors, thus hindering the exploitation of cross-modal complementary information within the multimodal data. For that, we propose a novel network for RGB-T semantic segmentation, i.e. MDRNet + , which is an improved version of our previous work ABMDRNet. The core of MDRNet + is a brand new idea, termed the strategy of bridging-then-fusing, which mitigates modality discrepancies before cross-modal feature fusion. Concretely, an improved Modality Discrepancy Reduction (MDR + ) subnetwork is designed, which first extracts unimodal features and reduces their modality discrepancies. Afterward, discriminative multimodal features for RGB-T semantic segmentation are adaptively selected and integrated via several channel-weighted fusion (CWF) modules. Furthermore, a multiscale spatial context (MSC) module and a multiscale channel context (MCC) module are presented to effectively capture the contextual information. Finally, we elaborately assemble a challenging RGB-T semantic segmentation dataset, i.e., RTSS, for urban scene understanding to mitigate the lack of well-annotated training data. Comprehensive experiments demonstrate that our proposed model surpasses other state-of-the-art models on the MFNet, PST900, and RTSS datasets remarkably.</api:text></api:field><api:field name="acceptance-date" type="date" display-name="Date of acceptance"><api:date><api:day>27</api:day><api:month>12</api:month><api:year>2022</api:year></api:date></api:field><api:field name="authors" type="person-list" display-name="Authors"><api:people><api:person><api:last-name>Shenlu</api:last-name><api:initials>Z</api:initials><api:first-names>Zhao</api:first-names><api:separate-first-names><api:first-name>Zhao</api:first-name></api:separate-first-names><api:author-types><api:author-type>first</api:author-type></api:author-types></api:person><api:person><api:last-name>Liu</api:last-name><api:initials>Y</api:initials><api:first-names>Yichen</api:first-names><api:separate-first-names><api:first-name>Yichen</api:first-name></api:separate-first-names></api:person><api:person><api:last-name>Jiao</api:last-name><api:initials>Q</api:initials><api:first-names>Qiang</api:first-names><api:separate-first-names><api:first-name>Qiang</api:first-name></api:separate-first-names></api:person><api:person><api:last-name>Zhang</api:last-name><api:initials>Q</api:initials><api:first-names>Qiang</api:first-names><api:separate-first-names><api:first-name>Qiang</api:first-name></api:separate-first-names></api:person><api:person><api:links><api:link type="elements/user" id="36768" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/users/36768"/></api:links><api:last-name>Han</api:last-name><api:initials>J</api:initials><api:first-names>Jungong</api:first-names><api:separate-first-names><api:first-name>Jungong</api:first-name></api:separate-first-names><api:author-types><api:author-type>last</api:author-type></api:author-types><api:roles><api:role type="contributor">Supervision</api:role></api:roles></api:person></api:people></api:field><api:field name="c-data-accessibility" type="text" display-name="Data Accessibility"><api:text>DataN</api:text></api:field><api:field name="c-data-availability-statement" type="boolean" display-name="Data Availability Statement"><api:boolean>false</api:boolean></api:field><api:field name="c-licence-statement" type="boolean" display-name="Licence statement?"><api:boolean>false</api:boolean></api:field><api:field name="c-ref-no-issn" type="boolean" display-name="REF No ISSN"><api:boolean>false</api:boolean></api:field><api:field name="c-ref-pre-2014" type="boolean" display-name="REF pre-2014"><api:boolean>false</api:boolean></api:field><api:field name="doi" type="text" display-name="DOI"><api:text>10.1109/TNNLS.2022.3233089</api:text><api:links><api:link type="doi" href="http://doi.org/10.1109/TNNLS.2022.3233089"/><api:link type="altmetric" href="http://www.altmetric.com/details.php?doi=10.1109/TNNLS.2022.3233089"/></api:links></api:field><api:field name="eissn" type="text" display-name="eISSN"><api:text>2162-2388</api:text><api:links><api:link type="elements/journal" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/journals/988802"/></api:links></api:field><api:field name="issn" type="text" display-name="ISSN"><api:text>2162-237X</api:text><api:links><api:link type="elements/journal" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/journals/988802"/></api:links></api:field><api:field name="journal" type="text" display-name="Journal"><api:text>IEEE Transactions on Neural Networks and Learning Systems</api:text></api:field><api:field name="online-publication-date" type="date" display-name="Online publication date"><api:date><api:day>6</api:day><api:month>1</api:month><api:year>2023</api:year></api:date></api:field><api:field name="publication-date" type="date" display-name="Publication date"><api:date><api:day>6</api:day><api:month>1</api:month><api:year>2023</api:year></api:date></api:field><api:field name="publication-status" type="text" display-name="Status"><api:text>Published online</api:text></api:field><api:field name="publisher" type="text" display-name="Publisher"><api:text>Institute of Electrical and Electronics Engineers</api:text></api:field><api:field name="record-created-at-source-date" type="date" display-name="Record created at source"><api:date><api:day>2</api:day><api:month>2</api:month><api:year>2023</api:year></api:date></api:field><api:field name="title" type="text" display-name="Title"><api:text>Mitigating modality discrepancies for RGB-T semantic segmentation</api:text></api:field><api:field name="types" type="list" display-name="Sub types"><api:items><api:item>Original research article</api:item></api:items></api:field></api:native></api:record><api:record format="native" id="4260589" source-id="13" source-name="crossref" source-display-name="Crossref" id-at-source="10.1109/tnnls.2022.3233089" last-modified-when="2024-02-29T12:23:18.277+00:00"><api:native><api:field name="authors" type="person-list" display-name="Authors"><api:people><api:person><api:last-name>Zhao</api:last-name><api:initials>S</api:initials><api:first-names>Shenlu</api:first-names><api:separate-first-names><api:first-name>Shenlu</api:first-name></api:separate-first-names><api:identifiers><api:identifier scheme="orcid">0000-0002-3788-2938</api:identifier></api:identifiers></api:person><api:person><api:last-name>Liu</api:last-name><api:initials>Y</api:initials><api:first-names>Yichen</api:first-names><api:separate-first-names><api:first-name>Yichen</api:first-name></api:separate-first-names></api:person><api:person><api:last-name>Jiao</api:last-name><api:initials>Q</api:initials><api:first-names>Qiang</api:first-names><api:separate-first-names><api:first-name>Qiang</api:first-name></api:separate-first-names><api:identifiers><api:identifier scheme="orcid">0000-0002-4725-5284</api:identifier></api:identifiers></api:person><api:person><api:last-name>Zhang</api:last-name><api:initials>Q</api:initials><api:first-names>Qiang</api:first-names><api:separate-first-names><api:first-name>Qiang</api:first-name></api:separate-first-names><api:identifiers><api:identifier scheme="orcid">0000-0002-2828-9905</api:identifier></api:identifiers></api:person><api:person><api:links><api:link type="elements/user" id="36768" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/users/36768"/></api:links><api:last-name>Han</api:last-name><api:initials>J</api:initials><api:first-names>Jungong</api:first-names><api:separate-first-names><api:first-name>Jungong</api:first-name></api:separate-first-names><api:identifiers><api:identifier scheme="orcid">0000-0003-4361-956X</api:identifier></api:identifiers></api:person></api:people></api:field><api:field name="doi" type="text" display-name="DOI"><api:text>10.1109/tnnls.2022.3233089</api:text><api:links><api:link type="doi" href="http://doi.org/10.1109/tnnls.2022.3233089"/><api:link type="altmetric" href="http://www.altmetric.com/details.php?doi=10.1109/tnnls.2022.3233089"/></api:links></api:field><api:field name="eissn" type="text" display-name="eISSN"><api:text>2162-2388</api:text><api:links><api:link type="elements/journal" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/journals/988802"/></api:links></api:field><api:field name="issn" type="text" display-name="ISSN"><api:text>2162-237X</api:text><api:links><api:link type="elements/journal" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/journals/988802"/></api:links></api:field><api:field name="journal" type="text" display-name="Journal"><api:text>IEEE Transactions on Neural Networks and Learning Systems</api:text></api:field><api:field name="pagination" type="pagination" display-name="Pagination"><api:pagination><api:begin-page>1</api:begin-page><api:end-page>15</api:end-page></api:pagination></api:field><api:field name="publication-date" type="date" display-name="Publication date"><api:date><api:year>2024</api:year></api:date></api:field><api:field name="publication-status" type="text" display-name="Status"><api:text>Published</api:text></api:field><api:field name="publisher" type="text" display-name="Publisher"><api:text>Institute of Electrical and Electronics Engineers (IEEE)</api:text></api:field><api:field name="publisher-url" type="text" display-name="Link 2"><api:text>http://dx.doi.org/10.1109/tnnls.2022.3233089</api:text></api:field><api:field name="record-created-at-source-date" type="date" display-name="Record created at source"><api:date><api:day>15</api:day><api:month>2</api:month><api:year>2024</api:year></api:date></api:field><api:field name="title" type="text" display-name="Title"><api:text>Mitigating Modality Discrepancies for RGB-T Semantic Segmentation</api:text></api:field></api:native></api:record><api:record format="native" id="4531749" source-id="7" source-name="scopus" source-display-name="Scopus" id-at-source="2-s2.0-85147315913" last-modified-when="2024-04-10T13:10:37.503+01:00"><api:citation-count>9</api:citation-count><api:native><api:field name="abstract" type="text" display-name="Abstract"><api:text>Semantic segmentation models gain robustness against adverse illumination conditions by taking advantage of complementary information from visible and thermal infrared (RGB-T) images. Despite its importance, most existing RGB-T semantic segmentation models directly adopt primitive fusion strategies, such as elementwise summation, to integrate multimodal features. Such strategies, unfortunately, overlook the modality discrepancies caused by inconsistent unimodal features obtained by two independent feature extractors, thus hindering the exploitation of cross-modal complementary information within the multimodal data. For that, we propose a novel network for RGB-T semantic segmentation, i.e. MDRNet&lt;inline-formula&gt; &lt;tex-math notation="LaTeX"&gt;$+$&lt;/tex-math&gt; &lt;/inline-formula&gt;, which is an improved version of our previous work ABMDRNet. The core of MDRNet&lt;inline-formula&gt; &lt;tex-math notation="LaTeX"&gt;$+$&lt;/tex-math&gt; &lt;/inline-formula&gt; is a brand new idea, termed the strategy of bridging-then-fusing, which mitigates modality discrepancies before cross-modal feature fusion. Concretely, an improved Modality Discrepancy Reduction (MDR&lt;inline-formula&gt; &lt;tex-math notation="LaTeX"&gt;$+$&lt;/tex-math&gt; &lt;/inline-formula&gt;) subnetwork is designed, which first extracts unimodal features and reduces their modality discrepancies. Afterward, discriminative multimodal features for RGB-T semantic segmentation are adaptively selected and integrated via several channel-weighted fusion (CWF) modules. Furthermore, a multiscale spatial context (MSC) module and a multiscale channel context (MCC) module are presented to effectively capture the contextual information. Finally, we elaborately assemble a challenging RGB-T semantic segmentation dataset, i.e., RTSS, for urban scene understanding to mitigate the lack of well-annotated training data. Comprehensive experiments demonstrate that our proposed model surpasses other state-of-the-art models on the MFNet, PST900, and RTSS datasets remarkably.</api:text></api:field><api:field name="authors" type="person-list" display-name="Authors"><api:people><api:person><api:last-name>Zhao</api:last-name><api:initials>S</api:initials><api:first-names>S</api:first-names><api:separate-first-names><api:first-name>S</api:first-name></api:separate-first-names><api:addresses><api:address iso-country-code="CN"><api:line type="organisation">Xidian University</api:line><api:line type="city">Xi'an</api:line><api:line type="country">China</api:line></api:address></api:addresses><api:identifiers><api:identifier scheme="scopus-author-id">57419254900</api:identifier></api:identifiers></api:person><api:person><api:last-name>Liu</api:last-name><api:initials>Y</api:initials><api:first-names>Y</api:first-names><api:separate-first-names><api:first-name>Y</api:first-name></api:separate-first-names><api:addresses><api:address iso-country-code="CN"><api:line type="organisation">Xidian University</api:line><api:line type="city">Xi'an</api:line><api:line type="country">China</api:line></api:address></api:addresses><api:identifiers><api:identifier scheme="scopus-author-id">57460387300</api:identifier></api:identifiers></api:person><api:person><api:last-name>Jiao</api:last-name><api:initials>Q</api:initials><api:first-names>Q</api:first-names><api:separate-first-names><api:first-name>Q</api:first-name></api:separate-first-names><api:addresses><api:address iso-country-code="CN"><api:line type="organisation">Xidian University</api:line><api:line type="city">Xi'an</api:line><api:line type="country">China</api:line></api:address></api:addresses><api:identifiers><api:identifier scheme="scopus-author-id">56819801000</api:identifier></api:identifiers></api:person><api:person><api:last-name>Zhang</api:last-name><api:initials>Q</api:initials><api:first-names>Q</api:first-names><api:separate-first-names><api:first-name>Q</api:first-name></api:separate-first-names><api:addresses><api:address iso-country-code="CN"><api:line type="organisation">Xidian University</api:line><api:line type="city">Xi'an</api:line><api:line type="country">China</api:line></api:address></api:addresses><api:identifiers><api:identifier scheme="scopus-author-id">55624487042</api:identifier></api:identifiers></api:person><api:person><api:links><api:link type="elements/user" id="36768" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/users/36768"/></api:links><api:last-name>Han</api:last-name><api:initials>J</api:initials><api:first-names>J</api:first-names><api:separate-first-names><api:first-name>J</api:first-name></api:separate-first-names><api:addresses><api:address iso-country-code="GB"><api:line type="organisation">Aberystwyth University</api:line><api:line type="city">Aberystwyth</api:line><api:line type="country">United Kingdom</api:line></api:address></api:addresses><api:identifiers><api:identifier scheme="scopus-author-id">14522692900</api:identifier></api:identifiers></api:person></api:people></api:field><api:field name="doi" type="text" display-name="DOI"><api:text>10.1109/TNNLS.2022.3233089</api:text><api:links><api:link type="doi" href="http://doi.org/10.1109/TNNLS.2022.3233089"/><api:link type="altmetric" href="http://www.altmetric.com/details.php?doi=10.1109/TNNLS.2022.3233089"/></api:links></api:field><api:field name="eissn" type="text" display-name="eISSN"><api:text>2162-2388</api:text><api:links><api:link type="elements/journal" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/journals/988802"/></api:links></api:field><api:field name="issn" type="text" display-name="ISSN"><api:text>2162-237X</api:text><api:links><api:link type="elements/journal" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/journals/988802"/></api:links></api:field><api:field name="journal" type="text" display-name="Journal"><api:text>IEEE Transactions on Neural Networks and Learning Systems</api:text></api:field><api:field name="publication-date" type="date" display-name="Publication date"><api:date><api:day>1</api:day><api:month>1</api:month><api:year>2023</api:year></api:date></api:field><api:field name="publication-status" type="text" display-name="Status"><api:text>Published</api:text></api:field><api:field name="title" type="text" display-name="Title"><api:text>Mitigating Modality Discrepancies for RGB-T Semantic Segmentation</api:text></api:field><api:field name="types" type="list" display-name="Sub types"><api:items><api:item>Journal Article</api:item></api:items></api:field></api:native></api:record><api:record format="native" id="4288362" source-id="2" source-name="pubmed" source-display-name="PubMed" id-at-source="37018600" last-modified-when="2023-04-27T12:31:33.33+01:00"><api:native><api:field name="abstract" type="text" display-name="Abstract"><api:text>Semantic segmentation models gain robustness against adverse illumination conditions by taking advantage of complementary information from visible and thermal infrared (RGB-T) images. Despite its importance, most existing RGB-T semantic segmentation models directly adopt primitive fusion strategies, such as elementwise summation, to integrate multimodal features. Such strategies, unfortunately, overlook the modality discrepancies caused by inconsistent unimodal features obtained by two independent feature extractors, thus hindering the exploitation of cross-modal complementary information within the multimodal data. For that, we propose a novel network for RGB-T semantic segmentation, i.e. MDRNet + , which is an improved version of our previous work ABMDRNet. The core of MDRNet + is a brand new idea, termed the strategy of bridging-then-fusing, which mitigates modality discrepancies before cross-modal feature fusion. Concretely, an improved Modality Discrepancy Reduction (MDR + ) subnetwork is designed, which first extracts unimodal features and reduces their modality discrepancies. Afterward, discriminative multimodal features for RGB-T semantic segmentation are adaptively selected and integrated via several channel-weighted fusion (CWF) modules. Furthermore, a multiscale spatial context (MSC) module and a multiscale channel context (MCC) module are presented to effectively capture the contextual information. Finally, we elaborately assemble a challenging RGB-T semantic segmentation dataset, i.e., RTSS, for urban scene understanding to mitigate the lack of well-annotated training data. Comprehensive experiments demonstrate that our proposed model surpasses other state-of-the-art models on the MFNet, PST900, and RTSS datasets remarkably.</api:text></api:field><api:field name="authors" type="person-list" display-name="Authors"><api:people><api:person><api:last-name>Zhao</api:last-name><api:initials>S</api:initials><api:first-names>Shenlu</api:first-names><api:separate-first-names><api:first-name>Shenlu</api:first-name></api:separate-first-names></api:person><api:person><api:last-name>Liu</api:last-name><api:initials>Y</api:initials><api:first-names>Yichen</api:first-names><api:separate-first-names><api:first-name>Yichen</api:first-name></api:separate-first-names></api:person><api:person><api:last-name>Jiao</api:last-name><api:initials>Q</api:initials><api:first-names>Qiang</api:first-names><api:separate-first-names><api:first-name>Qiang</api:first-name></api:separate-first-names></api:person><api:person><api:last-name>Zhang</api:last-name><api:initials>Q</api:initials><api:first-names>Qiang</api:first-names><api:separate-first-names><api:first-name>Qiang</api:first-name></api:separate-first-names></api:person><api:person><api:links><api:link type="elements/user" id="36768" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/users/36768"/></api:links><api:last-name>Han</api:last-name><api:initials>J</api:initials><api:first-names>Jungong</api:first-names><api:separate-first-names><api:first-name>Jungong</api:first-name></api:separate-first-names></api:person></api:people></api:field><api:field name="author-url" type="text" display-name="Link 1"><api:text>https://www.ncbi.nlm.nih.gov/pubmed/37018600</api:text></api:field><api:field name="doi" type="text" display-name="DOI"><api:text>10.1109/TNNLS.2022.3233089</api:text><api:links><api:link type="doi" href="http://doi.org/10.1109/TNNLS.2022.3233089"/><api:link type="altmetric" href="http://www.altmetric.com/details.php?doi=10.1109/TNNLS.2022.3233089"/></api:links></api:field><api:field name="eissn" type="text" display-name="eISSN"><api:text>2162-2388</api:text><api:links><api:link type="elements/journal" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/journals/988802"/></api:links></api:field><api:field name="journal" type="text" display-name="Journal"><api:text>IEEE Trans Neural Netw Learn Syst</api:text></api:field><api:field name="language" type="text" display-name="Language"><api:text>eng</api:text></api:field><api:field name="location" type="text" display-name="Country"><api:text>United States</api:text></api:field><api:field name="publication-date" type="date" display-name="Publication date"><api:date><api:day>6</api:day><api:month>1</api:month><api:year>2023</api:year></api:date></api:field><api:field name="publication-status" type="text" display-name="Status"><api:text>Published online</api:text></api:field><api:field name="title" type="text" display-name="Title"><api:text>Mitigating Modality Discrepancies for RGB-T Semantic Segmentation.</api:text></api:field><api:field name="types" type="list" display-name="Sub types"><api:items><api:item>Journal Article</api:item></api:items></api:field><api:field name="volume" type="text" display-name="Volume"><api:text>PP</api:text></api:field></api:native></api:record><api:record format="native" id="4288513" source-id="18" source-name="epmc" source-display-name="Europe PubMed Central" id-at-source="MED:37018600" last-modified-when="2024-03-01T01:39:58.587+00:00"><api:citation-count>0</api:citation-count><api:native><api:field name="abstract" type="text" display-name="Abstract"><api:text>Semantic segmentation models gain robustness against adverse illumination conditions by taking advantage of complementary information from visible and thermal infrared (RGB-T) images. Despite its importance, most existing RGB-T semantic segmentation models directly adopt primitive fusion strategies, such as elementwise summation, to integrate multimodal features. Such strategies, unfortunately, overlook the modality discrepancies caused by inconsistent unimodal features obtained by two independent feature extractors, thus hindering the exploitation of cross-modal complementary information within the multimodal data. For that, we propose a novel network for RGB-T semantic segmentation, i.e. MDRNet + , which is an improved version of our previous work ABMDRNet. The core of MDRNet + is a brand new idea, termed the strategy of bridging-then-fusing, which mitigates modality discrepancies before cross-modal feature fusion. Concretely, an improved Modality Discrepancy Reduction (MDR + ) subnetwork is designed, which first extracts unimodal features and reduces their modality discrepancies. Afterward, discriminative multimodal features for RGB-T semantic segmentation are adaptively selected and integrated via several channel-weighted fusion (CWF) modules. Furthermore, a multiscale spatial context (MSC) module and a multiscale channel context (MCC) module are presented to effectively capture the contextual information. Finally, we elaborately assemble a challenging RGB-T semantic segmentation dataset, i.e., RTSS, for urban scene understanding to mitigate the lack of well-annotated training data. Comprehensive experiments demonstrate that our proposed model surpasses other state-of-the-art models on the MFNet, PST900, and RTSS datasets remarkably.</api:text></api:field><api:field name="authors" type="person-list" display-name="Authors"><api:people><api:person><api:last-name>Zhao</api:last-name><api:initials>S</api:initials><api:first-names>Shenlu</api:first-names><api:separate-first-names><api:first-name>Shenlu</api:first-name></api:separate-first-names><api:identifiers><api:identifier scheme="orcid">0000-0002-3788-2938</api:identifier></api:identifiers></api:person><api:person><api:last-name>Liu</api:last-name><api:initials>Y</api:initials><api:first-names>Yichen</api:first-names><api:separate-first-names><api:first-name>Yichen</api:first-name></api:separate-first-names></api:person><api:person><api:last-name>Jiao</api:last-name><api:initials>Q</api:initials><api:first-names>Qiang</api:first-names><api:separate-first-names><api:first-name>Qiang</api:first-name></api:separate-first-names><api:identifiers><api:identifier scheme="orcid">0000-0002-4725-5284</api:identifier></api:identifiers></api:person><api:person><api:last-name>Zhang</api:last-name><api:initials>Q</api:initials><api:first-names>Qiang</api:first-names><api:separate-first-names><api:first-name>Qiang</api:first-name></api:separate-first-names></api:person><api:person><api:links><api:link type="elements/user" id="36768" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/users/36768"/></api:links><api:last-name>Han</api:last-name><api:initials>J</api:initials><api:first-names>Jungong</api:first-names><api:separate-first-names><api:first-name>Jungong</api:first-name></api:separate-first-names></api:person></api:people></api:field><api:field name="doi" type="text" display-name="DOI"><api:text>10.1109/tnnls.2022.3233089</api:text><api:links><api:link type="doi" href="http://doi.org/10.1109/tnnls.2022.3233089"/><api:link type="altmetric" href="http://www.altmetric.com/details.php?doi=10.1109/tnnls.2022.3233089"/></api:links></api:field><api:field name="eissn" type="text" display-name="eISSN"><api:text>2162-2388</api:text><api:links><api:link type="elements/journal" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/journals/988802"/></api:links></api:field><api:field name="external-identifiers" type="identifier-list" display-name="External identifiers"><api:identifiers><api:identifier scheme="pubmed">37018600</api:identifier></api:identifiers></api:field><api:field name="is-open-access" type="boolean" display-name="Open access"><api:boolean>false</api:boolean></api:field><api:field name="issn" type="text" display-name="ISSN"><api:text>2162-237X</api:text><api:links><api:link type="elements/journal" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/journals/988802"/></api:links></api:field><api:field name="journal" type="text" display-name="Journal"><api:text>IEEE transactions on neural networks and learning systems</api:text></api:field><api:field name="language" type="text" display-name="Language"><api:text>eng</api:text></api:field><api:field name="medium" type="text" display-name="Medium"><api:text>Print-Electronic</api:text></api:field><api:field name="online-publication-date" type="date" display-name="Online publication date"><api:date><api:day>6</api:day><api:month>1</api:month><api:year>2023</api:year></api:date></api:field><api:field name="publication-date" type="date" display-name="Publication date"><api:date><api:month>1</api:month><api:year>2023</api:year></api:date></api:field><api:field name="publication-status" type="text" display-name="Status"><api:text>Published</api:text></api:field><api:field name="record-created-at-source-date" type="date" display-name="Record created at source"><api:date><api:day>5</api:day><api:month>4</api:month><api:year>2023</api:year></api:date></api:field><api:field name="title" type="text" display-name="Title"><api:text>Mitigating Modality Discrepancies for RGB-T Semantic Segmentation.</api:text></api:field><api:field name="types" type="list" display-name="Sub types"><api:items><api:item>Journal Article</api:item></api:items></api:field><api:field name="volume" type="text" display-name="Volume"><api:text>PP</api:text></api:field></api:native></api:record><api:record format="native" id="4260599" source-id="10" source-name="dimensions" source-display-name="Dimensions" id-at-source="pub.1154302985" last-modified-when="2024-04-25T12:41:22.197+01:00"><api:citation-count>10</api:citation-count><api:native><api:field name="associated-identifiers" type="identifier-list" display-name="Associated Identifiers"><api:identifiers><api:identifier scheme="dimensions-grant-id">grant.8311571</api:identifier></api:identifiers></api:field><api:field name="authors" type="person-list" display-name="Authors"><api:people><api:person><api:last-name>Zhao</api:last-name><api:initials>S</api:initials><api:first-names>Shenlu</api:first-names><api:separate-first-names><api:first-name>Shenlu</api:first-name></api:separate-first-names><api:addresses><api:address iso-country-code="CN"><api:line type="organisation">Ministry of Education of the People's Republic of China</api:line><api:line type="city">Beijing</api:line><api:line type="country">China</api:line></api:address></api:addresses><api:identifiers><api:identifier scheme="dimensions-researcher-id">ur.014336234510.33</api:identifier></api:identifiers></api:person><api:person><api:last-name>Liu</api:last-name><api:initials>Y</api:initials><api:first-names>Yichen</api:first-names><api:separate-first-names><api:first-name>Yichen</api:first-name></api:separate-first-names><api:addresses><api:address iso-country-code="CN"><api:line type="organisation">Ministry of Education of the People's Republic of China</api:line><api:line type="city">Beijing</api:line><api:line type="country">China</api:line></api:address></api:addresses></api:person><api:person><api:last-name>Jiao</api:last-name><api:initials>Q</api:initials><api:first-names>Qiang</api:first-names><api:separate-first-names><api:first-name>Qiang</api:first-name></api:separate-first-names><api:addresses><api:address iso-country-code="CN"><api:line type="organisation">Ministry of Education of the People's Republic of China</api:line><api:line type="city">Beijing</api:line><api:line type="country">China</api:line></api:address></api:addresses><api:identifiers><api:identifier scheme="dimensions-researcher-id">ur.014332125767.32</api:identifier></api:identifiers></api:person><api:person><api:last-name>Zhang</api:last-name><api:initials>Q</api:initials><api:first-names>Qiang</api:first-names><api:separate-first-names><api:first-name>Qiang</api:first-name></api:separate-first-names><api:addresses><api:address iso-country-code="CN"><api:line type="organisation">Ministry of Education of the People's Republic of China</api:line><api:line type="city">Beijing</api:line><api:line type="country">China</api:line></api:address></api:addresses><api:identifiers><api:identifier scheme="dimensions-researcher-id">ur.07356503437.94</api:identifier></api:identifiers></api:person><api:person><api:links><api:link type="elements/user" id="36768" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/users/36768"/></api:links><api:last-name>Han</api:last-name><api:initials>J</api:initials><api:first-names>Jungong</api:first-names><api:separate-first-names><api:first-name>Jungong</api:first-name></api:separate-first-names><api:addresses><api:address iso-country-code="GB"><api:line type="organisation">Aberystwyth University</api:line><api:line type="city">Aberystwyth</api:line><api:line type="country">United Kingdom</api:line></api:address></api:addresses><api:identifiers><api:identifier scheme="dimensions-researcher-id">ur.013141527713.39</api:identifier></api:identifiers></api:person></api:people></api:field><api:field name="doi" type="text" display-name="DOI"><api:text>10.1109/tnnls.2022.3233089</api:text><api:links><api:link type="doi" href="http://doi.org/10.1109/tnnls.2022.3233089"/><api:link type="altmetric" href="http://www.altmetric.com/details.php?doi=10.1109/tnnls.2022.3233089"/></api:links></api:field><api:field name="external-identifiers" type="identifier-list" display-name="External identifiers"><api:identifiers><api:identifier scheme="pubmed">37018600</api:identifier></api:identifiers></api:field><api:field name="issn" type="text" display-name="ISSN"><api:text>2162-237X</api:text><api:links><api:link type="elements/journal" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/journals/988802"/></api:links></api:field><api:field name="issue" type="text" display-name="Issue"><api:text>99</api:text></api:field><api:field name="journal" type="text" display-name="Journal"><api:text>IEEE Transactions on Neural Networks and Learning Systems</api:text></api:field><api:field name="keywords" type="keyword-list" display-name="Keywords"><api:keywords><api:keyword scheme="for-2020">4605 Data Management and Data Science</api:keyword><api:keyword scheme="for-2020">46 Information and Computing Sciences</api:keyword><api:keyword scheme="for-2020">4603 Computer Vision and Multimedia Computation</api:keyword></api:keywords></api:field><api:field name="online-publication-date" type="date" display-name="Online publication date"><api:date><api:day>6</api:day><api:month>1</api:month><api:year>2023</api:year></api:date></api:field><api:field name="open-access-status" type="text" display-name="Open access status"><api:text>Green OA</api:text></api:field><api:field name="pagination" type="pagination" display-name="Pagination"><api:pagination><api:begin-page>1</api:begin-page><api:end-page>15</api:end-page></api:pagination></api:field><api:field name="publication-date" type="date" display-name="Publication date"><api:date><api:day>6</api:day><api:month>1</api:month><api:year>2023</api:year></api:date></api:field><api:field name="publisher" type="text" display-name="Publisher"><api:text>Institute of Electrical and Electronics Engineers (IEEE)</api:text></api:field><api:field name="record-created-at-source-date" type="date" display-name="Record created at source"><api:date><api:day>8</api:day><api:month>1</api:month><api:year>2023</api:year></api:date></api:field><api:field name="title" type="text" display-name="Title"><api:text>Mitigating Modality Discrepancies for RGB-T Semantic Segmentation</api:text></api:field><api:field name="volume" type="text" display-name="Volume"><api:text>PP</api:text></api:field></api:native></api:record><api:record format="native" id="4532207" source-id="11" source-name="wos-lite" source-display-name="Web of Science (Lite)" id-at-source="WOS:000915817700001" last-modified-when="2024-04-25T03:31:23.377+01:00"><api:citation-count>7</api:citation-count><api:native><api:field name="authors" type="person-list" display-name="Authors"><api:people><api:person><api:last-name>Zhao</api:last-name><api:initials>S</api:initials><api:first-names>Shenlu</api:first-names><api:separate-first-names><api:first-name>Shenlu</api:first-name></api:separate-first-names></api:person><api:person><api:last-name>Liu</api:last-name><api:initials>Y</api:initials><api:first-names>Yichen</api:first-names><api:separate-first-names><api:first-name>Yichen</api:first-name></api:separate-first-names></api:person><api:person><api:last-name>Jiao</api:last-name><api:initials>Q</api:initials><api:first-names>Qiang</api:first-names><api:separate-first-names><api:first-name>Qiang</api:first-name></api:separate-first-names></api:person><api:person><api:last-name>Zhang</api:last-name><api:initials>Q</api:initials><api:first-names>Qiang</api:first-names><api:separate-first-names><api:first-name>Qiang</api:first-name></api:separate-first-names></api:person><api:person><api:links><api:link type="elements/user" id="36768" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/users/36768"/></api:links><api:last-name>Han</api:last-name><api:initials>J</api:initials><api:first-names>Jungong</api:first-names><api:separate-first-names><api:first-name>Jungong</api:first-name></api:separate-first-names></api:person></api:people></api:field><api:field name="author-url" type="text" display-name="Link 1"><api:text>https://www.webofscience.com/api/gateway?GWVersion=2&amp;SrcApp=sheffield_elements_live&amp;SrcAuth=WosAPI&amp;KeyUT=WOS:000915817700001&amp;DestLinkType=FullRecord&amp;DestApp=WOS_CPL</api:text></api:field><api:field name="doi" type="text" display-name="DOI"><api:text>10.1109/TNNLS.2022.3233089</api:text><api:links><api:link type="doi" href="http://doi.org/10.1109/TNNLS.2022.3233089"/><api:link type="altmetric" href="http://www.altmetric.com/details.php?doi=10.1109/TNNLS.2022.3233089"/></api:links></api:field><api:field name="eissn" type="text" display-name="eISSN"><api:text>2162-2388</api:text><api:links><api:link type="elements/journal" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/journals/988802"/></api:links></api:field><api:field name="external-identifiers" type="identifier-list" display-name="External identifiers"><api:identifiers><api:identifier scheme="isidoc">7Z8QM</api:identifier><api:identifier scheme="pubmed">37018600</api:identifier></api:identifiers></api:field><api:field name="issn" type="text" display-name="ISSN"><api:text>2162-237X</api:text><api:links><api:link type="elements/journal" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/journals/988802"/></api:links></api:field><api:field name="journal" type="text" display-name="Journal"><api:text>IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS</api:text></api:field><api:field name="keywords" type="keyword-list" display-name="Keywords"><api:keywords><api:keyword>Bridging-then-fusing</api:keyword><api:keyword>contextual information</api:keyword><api:keyword>dataset</api:keyword><api:keyword>modality discrepancy reduction</api:keyword><api:keyword>RGB-T semantic segmentation</api:keyword></api:keywords></api:field><api:field name="publication-date" type="date" display-name="Publication date"><api:date><api:year>2023</api:year></api:date></api:field><api:field name="publication-status" type="text" display-name="Status"><api:text>Published</api:text></api:field><api:field name="title" type="text" display-name="Title"><api:text>Mitigating Modality Discrepancies for RGB-T Semantic Segmentation</api:text></api:field><api:field name="types" type="list" display-name="Sub types"><api:items><api:item>Article</api:item><api:item>Early Access</api:item></api:items></api:field></api:native></api:record></api:records><api:fields/><api:all-labels type="keyword-list"><api:keywords><api:keyword origin="record-data" source="eprints">Bridging-then-fusing</api:keyword><api:keyword origin="record-data" source="eprints">contextual information</api:keyword><api:keyword origin="record-data" source="eprints">dataset</api:keyword><api:keyword origin="record-data" source="eprints">modality discrepancy reduction</api:keyword><api:keyword origin="record-data" source="eprints">RGB-T semantic segmentation</api:keyword><api:keyword scheme="for-2020" origin="record-data" source="dimensions">4605 Data Management and Data Science</api:keyword><api:keyword scheme="for-2020" origin="record-data" source="dimensions">46 Information and Computing Sciences</api:keyword><api:keyword scheme="for-2020" origin="record-data" source="dimensions">4603 Computer Vision and Multimedia Computation</api:keyword><api:keyword origin="record-data" source="wos-lite">Bridging-then-fusing</api:keyword><api:keyword origin="record-data" source="wos-lite">contextual information</api:keyword><api:keyword origin="record-data" source="wos-lite">dataset</api:keyword><api:keyword origin="record-data" source="wos-lite">modality discrepancy reduction</api:keyword><api:keyword origin="record-data" source="wos-lite">RGB-T semantic segmentation</api:keyword></api:keywords></api:all-labels><api:journal issn="2162-237X" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/journals/988802" title="IEEE Transactions on Neural Networks and Learning Systems"><api:records><api:record source-name="summary"><api:title>IEEE Transactions on Neural Networks and Learning Systems</api:title></api:record></api:records></api:journal><api:relationships href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/publications/1524986/relationships"/><api:flagged-as-not-externally-funded>true</api:flagged-as-not-externally-funded></api:object></api:result></api:response>