<?xml version="1.0" encoding="utf-8"?><api:response xmlns:api="http://www.symplectic.co.uk/publications/api"><api:version uri="https://mypublications.shef.ac.uk/" elements-version="6.17.0.4095" schema-version="6.13" product-name="myPublications"/><api:request href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/publications/1284834"/><api:result><api:object category="publication" id="1284834" last-affected-when="2024-04-28T05:36:43.037+01:00" last-modified-when="2024-04-28T05:36:43.037+01:00" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/publications/1284834" created-when="2020-01-08T00:09:49.027+00:00" type-id="5" type-display-name="Journal article" type="journal-article"><api:privacy-level>Public</api:privacy-level><api:privacy-level-locked>false</api:privacy-level-locked><api:ever-approved>true</api:ever-approved><api:reporting-date-1>2020-01-01</api:reporting-date-1><api:allow-type-switching>true</api:allow-type-switching><api:records><api:record format="native" id="4225934" source-id="28" source-name="eprints" source-display-name="White Rose Research Online" id-at-source="155409" last-modified-when="2022-12-29T16:25:17.597+00:00"><api:native><api:field name="abstract" type="text" display-name="Abstract"><api:text>Automation of agricultural processes requires systems that can accurately detect and classify produce in real industrial environments that include variation in fruit appearance due to illumination, occlusion, seasons, weather conditions, etc. In this paper we combine a visual processing approach inspired by colour-opponent theory in humans with recent advancements in one-stage deep learning networks to accurately, rapidly and robustly detect ripe soft fruits (strawberries) in real industrial settings and using standard (RGB) camera input. The resultant system was tested on an existent data-set captured in controlled conditions as well our new real-world data-set captured on a real strawberry farm over two months. We utilise F1 score, the harmonic mean of precision and recall, to show our system matches the state-of-the-art detection accuracy ( F1 : 0.793 vs. 0.799) in controlled conditions; has greater generalisation and robustness to variation of spatial parameters (camera viewpoint) in the real-world data-set ( F1 : 0.744); and at a fraction of the computational cost allowing classification at almost 30fps. We propose that the L*a*b*Fruits system addresses some of the most pressing limitations of current fruit detection systems and is well-suited to application in areas such as yield forecasting and harvesting. Beyond the target application in agriculture this work also provides a proof-of-principle whereby increased performance is achieved through analysis of the domain data, capturing features at the input level rather than simply increasing model complexity.</api:text></api:field><api:field name="acceptance-date" type="date" display-name="Date of acceptance"><api:date><api:day>30</api:day><api:month>12</api:month><api:year>2019</api:year></api:date></api:field><api:field name="authors" type="person-list" display-name="Authors"><api:people><api:person><api:last-name>Kirk</api:last-name><api:initials>R</api:initials><api:first-names>R</api:first-names><api:separate-first-names><api:first-name>R</api:first-name></api:separate-first-names></api:person><api:person><api:last-name>Cielniak</api:last-name><api:initials>G</api:initials><api:first-names>G</api:first-names><api:separate-first-names><api:first-name>G</api:first-name></api:separate-first-names></api:person><api:person><api:links><api:link type="elements/user" id="25575" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/users/25575"/></api:links><api:last-name>Mangan</api:last-name><api:initials>M</api:initials><api:first-names>M</api:first-names><api:separate-first-names><api:first-name>M</api:first-name></api:separate-first-names></api:person></api:people></api:field><api:field name="doi" type="text" display-name="DOI"><api:text>10.3390/s20010275</api:text><api:links><api:link type="doi" href="http://doi.org/10.3390/s20010275"/><api:link type="altmetric" href="http://www.altmetric.com/details.php?doi=10.3390/s20010275"/></api:links></api:field><api:field name="eissn" type="text" display-name="eISSN"><api:text>1424-8220</api:text><api:links><api:link type="elements/journal" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/journals/957309"/></api:links></api:field><api:field name="issue" type="text" display-name="Issue"><api:text>1</api:text></api:field><api:field name="journal" type="text" display-name="Journal"><api:text>Sensors</api:text></api:field><api:field name="keywords" type="keyword-list" display-name="Keywords"><api:keywords><api:keyword>fruit detection</api:keyword><api:keyword>deep learning</api:keyword><api:keyword>computer vision</api:keyword><api:keyword>agricultural robotics</api:keyword><api:keyword>multi-modal</api:keyword><api:keyword>strawberry perception</api:keyword><api:keyword>fruit localisation</api:keyword><api:keyword>outdoor detection</api:keyword><api:keyword>bio-inspired</api:keyword><api:keyword>one-stage networks</api:keyword></api:keywords></api:field><api:field name="notes" type="text" display-name="Other information"><api:text>Â© 2020 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (http://creativecommons.org/licenses/by/4.0/).</api:text></api:field><api:field name="number" type="text" display-name="Article number"><api:text>275</api:text></api:field><api:field name="online-publication-date" type="date" display-name="Online publication date"><api:date><api:day>3</api:day><api:month>1</api:month><api:year>2020</api:year></api:date></api:field><api:field name="publication-date" type="date" display-name="Publication date"><api:date><api:day>1</api:day><api:month>1</api:month><api:year>2020</api:year></api:date></api:field><api:field name="publication-status" type="text" display-name="Status"><api:text>Published</api:text></api:field><api:field name="public-url" type="text" display-name="Public URL"><api:text>https://eprints.whiterose.ac.uk/id/eprint/155409</api:text></api:field><api:field name="publisher" type="text" display-name="Publisher"><api:text>MDPI</api:text></api:field><api:field name="record-created-at-source-date" type="date" display-name="Record created at source"><api:date><api:day>8</api:day><api:month>1</api:month><api:year>2020</api:year></api:date></api:field><api:field name="record-made-public-at-source-date" type="date" display-name="Record made publicly available"><api:date><api:day>11</api:day><api:month>2</api:month><api:year>2020</api:year></api:date></api:field><api:field name="repository-status" type="text" display-name="Availability"><api:text>Public</api:text></api:field><api:field name="title" type="text" display-name="Title"><api:text>L*a*b*fruits : a rapid and robust outdoor fruit detection system combining bio-inspired features with one-stage deep learning networks</api:text></api:field><api:field name="volume" type="text" display-name="Volume"><api:text>20</api:text></api:field><api:files><api:file proprietary-id="https://eprints.whiterose.ac.uk/id/file/4143845"><api:filename>sensors-20-00275.pdf</api:filename><api:file-url>https://eprints.whiterose.ac.uk/155409/1/sensors-20-00275.pdf</api:file-url><api:extension>pdf</api:extension><api:file-size>21082793</api:file-size><api:mime-type>application/pdf</api:mime-type><api:checksum algorithm="md5">45A260604A61FBDD46F727326721AD82</api:checksum><api:file-version>Published version</api:file-version></api:file></api:files></api:native></api:record><api:record format="native" id="3500461" source-id="1" source-name="manual" source-display-name="Manual" id-at-source="0F393F32-86D2-47FD-8D10-5BB3DCC26111" last-modified-when="2020-02-11T12:05:17.273+00:00" is-locked="true"><api:verification-status>verified</api:verification-status><api:verification-comment>11/02/2020 AH</api:verification-comment><api:native><api:field name="abstract" type="text" display-name="Abstract"><api:text>Automation of agricultural processes requires systems that can accurately detect and classify produce in real industrial environments that include variation in fruit appearance due to illumination, occlusion, seasons, weather conditions, etc. In this paper we combine a visual processing approach inspired by colour-opponent theory in humans with recent advancements in one-stage deep learning networks to accurately, rapidly and robustly detect ripe soft fruits (strawberries) in real industrial settings and using standard (RGB) camera input. The resultant system was tested on an existent data-set captured in controlled conditions as well our new real-world data-set captured on a real strawberry farm over two months. We utilise F1 score, the harmonic mean of precision and recall, to show our system matches the state-of-the-art detection accuracy ( F1 : 0.793 vs. 0.799) in controlled conditions; has greater generalisation and robustness to variation of spatial parameters (camera viewpoint) in the real-world data-set ( F1 : 0.744); and at a fraction of the computational cost allowing classification at almost 30fps. We propose that the L*a*b*Fruits system addresses some of the most pressing limitations of current fruit detection systems and is well-suited to application in areas such as yield forecasting and harvesting. Beyond the target application in agriculture this work also provides a proof-of-principle whereby increased performance is achieved through analysis of the domain data, capturing features at the input level rather than simply increasing model complexity.</api:text></api:field><api:field name="acceptance-date" type="date" display-name="Date of acceptance"><api:date><api:day>30</api:day><api:month>12</api:month><api:year>2019</api:year></api:date></api:field><api:field name="authors" type="person-list" display-name="Authors"><api:people><api:person><api:last-name>Kirk</api:last-name><api:initials>R</api:initials><api:first-names>Raymond</api:first-names><api:separate-first-names><api:first-name>Raymond</api:first-name></api:separate-first-names></api:person><api:person><api:last-name>Cielniak</api:last-name><api:initials>G</api:initials><api:first-names>Grzegorz</api:first-names><api:separate-first-names><api:first-name>Grzegorz</api:first-name></api:separate-first-names></api:person><api:person><api:links><api:link type="elements/user" id="25575" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/users/25575"/></api:links><api:last-name>Mangan</api:last-name><api:initials>M</api:initials><api:first-names>Michael</api:first-names><api:separate-first-names><api:first-name>Michael</api:first-name></api:separate-first-names></api:person></api:people></api:field><api:field name="c-goldoa" type="boolean" display-name="Gold Open Access?"><api:boolean>true</api:boolean></api:field><api:field name="c-ref-no-issn" type="boolean" display-name="REF No ISSN"><api:boolean>false</api:boolean></api:field><api:field name="c-ref-pre-2014" type="boolean" display-name="REF pre-2014"><api:boolean>false</api:boolean></api:field><api:field name="doi" type="text" display-name="DOI"><api:text>10.3390/s20010275</api:text><api:links><api:link type="doi" href="http://doi.org/10.3390/s20010275"/><api:link type="altmetric" href="http://www.altmetric.com/details.php?doi=10.3390/s20010275"/></api:links></api:field><api:field name="eissn" type="text" display-name="eISSN"><api:text>1424-8220</api:text><api:links><api:link type="elements/journal" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/journals/957309"/></api:links></api:field><api:field name="issue" type="text" display-name="Issue"><api:text>1</api:text></api:field><api:field name="journal" type="text" display-name="Journal"><api:text>Sensors</api:text></api:field><api:field name="keywords" type="keyword-list" display-name="Keywords"><api:keywords><api:keyword>fruit detection</api:keyword><api:keyword>deep learning</api:keyword><api:keyword>computer vision</api:keyword><api:keyword>agricultural robotics</api:keyword><api:keyword>multi-modal</api:keyword><api:keyword>strawberry perception</api:keyword><api:keyword>fruit localisation</api:keyword><api:keyword>outdoor detection</api:keyword><api:keyword>bio-inspired</api:keyword><api:keyword>one-stage networks</api:keyword></api:keywords></api:field><api:field name="language" type="text" display-name="Language"><api:text>en</api:text></api:field><api:field name="number" type="text" display-name="Article number"><api:text>275</api:text></api:field><api:field name="online-publication-date" type="date" display-name="Online publication date"><api:date><api:day>3</api:day><api:month>1</api:month><api:year>2020</api:year></api:date></api:field><api:field name="publication-date" type="date" display-name="Publication date"><api:date><api:day>1</api:day><api:month>1</api:month><api:year>2020</api:year></api:date></api:field><api:field name="publication-status" type="text" display-name="Status"><api:text>Published</api:text></api:field><api:field name="publisher" type="text" display-name="Publisher"><api:text>MDPI</api:text></api:field><api:field name="publisher-licence" type="text" display-name="Publisher licence"><api:text>CC BY</api:text></api:field><api:field name="record-created-at-source-date" type="date" display-name="Record created at source"><api:date><api:day>11</api:day><api:month>2</api:month><api:year>2020</api:year></api:date></api:field><api:field name="title" type="text" display-name="Title"><api:text>L*a*b*fruits : a rapid and robust outdoor fruit detection system combining bio-inspired features with one-stage deep learning networks</api:text></api:field><api:field name="types" type="list" display-name="Sub types"><api:items><api:item>Original research article</api:item></api:items></api:field><api:field name="volume" type="text" display-name="Volume"><api:text>20</api:text></api:field></api:native></api:record><api:record format="native" id="3483967" source-id="13" source-name="crossref" source-display-name="Crossref" id-at-source="10.3390/s20010275" last-modified-when="2022-09-12T03:35:48.047+01:00"><api:native><api:field name="abstract" type="text" display-name="Abstract"><api:text>&lt;jats:p&gt;Automation of agricultural processes requires systems that can accurately detect and classify produce in real industrial environments that include variation in fruit appearance due to illumination, occlusion, seasons, weather conditions, etc. In this paper we combine a visual processing approach inspired by colour-opponent theory in humans with recent advancements in one-stage deep learning networks to accurately, rapidly and robustly detect ripe soft fruits (strawberries) in real industrial settings and using standard (RGB) camera input. The resultant system was tested on an existent data-set captured in controlled conditions as well our new real-world data-set captured on a real strawberry farm over two months. We utilise     F 1     score, the harmonic mean of precision and recall, to show our system matches the state-of-the-art detection accuracy (    F 1    : 0.793 vs. 0.799) in controlled conditions; has greater generalisation and robustness to variation of spatial parameters (camera viewpoint) in the real-world data-set (    F 1    : 0.744); and at a fraction of the computational cost allowing classification at almost 30fps. We propose that the L*a*b*Fruits system addresses some of the most pressing limitations of current fruit detection systems and is well-suited to application in areas such as yield forecasting and harvesting. Beyond the target application in agriculture this work also provides a proof-of-principle whereby increased performance is achieved through analysis of the domain data, capturing features at the input level rather than simply increasing model complexity.&lt;/jats:p&gt;</api:text></api:field><api:field name="authors" type="person-list" display-name="Authors"><api:people><api:person><api:last-name>Kirk</api:last-name><api:initials>R</api:initials><api:first-names>Raymond</api:first-names><api:separate-first-names><api:first-name>Raymond</api:first-name></api:separate-first-names><api:identifiers><api:identifier scheme="orcid">0000-0001-5118-9358</api:identifier></api:identifiers></api:person><api:person><api:last-name>Cielniak</api:last-name><api:initials>G</api:initials><api:first-names>Grzegorz</api:first-names><api:separate-first-names><api:first-name>Grzegorz</api:first-name></api:separate-first-names><api:identifiers><api:identifier scheme="orcid">0000-0002-6299-8465</api:identifier></api:identifiers></api:person><api:person><api:links><api:link type="elements/user" id="25575" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/users/25575"/></api:links><api:last-name>Mangan</api:last-name><api:initials>M</api:initials><api:first-names>Michael</api:first-names><api:separate-first-names><api:first-name>Michael</api:first-name></api:separate-first-names><api:identifiers><api:identifier scheme="orcid">0000-0002-0293-8874</api:identifier></api:identifiers></api:person></api:people></api:field><api:field name="doi" type="text" display-name="DOI"><api:text>10.3390/s20010275</api:text><api:links><api:link type="doi" href="http://doi.org/10.3390/s20010275"/><api:link type="altmetric" href="http://www.altmetric.com/details.php?doi=10.3390/s20010275"/></api:links></api:field><api:field name="eissn" type="text" display-name="eISSN"><api:text>1424-8220</api:text><api:links><api:link type="elements/journal" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/journals/957309"/></api:links></api:field><api:field name="issue" type="text" display-name="Issue"><api:text>1</api:text></api:field><api:field name="journal" type="text" display-name="Journal"><api:text>Sensors</api:text></api:field><api:field name="language" type="text" display-name="Language"><api:text>en</api:text></api:field><api:field name="online-publication-date" type="date" display-name="Online publication date"><api:date><api:day>3</api:day><api:month>1</api:month><api:year>2020</api:year></api:date></api:field><api:field name="pagination" type="pagination" display-name="Pagination"><api:pagination><api:begin-page>275</api:begin-page><api:end-page>275</api:end-page></api:pagination></api:field><api:field name="publication-status" type="text" display-name="Status"><api:text>Published online</api:text></api:field><api:field name="publisher" type="text" display-name="Publisher"><api:text>MDPI AG</api:text></api:field><api:field name="publisher-url" type="text" display-name="Link 2"><api:text>http://dx.doi.org/10.3390/s20010275</api:text></api:field><api:field name="record-created-at-source-date" type="date" display-name="Record created at source"><api:date><api:day>3</api:day><api:month>1</api:month><api:year>2020</api:year></api:date></api:field><api:field name="title" type="text" display-name="Title"><api:text>L*a*b*Fruits: A Rapid and Robust Outdoor Fruit Detection System Combining Bio-Inspired Features with One-Stage Deep Learning Networks</api:text></api:field><api:field name="volume" type="text" display-name="Volume"><api:text>20</api:text></api:field></api:native></api:record><api:record format="native" id="3492330" source-id="7" source-name="scopus" source-display-name="Scopus" id-at-source="2-s2.0-85077703251" last-modified-when="2024-04-16T13:04:35.75+01:00"><api:citation-count>51</api:citation-count><api:native><api:field name="abstract" type="text" display-name="Abstract"><api:text>Automation of agricultural processes requires systems that can accurately detect and classify produce in real industrial environments that include variation in fruit appearance due to illumination, occlusion, seasons, weather conditions, etc. In this paper we combine a visual processing approach inspired by colour-opponent theory in humans with recent advancements in one-stage deep learning networks to accurately, rapidly and robustly detect ripe soft fruits (strawberries) in real industrial settings and using standard (RGB) camera input. The resultant system was tested on an existent data-set captured in controlled conditions as well our new real-world data-set captured on a real strawberry farm over two months. We utilise F1 score, the harmonic mean of precision and recall, to show our system matches the state-of-the-art detection accuracy (F1: 0.793 vs. 0.799) in controlled conditions; has greater generalisation and robustness to variation of spatial parameters (camera viewpoint) in the real-world data-set (F1: 0.744); and at a fraction of the computational cost allowing classification at almost 30fps. We propose that the L*a*b*Fruits system addresses some of the most pressing limitations of current fruit detection systems and is well-suited to application in areas such as yield forecasting and harvesting. Beyond the target application in agriculture this work also provides a proof-of-principle whereby increased performance is achieved through analysis of the domain data, capturing features at the input level rather than simply increasing model complexity.</api:text></api:field><api:field name="authors" type="person-list" display-name="Authors"><api:people><api:person><api:last-name>Kirk</api:last-name><api:initials>R</api:initials><api:first-names>R</api:first-names><api:separate-first-names><api:first-name>R</api:first-name></api:separate-first-names><api:addresses><api:address iso-country-code="GB"><api:line type="organisation">University of Lincoln</api:line><api:line type="city">Lincoln</api:line><api:line type="country">United Kingdom</api:line></api:address></api:addresses><api:identifiers><api:identifier scheme="scopus-author-id">57213191400</api:identifier></api:identifiers></api:person><api:person><api:last-name>Cielniak</api:last-name><api:initials>G</api:initials><api:first-names>G</api:first-names><api:separate-first-names><api:first-name>G</api:first-name></api:separate-first-names><api:addresses><api:address iso-country-code="GB"><api:line type="organisation">University of Lincoln</api:line><api:line type="city">Lincoln</api:line><api:line type="country">United Kingdom</api:line></api:address></api:addresses><api:identifiers><api:identifier scheme="scopus-author-id">6508366670</api:identifier></api:identifiers></api:person><api:person><api:links><api:link type="elements/user" id="25575" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/users/25575"/></api:links><api:last-name>Mangan</api:last-name><api:initials>M</api:initials><api:first-names>M</api:first-names><api:separate-first-names><api:first-name>M</api:first-name></api:separate-first-names><api:addresses><api:address iso-country-code="GB"><api:line type="organisation">The University of Sheffield</api:line><api:line type="city">Sheffield</api:line><api:line type="country">United Kingdom</api:line></api:address></api:addresses><api:identifiers><api:identifier scheme="scopus-author-id">23767651300</api:identifier></api:identifiers></api:person></api:people></api:field><api:field name="doi" type="text" display-name="DOI"><api:text>10.3390/s20010275</api:text><api:links><api:link type="doi" href="http://doi.org/10.3390/s20010275"/><api:link type="altmetric" href="http://www.altmetric.com/details.php?doi=10.3390/s20010275"/></api:links></api:field><api:field name="external-identifiers" type="identifier-list" display-name="External identifiers"><api:identifiers><api:identifier scheme="pubmed">31947829</api:identifier></api:identifiers></api:field><api:field name="issn" type="text" display-name="ISSN"><api:text>1424-8220</api:text><api:links><api:link type="elements/journal" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/journals/957309"/></api:links></api:field><api:field name="issue" type="text" display-name="Issue"><api:text>1</api:text></api:field><api:field name="journal" type="text" display-name="Journal"><api:text>Sensors (Switzerland)</api:text></api:field><api:field name="publication-date" type="date" display-name="Publication date"><api:date><api:day>1</api:day><api:month>1</api:month><api:year>2020</api:year></api:date></api:field><api:field name="publication-status" type="text" display-name="Status"><api:text>Published</api:text></api:field><api:field name="title" type="text" display-name="Title"><api:text>L*a*b*Fruits: A rapid and robust outdoor fruit detection system combining bio-inspired features with one-stage deep learning networks</api:text></api:field><api:field name="types" type="list" display-name="Sub types"><api:items><api:item>Journal Article</api:item></api:items></api:field><api:field name="volume" type="text" display-name="Volume"><api:text>20</api:text></api:field></api:native></api:record><api:record format="native" id="3490478" source-id="2" source-name="pubmed" source-display-name="PubMed" id-at-source="31947829" last-modified-when="2020-04-29T18:10:15.667+01:00"><api:native><api:field name="abstract" type="text" display-name="Abstract"><api:text>Automation of agricultural processes requires systems that can accurately detect and classify produce in real industrial environments that include variation in fruit appearance due to illumination, occlusion, seasons, weather conditions, etc. In this paper we combine a visual processing approach inspired by colour-opponent theory in humans with recent advancements in one-stage deep learning networks to accurately, rapidly and robustly detect ripe soft fruits (strawberries) in real industrial settings and using standard (RGB) camera input. The resultant system was tested on an existent data-set captured in controlled conditions as well our new real-world data-set captured on a real strawberry farm over two months. We utilise F 1 score, the harmonic mean of precision and recall, to show our system matches the state-of-the-art detection accuracy ( F 1 : 0.793 vs. 0.799) in controlled conditions; has greater generalisation and robustness to variation of spatial parameters (camera viewpoint) in the real-world data-set ( F 1 : 0.744); and at a fraction of the computational cost allowing classification at almost 30fps. We propose that the L*a*b*Fruits system addresses some of the most pressing limitations of current fruit detection systems and is well-suited to application in areas such as yield forecasting and harvesting. Beyond the target application in agriculture this work also provides a proof-of-principle whereby increased performance is achieved through analysis of the domain data, capturing features at the input level rather than simply increasing model complexity.</api:text></api:field><api:field name="acceptance-date" type="date" display-name="Date of acceptance"><api:date><api:day>30</api:day><api:month>12</api:month><api:year>2019</api:year></api:date></api:field><api:field name="authors" type="person-list" display-name="Authors"><api:people><api:person><api:last-name>Kirk</api:last-name><api:initials>R</api:initials><api:first-names>Raymond</api:first-names><api:separate-first-names><api:first-name>Raymond</api:first-name></api:separate-first-names><api:addresses><api:address iso-country-code="GB"><api:line type="full">Lincoln Centre for Autonomous Systems, School of Computer Science, University of Lincoln, Lincoln LN6 7TS, UK.</api:line></api:address></api:addresses><api:identifiers><api:identifier scheme="orcid">0000-0001-5118-9358</api:identifier></api:identifiers></api:person><api:person><api:last-name>Cielniak</api:last-name><api:initials>G</api:initials><api:first-names>Grzegorz</api:first-names><api:separate-first-names><api:first-name>Grzegorz</api:first-name></api:separate-first-names><api:addresses><api:address iso-country-code="GB"><api:line type="full">Lincoln Centre for Autonomous Systems, School of Computer Science, University of Lincoln, Lincoln LN6 7TS, UK.</api:line></api:address></api:addresses><api:identifiers><api:identifier scheme="orcid">0000-0002-6299-8465</api:identifier></api:identifiers></api:person><api:person><api:links><api:link type="elements/user" id="25575" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/users/25575"/></api:links><api:last-name>Mangan</api:last-name><api:initials>M</api:initials><api:first-names>Michael</api:first-names><api:separate-first-names><api:first-name>Michael</api:first-name></api:separate-first-names><api:addresses><api:address iso-country-code="GB"><api:line type="full">Sheffield Robotics, School of Computer Science, University of Sheffield, Sheffield S10 2TN, UK.</api:line></api:address></api:addresses><api:identifiers><api:identifier scheme="orcid">0000-0002-0293-8874</api:identifier></api:identifiers></api:person></api:people></api:field><api:field name="author-url" type="text" display-name="Link 1"><api:text>https://www.ncbi.nlm.nih.gov/pubmed/31947829</api:text></api:field><api:field name="doi" type="text" display-name="DOI"><api:text>10.3390/s20010275</api:text><api:links><api:link type="doi" href="http://doi.org/10.3390/s20010275"/><api:link type="altmetric" href="http://www.altmetric.com/details.php?doi=10.3390/s20010275"/></api:links></api:field><api:field name="eissn" type="text" display-name="eISSN"><api:text>1424-8220</api:text><api:links><api:link type="elements/journal" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/journals/957309"/></api:links></api:field><api:field name="external-identifiers" type="identifier-list" display-name="External identifiers"><api:identifiers><api:identifier scheme="pmc">PMC6983004</api:identifier></api:identifiers></api:field><api:field name="issue" type="text" display-name="Issue"><api:text>1</api:text></api:field><api:field name="journal" type="text" display-name="Journal"><api:text>Sensors (Basel)</api:text></api:field><api:field name="keywords" type="keyword-list" display-name="Keywords"><api:keywords><api:keyword>agricultural robotics</api:keyword><api:keyword>bio-inspired</api:keyword><api:keyword>computer vision</api:keyword><api:keyword>deep learning</api:keyword><api:keyword>fruit detection</api:keyword><api:keyword>fruit localisation</api:keyword><api:keyword>multi-modal</api:keyword><api:keyword>one-stage networks</api:keyword><api:keyword>outdoor detection</api:keyword><api:keyword>strawberry perception</api:keyword></api:keywords></api:field><api:field name="language" type="text" display-name="Language"><api:text>eng</api:text></api:field><api:field name="location" type="text" display-name="Country"><api:text>Switzerland</api:text></api:field><api:field name="pii" type="text" display-name="PII"><api:text>s20010275</api:text></api:field><api:field name="publication-date" type="date" display-name="Publication date"><api:date><api:day>3</api:day><api:month>1</api:month><api:year>2020</api:year></api:date></api:field><api:field name="publication-status" type="text" display-name="Status"><api:text>Published online</api:text></api:field><api:field name="record-made-public-at-source-date" type="date" display-name="Record made publicly available"><api:date><api:day>22</api:day><api:month>1</api:month><api:year>2020</api:year></api:date></api:field><api:field name="title" type="text" display-name="Title"><api:text>L*a*b*Fruits: A Rapid and Robust Outdoor Fruit Detection System Combining Bio-Inspired Features with One-Stage Deep Learning Networks.</api:text></api:field><api:field name="types" type="list" display-name="Sub types"><api:items><api:item>Journal Article</api:item></api:items></api:field><api:field name="volume" type="text" display-name="Volume"><api:text>20</api:text></api:field></api:native></api:record><api:record format="native" id="3490476" source-id="18" source-name="epmc" source-display-name="Europe PubMed Central" id-at-source="MED:31947829" last-modified-when="2024-01-31T01:19:23.263+00:00"><api:citation-count>6</api:citation-count><api:native><api:field name="abstract" type="text" display-name="Abstract"><api:text>Automation of agricultural processes requires systems that can accurately detect and classify produce in real industrial environments that include variation in fruit appearance due to illumination, occlusion, seasons, weather conditions, etc. In this paper we combine a visual processing approach inspired by colour-opponent theory in humans with recent advancements in one-stage deep learning networks to accurately, rapidly and robustly detect ripe soft fruits (strawberries) in real industrial settings and using standard (&lt;i&gt;RGB&lt;/i&gt;) camera input. The resultant system was tested on an existent data-set captured in controlled conditions as well our new real-world data-set captured on a real strawberry farm over two months. We utilise F 1 score, the harmonic mean of precision and recall, to show our system matches the state-of-the-art detection accuracy ( F 1 : 0.793 vs. 0.799) in controlled conditions; has greater generalisation and robustness to variation of spatial parameters (camera viewpoint) in the real-world data-set ( F 1 : 0.744); and at a fraction of the computational cost allowing classification at almost 30fps. We propose that the L*a*b*Fruits system addresses some of the most pressing limitations of current fruit detection systems and is well-suited to application in areas such as yield forecasting and harvesting. Beyond the target application in agriculture this work also provides a proof-of-principle whereby increased performance is achieved through analysis of the domain data, capturing features at the input level rather than simply increasing model complexity.</api:text></api:field><api:field name="addresses" type="address-list" display-name="Addresses"><api:addresses><api:address iso-country-code="GB"><api:line type="full">Lincoln Centre for Autonomous Systems, School of Computer Science, University of Lincoln, Lincoln LN6 7TS, UK.</api:line></api:address></api:addresses></api:field><api:field name="authors" type="person-list" display-name="Authors"><api:people><api:person><api:last-name>Kirk</api:last-name><api:initials>R</api:initials><api:first-names>Raymond</api:first-names><api:separate-first-names><api:first-name>Raymond</api:first-name></api:separate-first-names><api:identifiers><api:identifier scheme="orcid">0000-0001-5118-9358</api:identifier></api:identifiers></api:person><api:person><api:last-name>Cielniak</api:last-name><api:initials>G</api:initials><api:first-names>Grzegorz</api:first-names><api:separate-first-names><api:first-name>Grzegorz</api:first-name></api:separate-first-names><api:identifiers><api:identifier scheme="orcid">0000-0002-6299-8465</api:identifier></api:identifiers></api:person><api:person><api:links><api:link type="elements/user" id="25575" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/users/25575"/></api:links><api:last-name>Mangan</api:last-name><api:initials>M</api:initials><api:first-names>Michael</api:first-names><api:separate-first-names><api:first-name>Michael</api:first-name></api:separate-first-names><api:identifiers><api:identifier scheme="orcid">0000-0002-0293-8874</api:identifier></api:identifiers></api:person></api:people></api:field><api:field name="doi" type="text" display-name="DOI"><api:text>10.3390/s20010275</api:text><api:links><api:link type="doi" href="http://doi.org/10.3390/s20010275"/><api:link type="altmetric" href="http://www.altmetric.com/details.php?doi=10.3390/s20010275"/></api:links></api:field><api:field name="eissn" type="text" display-name="eISSN"><api:text>1424-8220</api:text><api:links><api:link type="elements/journal" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/journals/957309"/></api:links></api:field><api:field name="external-identifiers" type="identifier-list" display-name="External identifiers"><api:identifiers><api:identifier scheme="pubmed">31947829</api:identifier><api:identifier scheme="pmc">PMC6983004</api:identifier></api:identifiers></api:field><api:field name="is-open-access" type="boolean" display-name="Open access"><api:boolean>true</api:boolean></api:field><api:field name="issn" type="text" display-name="ISSN"><api:text>1424-8220</api:text><api:links><api:link type="elements/journal" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/journals/957309"/></api:links></api:field><api:field name="issue" type="text" display-name="Issue"><api:text>1</api:text></api:field><api:field name="journal" type="text" display-name="Journal"><api:text>Sensors (Basel, Switzerland)</api:text></api:field><api:field name="language" type="text" display-name="Language"><api:text>eng</api:text></api:field><api:field name="medium" type="text" display-name="Medium"><api:text>Electronic</api:text></api:field><api:field name="online-publication-date" type="date" display-name="Online publication date"><api:date><api:day>3</api:day><api:month>1</api:month><api:year>2020</api:year></api:date></api:field><api:field name="open-access-status" type="text" display-name="Open access status"><api:text>Open Access</api:text></api:field><api:field name="pagination" type="pagination" display-name="Pagination"><api:pagination><api:begin-page>E275</api:begin-page></api:pagination></api:field><api:field name="publication-date" type="date" display-name="Publication date"><api:date><api:month>1</api:month><api:year>2020</api:year></api:date></api:field><api:field name="publication-status" type="text" display-name="Status"><api:text>Published</api:text></api:field><api:field name="publisher-licence" type="text" display-name="Publisher licence"><api:text>CC BY</api:text></api:field><api:field name="record-created-at-source-date" type="date" display-name="Record created at source"><api:date><api:day>18</api:day><api:month>1</api:month><api:year>2020</api:year></api:date></api:field><api:field name="title" type="text" display-name="Title"><api:text>L*a*b*Fruits: A Rapid and Robust Outdoor Fruit Detection System Combining Bio-Inspired Features with One-Stage Deep Learning Networks.</api:text></api:field><api:field name="types" type="list" display-name="Sub types"><api:items><api:item>research-article</api:item><api:item>Journal Article</api:item></api:items></api:field><api:field name="volume" type="text" display-name="Volume"><api:text>20</api:text></api:field><api:files><api:file proprietary-id="https://www.mdpi.com/1424-8220/20/1/275/pdf?version=1578051619"><api:file-url>https://www.mdpi.com/1424-8220/20/1/275/pdf?version=1578051619</api:file-url><api:extension>pdf</api:extension><api:is-open-access>true</api:is-open-access><api:file-version>Published version</api:file-version></api:file><api:file proprietary-id="https://europepmc.org/articles/PMC6983004?pdf=render"><api:file-url>https://europepmc.org/articles/PMC6983004?pdf=render</api:file-url><api:extension>pdf</api:extension><api:is-open-access>true</api:is-open-access><api:file-version>Published version</api:file-version></api:file></api:files></api:native></api:record><api:record format="native" id="3494336" source-id="10" source-name="dimensions" source-display-name="Dimensions" id-at-source="pub.1124029599" last-modified-when="2024-04-28T05:36:43.04+01:00"><api:citation-count>54</api:citation-count><api:native><api:field name="altmetric-attention-score" type="integer" display-name="Altmetric attention score"><api:integer>4</api:integer></api:field><api:field name="associated-identifiers" type="identifier-list" display-name="Associated Identifiers"><api:identifiers><api:identifier scheme="dimensions-grant-id">grant.8483143</api:identifier></api:identifiers></api:field><api:field name="authors" type="person-list" display-name="Authors"><api:people><api:person><api:last-name>Kirk</api:last-name><api:initials>R</api:initials><api:first-names>Raymond</api:first-names><api:separate-first-names><api:first-name>Raymond</api:first-name></api:separate-first-names><api:addresses><api:address iso-country-code="GB"><api:line type="organisation">University of Lincoln</api:line><api:line type="city">Lincoln</api:line><api:line type="country">United Kingdom</api:line></api:address></api:addresses><api:identifiers><api:identifier scheme="dimensions-researcher-id">ur.012247357223.17</api:identifier></api:identifiers><api:author-types><api:author-type>corresponding</api:author-type></api:author-types></api:person><api:person><api:last-name>Cielniak</api:last-name><api:initials>G</api:initials><api:first-names>Grzegorz</api:first-names><api:separate-first-names><api:first-name>Grzegorz</api:first-name></api:separate-first-names><api:addresses><api:address iso-country-code="GB"><api:line type="organisation">University of Lincoln</api:line><api:line type="city">Lincoln</api:line><api:line type="country">United Kingdom</api:line></api:address></api:addresses><api:identifiers><api:identifier scheme="dimensions-researcher-id">ur.0615002723.39</api:identifier></api:identifiers><api:author-types><api:author-type>corresponding</api:author-type></api:author-types></api:person><api:person><api:links><api:link type="elements/user" id="25575" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/users/25575"/></api:links><api:last-name>Mangan</api:last-name><api:initials>M</api:initials><api:first-names>Michael</api:first-names><api:separate-first-names><api:first-name>Michael</api:first-name></api:separate-first-names><api:addresses><api:address iso-country-code="GB"><api:line type="organisation">University of Sheffield</api:line><api:line type="city">Sheffield</api:line><api:line type="country">United Kingdom</api:line></api:address></api:addresses><api:identifiers><api:identifier scheme="dimensions-researcher-id">ur.0607047473.13</api:identifier></api:identifiers></api:person></api:people></api:field><api:field name="doi" type="text" display-name="DOI"><api:text>10.3390/s20010275</api:text><api:links><api:link type="doi" href="http://doi.org/10.3390/s20010275"/><api:link type="altmetric" href="http://www.altmetric.com/details.php?doi=10.3390/s20010275"/></api:links></api:field><api:field name="external-identifiers" type="identifier-list" display-name="External identifiers"><api:identifiers><api:identifier scheme="pubmed">31947829</api:identifier><api:identifier scheme="pmc">PMC6983004</api:identifier></api:identifiers></api:field><api:field name="field-citation-ratio" type="decimal" display-name="Field citation ratio"><api:decimal>17.58</api:decimal></api:field><api:field name="issn" type="text" display-name="ISSN"><api:text>1424-8220</api:text><api:links><api:link type="elements/journal" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/journals/957309"/></api:links></api:field><api:field name="issue" type="text" display-name="Issue"><api:text>1</api:text></api:field><api:field name="journal" type="text" display-name="Journal"><api:text>Sensors</api:text></api:field><api:field name="keywords" type="keyword-list" display-name="Keywords"><api:keywords><api:keyword scheme="for-2020">4605 Data Management and Data Science</api:keyword><api:keyword scheme="for-2020">46 Information and Computing Sciences</api:keyword><api:keyword scheme="for-2020">4611 Machine Learning</api:keyword><api:keyword scheme="sdg">2 Zero Hunger</api:keyword></api:keywords></api:field><api:field name="online-publication-date" type="date" display-name="Online publication date"><api:date><api:day>3</api:day><api:month>1</api:month><api:year>2020</api:year></api:date></api:field><api:field name="open-access-status" type="text" display-name="Open access status"><api:text>Gold OA</api:text></api:field><api:field name="pagination" type="pagination" display-name="Pagination"><api:pagination><api:begin-page>275</api:begin-page></api:pagination></api:field><api:field name="publisher" type="text" display-name="Publisher"><api:text>MDPI</api:text></api:field><api:field name="record-created-at-source-date" type="date" display-name="Record created at source"><api:date><api:day>12</api:day><api:month>1</api:month><api:year>2020</api:year></api:date></api:field><api:field name="relative-citation-ratio" type="decimal" display-name="Relative citation ratio"><api:decimal>1.41</api:decimal></api:field><api:field name="title" type="text" display-name="Title"><api:text>L*a*b*Fruits: A Rapid and Robust Outdoor Fruit Detection System Combining Bio-Inspired Features with One-Stage Deep Learning Networks</api:text></api:field><api:field name="volume" type="text" display-name="Volume"><api:text>20</api:text></api:field></api:native></api:record><api:record format="native" id="3504700" source-id="11" source-name="wos-lite" source-display-name="Web of Science (Lite)" id-at-source="WOS:000510493100275" last-modified-when="2024-03-15T02:49:10.87+00:00"><api:citation-count>40</api:citation-count><api:native><api:field name="authors" type="person-list" display-name="Authors"><api:people><api:person><api:last-name>Kirk</api:last-name><api:initials>R</api:initials><api:first-names>Raymond</api:first-names><api:separate-first-names><api:first-name>Raymond</api:first-name></api:separate-first-names></api:person><api:person><api:last-name>Cielniak</api:last-name><api:initials>G</api:initials><api:first-names>Grzegorz</api:first-names><api:separate-first-names><api:first-name>Grzegorz</api:first-name></api:separate-first-names></api:person><api:person><api:links><api:link type="elements/user" id="25575" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/users/25575"/></api:links><api:last-name>Mangan</api:last-name><api:initials>M</api:initials><api:first-names>Michael</api:first-names><api:separate-first-names><api:first-name>Michael</api:first-name></api:separate-first-names></api:person></api:people></api:field><api:field name="author-url" type="text" display-name="Link 1"><api:text>https://www.webofscience.com/api/gateway?GWVersion=2&amp;SrcApp=sheffield_elements_live&amp;SrcAuth=WosAPI&amp;KeyUT=WOS:000510493100275&amp;DestLinkType=FullRecord&amp;DestApp=WOS_CPL</api:text></api:field><api:field name="doi" type="text" display-name="DOI"><api:text>10.3390/s20010275</api:text><api:links><api:link type="doi" href="http://doi.org/10.3390/s20010275"/><api:link type="altmetric" href="http://www.altmetric.com/details.php?doi=10.3390/s20010275"/></api:links></api:field><api:field name="eissn" type="text" display-name="eISSN"><api:text>1424-8220</api:text><api:links><api:link type="elements/journal" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/journals/957309"/></api:links></api:field><api:field name="external-identifiers" type="identifier-list" display-name="External identifiers"><api:identifiers><api:identifier scheme="isidoc">KH2QT</api:identifier><api:identifier scheme="pubmed">31947829</api:identifier></api:identifiers></api:field><api:field name="issue" type="text" display-name="Issue"><api:text>1</api:text></api:field><api:field name="journal" type="text" display-name="Journal"><api:text>SENSORS</api:text></api:field><api:field name="keywords" type="keyword-list" display-name="Keywords"><api:keywords><api:keyword>fruit detection</api:keyword><api:keyword>deep learning</api:keyword><api:keyword>computer vision</api:keyword><api:keyword>agricultural robotics</api:keyword><api:keyword>multi-modal</api:keyword><api:keyword>strawberry perception</api:keyword><api:keyword>fruit localisation</api:keyword><api:keyword>outdoor detection</api:keyword><api:keyword>bio-inspired</api:keyword><api:keyword>one-stage networks</api:keyword></api:keywords></api:field><api:field name="number" type="text" display-name="Article number"><api:text>ARTN 275</api:text></api:field><api:field name="publication-date" type="date" display-name="Publication date"><api:date><api:year>2020</api:year></api:date></api:field><api:field name="publication-status" type="text" display-name="Status"><api:text>Published</api:text></api:field><api:field name="title" type="text" display-name="Title"><api:text>L*a*b*Fruits: A Rapid and Robust Outdoor Fruit Detection System Combining Bio-Inspired Features with One-Stage Deep Learning Networks</api:text></api:field><api:field name="types" type="list" display-name="Sub types"><api:items><api:item>Article</api:item></api:items></api:field><api:field name="volume" type="text" display-name="Volume"><api:text>20</api:text></api:field></api:native></api:record></api:records><api:fields><api:field name="fulltext-comment" type="text" display-name="fulltext-comment"><api:text>published open access manuscript</api:text></api:field></api:fields><api:all-labels type="keyword-list"><api:keywords><api:keyword origin="record-data" source="eprints">fruit detection</api:keyword><api:keyword origin="record-data" source="eprints">deep learning</api:keyword><api:keyword origin="record-data" source="eprints">computer vision</api:keyword><api:keyword origin="record-data" source="eprints">agricultural robotics</api:keyword><api:keyword origin="record-data" source="eprints">multi-modal</api:keyword><api:keyword origin="record-data" source="eprints">strawberry perception</api:keyword><api:keyword origin="record-data" source="eprints">fruit localisation</api:keyword><api:keyword origin="record-data" source="eprints">outdoor detection</api:keyword><api:keyword origin="record-data" source="eprints">bio-inspired</api:keyword><api:keyword origin="record-data" source="eprints">one-stage networks</api:keyword><api:keyword origin="record-data" source="manual">fruit detection</api:keyword><api:keyword origin="record-data" source="manual">deep learning</api:keyword><api:keyword origin="record-data" source="manual">computer vision</api:keyword><api:keyword origin="record-data" source="manual">agricultural robotics</api:keyword><api:keyword origin="record-data" source="manual">multi-modal</api:keyword><api:keyword origin="record-data" source="manual">strawberry perception</api:keyword><api:keyword origin="record-data" source="manual">fruit localisation</api:keyword><api:keyword origin="record-data" source="manual">outdoor detection</api:keyword><api:keyword origin="record-data" source="manual">bio-inspired</api:keyword><api:keyword origin="record-data" source="manual">one-stage networks</api:keyword><api:keyword origin="record-data" source="pubmed">agricultural robotics</api:keyword><api:keyword origin="record-data" source="pubmed">bio-inspired</api:keyword><api:keyword origin="record-data" source="pubmed">computer vision</api:keyword><api:keyword origin="record-data" source="pubmed">deep learning</api:keyword><api:keyword origin="record-data" source="pubmed">fruit detection</api:keyword><api:keyword origin="record-data" source="pubmed">fruit localisation</api:keyword><api:keyword origin="record-data" source="pubmed">multi-modal</api:keyword><api:keyword origin="record-data" source="pubmed">one-stage networks</api:keyword><api:keyword origin="record-data" source="pubmed">outdoor detection</api:keyword><api:keyword origin="record-data" source="pubmed">strawberry perception</api:keyword><api:keyword scheme="for-2020" origin="record-data" source="dimensions">4605 Data Management and Data Science</api:keyword><api:keyword scheme="for-2020" origin="record-data" source="dimensions">46 Information and Computing Sciences</api:keyword><api:keyword scheme="for-2020" origin="record-data" source="dimensions">4611 Machine Learning</api:keyword><api:keyword scheme="sdg" origin="record-data" source="dimensions">2 Zero Hunger</api:keyword><api:keyword origin="record-data" source="wos-lite">fruit detection</api:keyword><api:keyword origin="record-data" source="wos-lite">deep learning</api:keyword><api:keyword origin="record-data" source="wos-lite">computer vision</api:keyword><api:keyword origin="record-data" source="wos-lite">agricultural robotics</api:keyword><api:keyword origin="record-data" source="wos-lite">multi-modal</api:keyword><api:keyword origin="record-data" source="wos-lite">strawberry perception</api:keyword><api:keyword origin="record-data" source="wos-lite">fruit localisation</api:keyword><api:keyword origin="record-data" source="wos-lite">outdoor detection</api:keyword><api:keyword origin="record-data" source="wos-lite">bio-inspired</api:keyword><api:keyword origin="record-data" source="wos-lite">one-stage networks</api:keyword><api:keyword scheme="for" origin="issn-inferred">0301 Analytical Chemistry</api:keyword><api:keyword scheme="for" origin="issn-inferred">0502 Environmental Science and Management</api:keyword><api:keyword scheme="for" origin="issn-inferred">0602 Ecology</api:keyword><api:keyword scheme="for" origin="issn-inferred">0805 Distributed Computing</api:keyword><api:keyword scheme="for" origin="issn-inferred">0906 Electrical and Electronic Engineering</api:keyword><api:keyword scheme="science-metrix" origin="issn-inferred">Analytical Chemistry</api:keyword><api:keyword scheme="for-2020" origin="issn-inferred">3103 Ecology</api:keyword><api:keyword scheme="for-2020" origin="issn-inferred">4008 Electrical engineering</api:keyword><api:keyword scheme="for-2020" origin="issn-inferred">4009 Electronics, sensors and digital hardware</api:keyword><api:keyword scheme="for-2020" origin="issn-inferred">4104 Environmental management</api:keyword><api:keyword scheme="for-2020" origin="issn-inferred">4606 Distributed computing and systems software</api:keyword></api:keywords></api:all-labels><api:journal issn="1424-8220" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/journals/957309" title="Sensors"><api:records><api:record source-name="summary"><api:title>Sensors</api:title></api:record></api:records></api:journal><api:relationships href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/publications/1284834/relationships"/></api:object></api:result></api:response>