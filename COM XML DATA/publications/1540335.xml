<?xml version="1.0" encoding="utf-8"?><api:response xmlns:api="http://www.symplectic.co.uk/publications/api"><api:version uri="https://mypublications.shef.ac.uk/" elements-version="6.17.0.4095" schema-version="6.13" product-name="myPublications"/><api:request href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/publications/1540335"/><api:result><api:object category="publication" id="1540335" last-affected-when="2024-04-17T23:22:26.56+01:00" last-modified-when="2024-04-17T23:22:26.56+01:00" href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/publications/1540335" created-when="2023-05-17T23:06:07.607+01:00" type-id="4" type-display-name="Conference proceedings paper" type="conference"><api:privacy-level>Public</api:privacy-level><api:privacy-level-locked>false</api:privacy-level-locked><api:ever-approved>true</api:ever-approved><api:reporting-date-1>2021-01-01</api:reporting-date-1><api:allow-type-switching>true</api:allow-type-switching><api:records><api:record format="native" id="4301894" source-id="7" source-name="scopus" source-display-name="Scopus" id-at-source="2-s2.0-85134393142" last-modified-when="2024-04-17T23:22:26.563+01:00"><api:citation-count>49</api:citation-count><api:native><api:field name="abstract" type="text" display-name="Abstract"><api:text>The introduction of transformer-based language models has been a revolutionary step for natural language processing (NLP) research. These models, such as BERT, GPT and ELECTRA, led to state-of-the-art performance in many NLP tasks. Most of these models were initially developed for English and other languages followed later. Recently, several Arabic-specific models started emerging. However, there are limited direct comparisons between these models. In this paper, we evaluate the performance of 24 of these models on Arabic sentiment and sarcasm detection. Our results show that the models achieving the best performance are those that are trained on only Arabic data, including dialectal Arabic, and use a larger number of parameters, such as the recently released MARBERT. However, we noticed that AraELECTRA is one of the top performing models while being much more efficient in its computational cost. Finally, the experiments on AraGPT2 variants showed low performance compared to BERT models, which indicates that it might not be suitable for classification tasks.</api:text></api:field><api:field name="authors" type="person-list" display-name="Authors"><api:people><api:person><api:last-name>Farha</api:last-name><api:initials>IA</api:initials><api:first-names>IA</api:first-names><api:separate-first-names><api:first-name>I</api:first-name><api:first-name>A</api:first-name></api:separate-first-names><api:addresses><api:address iso-country-code="GB"><api:line type="organisation">The University of Edinburgh</api:line><api:line type="city">Edinburgh</api:line><api:line type="country">United Kingdom</api:line></api:address></api:addresses><api:identifiers><api:identifier scheme="scopus-author-id">57880138500</api:identifier></api:identifiers></api:person><api:person><api:last-name>Magdy</api:last-name><api:initials>W</api:initials><api:first-names>W</api:first-names><api:separate-first-names><api:first-name>W</api:first-name></api:separate-first-names><api:addresses><api:address iso-country-code="GB"><api:line type="organisation">The University of Edinburgh</api:line><api:line type="city">Edinburgh</api:line><api:line type="country">United Kingdom</api:line></api:address><api:address iso-country-code="GB"><api:line type="organisation">The Alan Turing Institute</api:line><api:line type="city">London</api:line><api:line type="country">United Kingdom</api:line></api:address></api:addresses><api:identifiers><api:identifier scheme="scopus-author-id">15044913900</api:identifier></api:identifiers></api:person></api:people></api:field><api:field name="isbn-13" type="text" display-name="ISBN-13"><api:text>9781954085091</api:text></api:field><api:field name="journal" type="text" display-name="Title of published proceedings"><api:text>WANLP 2021 - 6th Arabic Natural Language Processing Workshop, Proceedings of the Workshop</api:text></api:field><api:field name="pagination" type="pagination" display-name="Pagination"><api:pagination><api:begin-page>21</api:begin-page><api:end-page>31</api:end-page></api:pagination></api:field><api:field name="publication-date" type="date" display-name="Publication date"><api:date><api:day>1</api:day><api:month>1</api:month><api:year>2021</api:year></api:date></api:field><api:field name="publication-status" type="text" display-name="Status"><api:text>Published</api:text></api:field><api:field name="title" type="text" display-name="Title"><api:text>Benchmarking Transformer-based Language Models for Arabic Sentiment and Sarcasm Detection</api:text></api:field></api:native></api:record></api:records><api:fields/><api:relationships href="https://mypublications.shef.ac.uk:8091/secure-api/v6.13/publications/1540335/relationships"/></api:object></api:result></api:response>