# -*- coding: utf-8 -*-
"""ChatAcademyImplementation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lUWHGEOSojUxIwz2cJX6rSXs1599_5tl

# LLama Index Demo - By Seth Steele
---

This is a simple demo of RAG on LLama-2

## 1. Change to GPU runtime
Click on "Runtime" -> "Change runtime type" and make sure "T4 GPU" is selected (the only GPU available on the free plan).

## 2. Installation and Setup

The following snippet of code will:
1. Install the transformers and accelerate libraries that we will use to access and run the Llama model.
2. Initiate a login to your HuggingFace account.
3. Install the necessary packages and our LLama-2 LLM.
4. Imports the packages we need
5. Connects to the google drive to access data if needed (Comment this out if not on colab)

This second step is nessecary because, whilst Llama is an open-source model, access to it is still restricted to those who have been given access by Meta. Instructions for getting access to Llama + granting that access to your HuggingFace account can be found here: https://ai.meta.com/llama/get-started/
"""

import os
import sys
import nest_asyncio

nest_asyncio.apply()

hf_token = sys.argv[1]

import numpy as np
from accelerate import Accelerator
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, pipeline, logging
from llama_index.core import VectorStoreIndex, PromptTemplate, Settings, Document, Response
from llama_index.core.llama_pack import download_llama_pack
from llama_index.core.llama_dataset.generator import RagDatasetGenerator
from llama_index.llms.huggingface import HuggingFaceLLM
from llama_index.core.evaluation import FaithfulnessEvaluator, RelevancyEvaluator, CorrectnessEvaluator, BatchEvalRunner
from bs4 import BeautifulSoup
import torch
import calendar
import spacy
import asyncio


"""**Note** - you may have to restart the runtime
by clicking "Runtime" -> "Restart runtime" after loading in the accelerator library for the subsequent code to run.

# 3. Setup The LLM

These are the settings that change the LLM in use to the 7 billion parameter model of Llama-2 and get it ready for fine tuning.
"""

compute_dtype = getattr(torch, "float16")

baseModel = "meta-llama/Llama-2-13b-chat-hf"
"""
quant_config = BitsAndBytesConfig(
    load_in_4bit=True,
    load_in_8bit=False,
    bnb_4bit_compute_dtype=compute_dtype,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
)

llm = AutoModelForCausalLM.from_pretrained(
    baseModel,
    quantization_config=quant_config,
    device_map={"": 0}
)

llm.config.use_cache = False
llm.config.pretraining_tp = 1

datasetName = "cais/mmlu"
dataset = load_dataset(datasetName , split="test", name = "all")

new_model = "llama-2-7b-chat-academy-test"

tokenizer = AutoTokenizer.from_pretrained(baseModel, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

peft_params = LoraConfig(
    lora_alpha=16,
    lora_dropout=0.1,
    r=64,
    bias="none",
    task_type="QUESTION_ANS",
)

training_params = TrainingArguments(
    output_dir="./results",
    num_train_epochs=1,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=1,
    optim="paged_adamw_32bit",
    save_steps=25,
    logging_steps=25,
    learning_rate=2e-4,
    weight_decay=0.001,
    fp16=False,
    bf16=False,
    max_grad_norm=0.3,
    max_steps=-1,
    warmup_ratio=0.03,
    group_by_length=True,
    lr_scheduler_type="constant",
    report_to="tensorboard"
)

trainer = SFTTrainer(
    model=llm,
    train_dataset=dataset,
    peft_config=peft_params,
    dataset_text_field="question",
    max_seq_length=None,
    tokenizer=tokenizer,
    args=training_params,
    packing=False,
)

trainer.model.save_pretrained(new_model)
trainer.tokenizer.save_pretrained(new_model)

trainedLLM = HuggingFaceLLM(
    model_name= new_model,
    tokenizer_name= new_model,
    query_wrapper_prompt=PromptTemplate("<s> [INST] {query_str} [/INST] "),
    context_window=3900,
    model_kwargs={"token": hf_token, "quantization_config": quant_config},
    tokenizer_kwargs={"token": hf_token},
    device_map="auto",
)
"""

llm = HuggingFaceLLM(model_name=baseModel, tokenizer_name =baseModel)
evalLLM = HuggingFaceLLM(model_name=baseModel, tokenizer_name =baseModel)
genLLM = HuggingFaceLLM(model_name=baseModel, tokenizer_name =baseModel)

"""#  4. Load the data and build an index

The following code defines some custom readers creates an index over the xml files.

This part defines all these methods.
"""

def readRole(root):
  isStudent = False
  isAcademic = False
  try:
    if(root.find("api:is-student").text == "true"):
      isStudent = True
  except AttributeError:
    isStudent = False

  try:
    if(root.find("api:is-academic").text == "true"):
      isAcademic = True
  except AttributeError:
    isAcademic = False

  if(isStudent == True):
    if(isAcademic == False):
      return "student"
    else:
      return "student and academic"
  else:
    if(isAcademic == True):
      return "academic"
    else:
      return "neither student nor academic"

def readCurrent(root):
  isCurrent = False
  try:
    if(root.find("api:is-current-staff").text == "true"):
      isCurrent = True
  except AttributeError:
    isCurrent = False

  return str(isCurrent)

def readEmail(root):
  email = ""
  try:
    email = root.find("api:email-address").text
  except AttributeError:
    email = "N/A"

  return email

def readDepartment(root):
  department = ""
  try:
    department = root.find("api:primary-group-descriptor").text
  except AttributeError:
    department = "N/A"

  return department

def readArrivalDate(root):
  ArrivalDate = ""
  try:
    ArrivalDate = root.find("api:arrive-date").text
  except AttributeError:
    ArrivalDate = "N/A"

  return ArrivalDate

def readUsername(root):
  username = ""
  try:
    username = root.find("api:object", {"category": "user", "type":"person"})["username"]
  except TypeError:
    username = "N/A"

  return username

def readID(root):
  id = ""
  try:
    id = root.find("api:object", {"category": "user", "type":"person"})["id"]
  except TypeError:
    id = "N/A"
  return id

def authorReader(autFiles):
  authorDocs = []
  for path in autFiles:
    with open(path, 'r') as f:
      data = f.read()
    root = BeautifulSoup(data, "xml")
    title = root.find("api:title").text
    initials = root.find("api:initials").text
    first_name = root.find("api:first-name").text
    last_name = root.find("api:last-name").text
    role = readRole(root)
    current_member = readCurrent(root)
    email = readEmail(root)
    department = readDepartment(root)
    arrival_date = readArrivalDate(root)
    username = readUsername(root)
    id = readID(root)
    doc = Document(
      text = title + " " + first_name + " " + last_name,
      metadata={
          "title": title,
          "initials": initials,
          "first name": first_name,
          "last_name": last_name,
          "role": role,
          "current member": current_member,
          "email": email,
          "department": department,
          "arrival_date": arrival_date,
          "username": username,
          "ID": id,
          "type" : "person"
          }
    )
    authorDocs.append(doc)
  return authorDocs

def amountReader(root):
  amount = ""
  try:
     amount = root.find("api:field", {"name": "amount", "type":"money"}).text
     amount = "".join(filter(str.isdigit, amount))
  except AttributeError:
     amount = "N/A"
  return amount

def currencyReader(root):
  currency = ""
  try:
    currency = root.find("api:money")["iso-currency"]
  except TypeError:
    currency = "N/A"
  return currency

def grantEndDateReader(root):
  present = True
  endYear = ""
  endMonth = ""
  try:
    root = root.find("api:field", {"name": "end-date", "type":"date", "display-name":"End date"})
  except AttributeError:
    endYear = "N/A"
    endMonth = "N/A"
    present = False

  if(present):
    try:
      endYear = int(root.find("api:year").text)
    except AttributeError:
      endYear = "N/A"
    try:
      endMonth = int(root.find("api:month").text)
    except AttributeError:
      endMonth = "N/A"
  return endMonth, endYear

def grantStartDateReader(root):
  present = True
  endYear = ""
  endMonth = ""
  try:
    root = root.find("api:field", {"name": "end-date", "type":"date", "display-name":"End date"})
  except AttributeError:
    endYear = "N/A"
    endMonth = "N/A"
    present = False

  if(present):
    try:
      endYear = int(root.find("api:year").text)
    except AttributeError:
      endYear = "N/A"
    try:
      endMonth = calendar.month_name[int(root.find("api:month").text)]
    except AttributeError:
      endMonth = "N/A"
  return endMonth, endYear

def funderReader(root):
  funder = ""
  try:
    funder = root.find("api:field", {"name": "funder-name", "type":"text"}).text.strip()
  except AttributeError:
    funder = "N/A"
  return funder

def funderReferenceReader(root):
  funderRef = ""
  try:
    funderRef = root.find("api:field", {"name": "funder-reference", "type":"text"}).text.strip()
  except AttributeError:
    funderRef = "N/A"
  return funderRef

def statusReader(root):
  statusRef = ""
  try:
    statusRef = root.find("api:field", {"name": "status", "type":"text"}).text.strip().lower()
  except AttributeError:
    statusRef = "N/A"
  return statusRef

def grantTitleReader(root):
  title = ""
  try:
    title = root.find("api:field", {"name": "title", "type":"text"}).text
  except AttributeError:
    title = "N/A"
  return title

def grantReader(files):
  grantDocs = []
  for path in files:
    with open(path, 'r') as f:
      data = f.read()
    root = BeautifulSoup(data, "xml")
    amount = amountReader(root)
    currency = currencyReader(root)
    endDate = grantEndDateReader(root)
    funder = funderReader(root)
    funderRef = funderReferenceReader(root)
    startDate = grantStartDateReader(root)
    status = statusReader(root)
    title = grantTitleReader(root)
    doc = Document(
      text = title,
      metadata={
            "amount": int(amount),
            "currency": currency,
            "end month": endDate[0],
            "end year": endDate[1],
            "start month": startDate[0],
            "start year": startDate[1],
            "funder name": funder,
            "funder reference": funderRef,
            "status": status,
            "type": "grant"
            }
    )
    grantDocs.append(doc)
  return grantDocs

def licenceReader(root):
  licence = ""
  try:
     licence = root.find("api:field", {"name": "cc-licence", "type":"text"}).text.strip()
  except AttributeError:
     licence = "N/A"
  return licence

def issnReader(root):
  root = root.find("api:field", {"name": "issns", "type":"issn-list", "display-name":"ISSNs"})
  issnData = root.find_all("api:issn")
  issnList = []
  for issn in issnData:
    try:
      issnList.append(issn.text)
    except AttributeError:
      issnList = "N/A"
  return issnList

def journalStartDateReader(root):
  startYear = ""
  try:
    startYear = root.find("api:field", {"name": "start-year-oa", "type":"date", "display-name":"Start year Open Access"}).text.strip()
  except AttributeError:
    startYear = "N/A"
  return startYear

def pubFeeReader(root):
  pubFee = ""
  try:
    pubFee = root.find("api:field", {"name": "publication-fee", "type":"text"}).text.strip()
  except AttributeError:
    pubFee = "N/A"
  return pubFee

def publisherReader(root):
  publisher = ""
  try:
    publisher = root.find("api:field", {"name": "publisher", "type":"text"}).text.strip()
  except AttributeError:
    publisher = "N/A"
  return publisher

def urlReader(root):
  statusRef = ""
  try:
    statusRef = root.find("api:field", {"name": "submission-info-url", "type":"text"}).text
  except AttributeError:
    statusRef = "N/A"
  return statusRef

def journalTitleReader(root):
  title = ""
  try:
    title = root.find("api:field", {"name": "title", "type":"text"}).text
  except AttributeError:
    title = "N/A"
  return title

def journalReader(files):
  journalDocs = []
  for path in files:
    with open(path, 'r') as f:
      data = f.read()
    root = BeautifulSoup(data, "xml")
    title = journalTitleReader(root)
    licence = licenceReader(root)
    issns = issnReader(root)
    pubFee = pubFeeReader(root)
    publisher = publisherReader(root)
    startDate = journalStartDateReader(root)
    url = urlReader(root)
    doc = Document(
      text = title,
      metadata={
            "licence": licence,
            "issns": issns,
            "publication fee": pubFee,
            "publisher": publisher,
            "start date": startDate,
            "url": url,
            "type": "journal"
            }
    )
    journalDocs.append(doc)
  return journalDocs

def abstractReader(root):
  abstract = ""
  try:
      abstract = root.find("api:field", {"name": "abstract", "type":"text", "display-name":"Abstract"}).text
  except AttributeError:
     abstract = "N/A"
  return abstract

def authorFromPubReader(root):
  authorsData = root.find("api:field", {"name": "authors", "type":"person-list", "display-name":"Authors"})
  authorData = authorsData.find_all("api:person")
  authorList = []
  for author in authorData:
    initials = author.find("api:initials").text
    try:
      first_name = author.find("api:first-names").text
    except AttributeError:
      first_name = initials
    last_name = author.find("api:last-name").text
    authorDict = {"initials": initials, "first name": first_name, "last name": last_name}
    authorList.append(authorDict)
  return authorList

def pubDateReader(root):
  data = root.find("api:field", {"name": "publication-date", "type":"date", "display-name":"Publication date"})
  day = None
  month = None
  year = None
  try:
    year = data.find("api:year").text
  except AttributeError:
    year = "N/A"
  try:
    month = calendar.month_name[int(data.find("api:month").text)]
  except AttributeError:
    month = "N/A"
  try:
    day = data.find("api:day").text
  except AttributeError:
    day = "N/A"

  return day, month, year

def doiReader(root):
  try:
     doi = root.find("api:field", {"name": "doi", "type":"text", "display-name":"DOI"}).find("api:text").text
  except AttributeError:
     doi= "N/A"
  return doi

def keywordsReader(root):
  try:
     keywords = root.find("api:field", {"name": "keywords", "type":"keyword-list", "display-name":"Keywords"}).find_all("api:keyword").text
  except AttributeError:
     keywords = "N/A"
  return keywords

def pubJournalReader(root):
  try:
     journal = root.find("api:field", {"name": "journal", "type":"text", "display-name":"Journal"}).find("api:text").text
  except AttributeError:
     journal = "N/A"
  return journal

def publicationReader(pubFiles):
  publicationDocs = []
  for path in pubFiles:
    with open(path, 'r') as f:
      data = f.read()
    root = BeautifulSoup(data, "xml")
    abstract = abstractReader(root)
    authorList = authorFromPubReader(root)
    title = root.find("api:field", {"name": "title", "type":"text", "display-name":"Title"}).text
    keywords = keywordsReader(root)
    date = pubDateReader(root)
    doi = doiReader(root)
    journal = pubJournalReader(root)
    doc = Document(
      text = abstract,
      metadata={
            "authors": authorList,
            "title": title,
            "keywords": keywords,
            "day published": date[0],
            "month published": date[1],
            "year published": date[2],
            "doi": doi,
            "journal published in": journal
            }
    )
    publicationDocs.append(doc)
  return publicationDocs

"""And this section actually builds the index."""

def absoluteFilePaths(directory):
    files = []
    for dirpath,_,filenames in os.walk(directory):
        for f in filenames:
            files.append(os.path.abspath(os.path.join(dirpath, f)))
    return files

publicationFiles = absoluteFilePaths("data/publications")
authorFiles = absoluteFilePaths("data/staff")
grantFiles = absoluteFilePaths("data/grants")
journalFiles = absoluteFilePaths("data/journals")

print(publicationFiles + authorFiles + grantFiles + journalFiles)

docs = publicationReader(publicationFiles) + authorReader(authorFiles) + grantReader(grantFiles) + journalReader(journalFiles)

evalQuestionPrompt = "You are a Teacher/Professor. Your task is to setup a question for an upcoming quiz/examination. The question should be diverse in nature across the document. Restrict the questions to the context information provided. The context information is about research at the University of Sheffield. The context infromation will contain information about either academic publications, people, grants or journals. You will have to include the name or title of the publication, grant, person or journal in the question so the candidates know which document you are asking the question about."

dataset_generator = RagDatasetGenerator.from_documents(docs, llm=genLLM, num_questions_per_chunk=1, question_gen_query = evalQuestionPrompt)

ragDataset = dataset_generator.generate_dataset_from_nodes()

for i in range(len(ragDataset.dict()['examples'])):
  print(str(ragDataset[i]['query']) + "\n")
  print(str(ragDataset[i]['reference_contexts']) + "\n")
  print(str(ragDataset[i]['reference_answer']) + "\n")

Settings.chunk_size = 768
Settings.chunk_overlap = 50
Settings.llm = llm
Settings.embed_model = "local:BAAI/bge-small-en-v1.5"

index = VectorStoreIndex.from_documents(docs)

"""# 5. Use the model to respond to a query
In this section we can write out our query and then get the model to respond.

The following line is simply to set our query, change this to whatever you would like to ask the model.
"""

prompt ="Tell me about a paper titled Connectionist simulation of attitude learning: Asymmetries in the acquisition of positive and negative evaluations by JR Eiser?"

query_engine = index.as_query_engine()
response = query_engine.query(prompt)
print(response)

#chat_engine = index.as_chat_engine()
#response = chat_engine.chat(prompt)
#print(response)

"""# 6. Import Some Dataset Questions
This section of the implementation takes some predetermined questions abou the system and judges how relevant the response is.


faithfulness_llm = FaithfulnessEvaluator(llm=evalLLM)
relevancy_llm = RelevancyEvaluator(llm=evalLLM)

runner = BatchEvalRunner({"faithfulness": faithfulness_llm, "relevancy": relevancy_llm},workers=8)

eval_results = asyncio.run(runner.aevaluate_queries(index.as_query_engine(llm=llm), queries=qas.questions))
"""

RagEvaluatorPack = download_llama_pack("RagEvaluatorPack", "./rag_evaluator_pack")

ragEvaluator = RagEvaluatorPack(query_engine=query_engine, rag_dataset=ragDataset, llm = evalLLM)

def get_eval_results(key, eval_results):
    results = eval_results[key]
    correct = 0
    for result in results:
        if result.passing:
            correct += 1
    score = correct / len(results)
    return f"{key} Score: {score} \n"


file1 = open("results.txt", "w")
toFile = asyncio.run(ragEvaluator.run())
file1.write(str(toFile))
file1.close()